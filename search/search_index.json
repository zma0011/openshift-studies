{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenShift Studies OpenShift Container Platform is about developing, deploying, and running containerized applications. It is based on docker and kubernetes and add the following features: Routes: represents the way external clients are able to access applications running in OpenShift Deployment config: Represents the set of containers included in a pod, and the deployment strategies to be used. CLI, REST API for administration or Web Console, and Eclipse plugin . Built to be multi tenants. You can also grant other users access to any of your projects. Use the concept of project to allow for controlled accesses and quotas for developers. Projects are mapped to k8s namespaces. Source-to-image (S2I) is a tool for building reproducible Docker images. S2I supports incremental builds which re-use previously downloaded dependencies, and previously built artifacts. OpenShift is S2I-enabled and can use S2I as one of its build mechanisms. Build config: Used by the OpenShift Source-to-Image (S2I) feature to build a container image from application source code stored in a Git repository OpenShift for production comes in several variants: OpenShift Origin: from http://openshift.org OpenShift Container Platform: integrated with RHEL and supported by RedHat. It allows for building a private or public PaaS cloud. OpenShift Online: multi-tenant public cloud managed by Red Hat OpenShift Dedicated: single-tenant container application platform hosted on Amazon Web Services (AWS) or Google Cloud Platform and managed by Red Hat. The way that external clients are able to access applications running in OpenShift is through the OpenShift routing layer. The default OpenShift router (HAProxy) uses the HTTP header of the incoming request to determine where to proxy the connection. See also my summary on k8s . Concepts Openshift is based on kubernetes. It adds the concept of project, mapped to a k8s namespace, to govern the application access control, resource quota and life cycle. It is the top-level element for one to many applications. We can deploy any docker image as soon as they are well built: such as defining the port any service is exposed on, not needing to run specifically as the root user or other dedicated user, and which embeds a default command for running the application. The default OpenShift router (HAProxy) uses the HTTP header of the incoming request to determine where to proxy the connection. Routes defines hostname, service name, port number and TLS settings: Routes are used to expose app over HTTP. OpenShift can handle termination for secure HTTP connections, or a secure connection can be tunneled through direct to the application, with the application handling termination of the secure connection. Non HTTP applications can be exposed via a tunneled secure connection if the client supports the SNI extension for a secure connection using TLS. A router (ingress controller) forwards HTTP and TLS requests to the service addresses inside the Kubernetes SDN. OpenShift routes are implemented by a cluster-wide router service, which runs as a containerized application in the OpenShift cluster. The router service uses HAProxy as the default implementation. Getting started Use IBM Cloud cluster to get an Openshift cluster.. We can also use openshift online Here is summary of OC cli commands . Collaborate User can be added to an existing project, via the View membership menu on a project. Each user can have different roles. Edit Role can perform most tasks within the project, except tasks related to administration of the project. Remark state about the current login session is stored in the home directory of the local user running the oc command, so user need to logout and login to the second cluster he wants to access. You can get a list of all OpenShift clusters you have ever logged into by running: oc config get-clusters Source to image (s2i) Source to image toolkit aims to simplify the deployment to openshift. It uses a build image to execute an assemble script that builds code and docker image without Dockerfile. The following figure, shows the resources created by the oc new-app command when the argument is an application source code repository. From an existing repository, s2i create add a set of elements to define the workflow into the repo. For example the command below will add Dockerfile and scripts to create a build image named ibmcase/buildorderproducer from the local folder where the code is. s2i create ibmcase/buildorderproducer . When the assemble script is done, the container image is committed to internal image repository. The CMD part of the dockerfile execute a run script. Here is another command to build the output image using existing build image on local code: s2i build --copy . centos/python-36-centos7 ibmcase/orderproducer Note s2i takes the code from git, so to use the local code before committing it to github, add the --copy argument. OpenShift builds applications against an image stream. The OpenShift installer populates several image streams by default during installation. oc get is -n openshift If only a source repository is specified, oc new-app tries to identify the correct image stream to use for building the application oc command Create an app and build from a specific context directory. oc new-app https://github.com/jbcodeforce/refarch-kc-order-ms --context-dir=order-command-ms/","title":"Introduction"},{"location":"#openshift-studies","text":"OpenShift Container Platform is about developing, deploying, and running containerized applications. It is based on docker and kubernetes and add the following features: Routes: represents the way external clients are able to access applications running in OpenShift Deployment config: Represents the set of containers included in a pod, and the deployment strategies to be used. CLI, REST API for administration or Web Console, and Eclipse plugin . Built to be multi tenants. You can also grant other users access to any of your projects. Use the concept of project to allow for controlled accesses and quotas for developers. Projects are mapped to k8s namespaces. Source-to-image (S2I) is a tool for building reproducible Docker images. S2I supports incremental builds which re-use previously downloaded dependencies, and previously built artifacts. OpenShift is S2I-enabled and can use S2I as one of its build mechanisms. Build config: Used by the OpenShift Source-to-Image (S2I) feature to build a container image from application source code stored in a Git repository OpenShift for production comes in several variants: OpenShift Origin: from http://openshift.org OpenShift Container Platform: integrated with RHEL and supported by RedHat. It allows for building a private or public PaaS cloud. OpenShift Online: multi-tenant public cloud managed by Red Hat OpenShift Dedicated: single-tenant container application platform hosted on Amazon Web Services (AWS) or Google Cloud Platform and managed by Red Hat. The way that external clients are able to access applications running in OpenShift is through the OpenShift routing layer. The default OpenShift router (HAProxy) uses the HTTP header of the incoming request to determine where to proxy the connection. See also my summary on k8s .","title":"OpenShift Studies"},{"location":"#concepts","text":"Openshift is based on kubernetes. It adds the concept of project, mapped to a k8s namespace, to govern the application access control, resource quota and life cycle. It is the top-level element for one to many applications. We can deploy any docker image as soon as they are well built: such as defining the port any service is exposed on, not needing to run specifically as the root user or other dedicated user, and which embeds a default command for running the application. The default OpenShift router (HAProxy) uses the HTTP header of the incoming request to determine where to proxy the connection. Routes defines hostname, service name, port number and TLS settings: Routes are used to expose app over HTTP. OpenShift can handle termination for secure HTTP connections, or a secure connection can be tunneled through direct to the application, with the application handling termination of the secure connection. Non HTTP applications can be exposed via a tunneled secure connection if the client supports the SNI extension for a secure connection using TLS. A router (ingress controller) forwards HTTP and TLS requests to the service addresses inside the Kubernetes SDN. OpenShift routes are implemented by a cluster-wide router service, which runs as a containerized application in the OpenShift cluster. The router service uses HAProxy as the default implementation.","title":"Concepts"},{"location":"#getting-started","text":"Use IBM Cloud cluster to get an Openshift cluster.. We can also use openshift online Here is summary of OC cli commands .","title":"Getting started"},{"location":"#collaborate","text":"User can be added to an existing project, via the View membership menu on a project. Each user can have different roles. Edit Role can perform most tasks within the project, except tasks related to administration of the project. Remark state about the current login session is stored in the home directory of the local user running the oc command, so user need to logout and login to the second cluster he wants to access. You can get a list of all OpenShift clusters you have ever logged into by running: oc config get-clusters","title":"Collaborate"},{"location":"#source-to-image-s2i","text":"Source to image toolkit aims to simplify the deployment to openshift. It uses a build image to execute an assemble script that builds code and docker image without Dockerfile. The following figure, shows the resources created by the oc new-app command when the argument is an application source code repository. From an existing repository, s2i create add a set of elements to define the workflow into the repo. For example the command below will add Dockerfile and scripts to create a build image named ibmcase/buildorderproducer from the local folder where the code is. s2i create ibmcase/buildorderproducer . When the assemble script is done, the container image is committed to internal image repository. The CMD part of the dockerfile execute a run script. Here is another command to build the output image using existing build image on local code: s2i build --copy . centos/python-36-centos7 ibmcase/orderproducer Note s2i takes the code from git, so to use the local code before committing it to github, add the --copy argument. OpenShift builds applications against an image stream. The OpenShift installer populates several image streams by default during installation. oc get is -n openshift If only a source repository is specified, oc new-app tries to identify the correct image stream to use for building the application","title":"Source to image (s2i)"},{"location":"#oc-command","text":"Create an app and build from a specific context directory. oc new-app https://github.com/jbcodeforce/refarch-kc-order-ms --context-dir=order-command-ms/","title":"oc command"},{"location":"ansible/","text":"Ansible","title":"Ansible"},{"location":"ansible/#ansible","text":"","title":"Ansible"},{"location":"code-ready/","text":"Code Ready Red Hat tools to help developer get the most of k8s and get started quickly. It includes workspace, container, studio, builder, toolchain and dependencies. ODO: Openshift Do ODO is a CLI for developer to abstract kubernetes. odo can build and deploy your code to your cluster immediately after you save your changes. Odo abstracts away Kubernetes and OpenShift concepts. odo helps manage the components in a grouping to support the application features. A selection of runtimes, frameworks, and other components are available on an OpenShift cluster for building your applications. This list is referred to as the Developer Catalog. Installing odo: curl -L https://mirror.openshift.com/pub/openshift-v4/clients/odo/latest/odo-darwin-amd64 -o /usr/local/bin/odo chmod +x /usr/local/bin/odo List existing software runtime catalog deployed on a cluster (For example Java and nodejs are supported runtimes): odo catalog list components To create a component (create a config.yml) from a java springboot app, once the jar is built, the following command defines (a backend named component) to run it on top of the java runtime: odo create java backend --binary target/wildwest-1.0.jar The component is not yet deployed on OpenShift. With an odo create command, a configuration file called config.yaml has been created in the local directory. To see the config use: odo config view COMPONENT SETTINGS ------------------------------------------------ PARAMETER CURRENT_VALUE Type java:8 Application app Project myproject SourceType binary Ref SourceLocation target/wildwest-1.0.jar Ports 8080 /TCP,8443/TCP,8778/TCP Name backend Then to deploy the binary jar file to Openshift: odo push OpenShift has created a container to host the backend component, deployed the container into a pod running on the OpenShift cluster, and started up the backend component. You can view the backend component being started up, in the Developer perspective, under the Topology view. When a dark blue circle appears around the backend component, the pod is ready and the backend component container will start running on it. (A light blue ring means the pod is in a pending state and hasn't started yet) OpenShift provides mechanisms to publish communication bindings from a program to its clients. This is referred to as linking. To link the current frontend component to the backend: odo link backend --component frontend --port 8080 This will inject configuration information into the frontend about the backend and then restart the frontend component. To expose an application to external client, we need to add a URL: odo url create frontend --port 8080 odo push To adapt to the developer changes, we can tell odo to watch for changes on the file system in the background using: odo watch Once the change is recognized, odo will push the changes to the frontend component and print its status to the terminal. See odo github Code Ready Container Red Hat CodeReady Containers brings a minimal OpenShift 4.0 or newer cluster to your local computer. See getting started here Install quick summary Download 1.8 G binary from here , extract it in a folder with $PATH. It shoud just create a crc command. Run crc setup to set up the environment of your host machine Start the VM crc start . Keep the password for the developer user. oc login -u developer -p developer https://api.crc.testing:6443 Stop the VM crc stop Access the console crc console or crc console --credentials Delete the VM: crc delete The crc ip command can be used to obtain the VM IP address as needed Notes CodeReady Containers creates a /etc/resolver/testing file which instructs macOS to forward all DNS requests for the testing domain to the CodeReady Containers virtual machine.CodeReady Containers also adds an api.crc.testing entry to /etc/hosts pointing at the VM IP address. This is needed by the oc binary. To access the OpenShift web console, follow these steps: Run crc console. This will open your web browser and direct it to the web console. Log in to the OpenShift web console as the developer user with the password printed in the output of the crc start command or by running: crc console --credentials To access to the administrator user login as kubeadmin, something like: oc login -u kubeadmin -p 7z6T5-qmTth-oxaoD-p3xQF https://api.crc.testing:6443 Use oc with CRC To access the OpenShift cluster via the oc command: crc oc-env Get the Cluster Operators: oc get co CodeReady workspaces CodeReady is a web-based IDE running on Openshift and in a web browser, it is based on Eclipse Che 7. It is installed using the OperatorHub Catalog present in the OpenShift web console, see installation note It uses the concept of devfile to define the development environment as portable and committable to github.","title":"Code Ready"},{"location":"code-ready/#code-ready","text":"Red Hat tools to help developer get the most of k8s and get started quickly. It includes workspace, container, studio, builder, toolchain and dependencies.","title":"Code Ready"},{"location":"code-ready/#odo-openshift-do","text":"ODO is a CLI for developer to abstract kubernetes. odo can build and deploy your code to your cluster immediately after you save your changes. Odo abstracts away Kubernetes and OpenShift concepts. odo helps manage the components in a grouping to support the application features. A selection of runtimes, frameworks, and other components are available on an OpenShift cluster for building your applications. This list is referred to as the Developer Catalog. Installing odo: curl -L https://mirror.openshift.com/pub/openshift-v4/clients/odo/latest/odo-darwin-amd64 -o /usr/local/bin/odo chmod +x /usr/local/bin/odo List existing software runtime catalog deployed on a cluster (For example Java and nodejs are supported runtimes): odo catalog list components To create a component (create a config.yml) from a java springboot app, once the jar is built, the following command defines (a backend named component) to run it on top of the java runtime: odo create java backend --binary target/wildwest-1.0.jar The component is not yet deployed on OpenShift. With an odo create command, a configuration file called config.yaml has been created in the local directory. To see the config use: odo config view COMPONENT SETTINGS ------------------------------------------------ PARAMETER CURRENT_VALUE Type java:8 Application app Project myproject SourceType binary Ref SourceLocation target/wildwest-1.0.jar Ports 8080 /TCP,8443/TCP,8778/TCP Name backend Then to deploy the binary jar file to Openshift: odo push OpenShift has created a container to host the backend component, deployed the container into a pod running on the OpenShift cluster, and started up the backend component. You can view the backend component being started up, in the Developer perspective, under the Topology view. When a dark blue circle appears around the backend component, the pod is ready and the backend component container will start running on it. (A light blue ring means the pod is in a pending state and hasn't started yet) OpenShift provides mechanisms to publish communication bindings from a program to its clients. This is referred to as linking. To link the current frontend component to the backend: odo link backend --component frontend --port 8080 This will inject configuration information into the frontend about the backend and then restart the frontend component. To expose an application to external client, we need to add a URL: odo url create frontend --port 8080 odo push To adapt to the developer changes, we can tell odo to watch for changes on the file system in the background using: odo watch Once the change is recognized, odo will push the changes to the frontend component and print its status to the terminal. See odo github","title":"ODO: Openshift Do"},{"location":"code-ready/#code-ready-container","text":"Red Hat CodeReady Containers brings a minimal OpenShift 4.0 or newer cluster to your local computer. See getting started here","title":"Code Ready Container"},{"location":"code-ready/#install-quick-summary","text":"Download 1.8 G binary from here , extract it in a folder with $PATH. It shoud just create a crc command. Run crc setup to set up the environment of your host machine Start the VM crc start . Keep the password for the developer user. oc login -u developer -p developer https://api.crc.testing:6443 Stop the VM crc stop Access the console crc console or crc console --credentials Delete the VM: crc delete The crc ip command can be used to obtain the VM IP address as needed Notes CodeReady Containers creates a /etc/resolver/testing file which instructs macOS to forward all DNS requests for the testing domain to the CodeReady Containers virtual machine.CodeReady Containers also adds an api.crc.testing entry to /etc/hosts pointing at the VM IP address. This is needed by the oc binary. To access the OpenShift web console, follow these steps: Run crc console. This will open your web browser and direct it to the web console. Log in to the OpenShift web console as the developer user with the password printed in the output of the crc start command or by running: crc console --credentials To access to the administrator user login as kubeadmin, something like: oc login -u kubeadmin -p 7z6T5-qmTth-oxaoD-p3xQF https://api.crc.testing:6443","title":"Install quick summary"},{"location":"code-ready/#use-oc-with-crc","text":"To access the OpenShift cluster via the oc command: crc oc-env Get the Cluster Operators: oc get co","title":"Use oc with CRC"},{"location":"code-ready/#codeready-workspaces","text":"CodeReady is a web-based IDE running on Openshift and in a web browser, it is based on Eclipse Che 7. It is installed using the OperatorHub Catalog present in the OpenShift web console, see installation note It uses the concept of devfile to define the development environment as portable and committable to github.","title":"CodeReady workspaces"},{"location":"compendium/","text":"OpenShift Compendium Getting Started with openshift on learn.openshift.com videos for Developers Red Hat demo system Openshift 4.2 learning playground Source-to-image (S2I) oc cluster wrapper REST API for openshift Minishift github Deploy openshift on IBM classic infrastructure openshift IKS Bastion node Configure user and authorization in openshift Red hat Open TLC","title":"Compendium"},{"location":"compendium/#openshift-compendium","text":"Getting Started with openshift on learn.openshift.com videos for Developers Red Hat demo system Openshift 4.2 learning playground Source-to-image (S2I) oc cluster wrapper REST API for openshift Minishift github Deploy openshift on IBM classic infrastructure openshift IKS Bastion node Configure user and authorization in openshift Red hat Open TLC","title":"OpenShift Compendium"},{"location":"cp4i/","text":"Some notes on Cloud Pak for integration Marketing page Installation instructions from the Playbook MQ MQ Marketing page MQ on cloud product tour tutorial App wih jms to MQ","title":"cp4i"},{"location":"cp4i/#some-notes-on-cloud-pak-for-integration","text":"Marketing page Installation instructions from the Playbook","title":"Some notes on Cloud Pak for integration"},{"location":"cp4i/#mq","text":"MQ Marketing page MQ on cloud product tour tutorial App wih jms to MQ","title":"MQ"},{"location":"deployment-ex/","text":"Deployments examples Multiple ways to deploy an app to openshift: Deploy an application from an existing Docker image. (Using Deploy Image in the project view.) Note There are two options: from an image imported in the openshift cluster, or built from a dockerfile inside the cluster. by accessing a remote image repository like Dockerhub or quay.io . The image will be pulled down and stored within the internal OpenShift image registry. The image will then be copied to any node in the OpenShift cluster where an instance of the application will be scheduled. Application will, by default, be visible internally to the OpenShift cluster, and usually only to other applications within the same project. Use Create route to make the app public. Build and deploy from source code contained in a Git repository using a Source-to-Image toolkit. See this video to get s2i presentation and this section goes to a simple Flask app deployment using s2i. Build and deploy from source code contained in a Git repository from a Dockerfile. Build the first time oc new-build --binary --name = vertx-greeting-application -l app = vertx-greeting-application mvn dependency:copy-dependencies compile oc start-build vertx-greeting-application --from-dir = . --follow oc new-app vertx-greeting-application -l app = vertx-greeting-application oc expose service vertx-greeting-application To update the application, just update the code and run: mvn dependency:copy-dependencies compile oc start-build vertx-greeting-application --from-dir = . --follow Using Helm charts and helm CLI: Helm can be used as well to define the config files and deploy. Here is a new CI/CD example done from scratch based on the Reefer ML project simulator code . See getting started with helm guide. Create helm chart using the command helm create cd simulator/chart helm create kcontainer-reefer-simulator Change the values.yaml to reflect environment and app settings. Remove Ingress as we will define Openshift route for the app to be visible. In the templates folder modify the deployment.yaml to add env variables section: env : - name : PORT value : \"{{ .Values.service.servicePort }}\" - name : APPLICATION_NAME value : \"{{ .Release.Name }}\" - name : KAFKA_BROKERS valueFrom : configMapKeyRef : name : \"{{ .Values.kafka.brokersConfigMap }}\" key : brokers - name : TELEMETRY_TOPIC value : \"{{ .Values.kafka.telemetryTopicName }}\" - name : CONTAINER_TOPIC value : \"{{ .Values.kafka.containerTopicName }}\" {{ - if .Values.eventstreams.enabled }} - name : KAFKA_APIKEY valueFrom : secretKeyRef : name : \"{{ .Values.eventstreams.apikeyConfigMap }}\" key : binding {{ - end }} Create helm template file for deployment: helm template --output-dir templates --namespace eda-demo chart/kcontainer-reefer-simulator/ Push the service.yaml and deployment.yml template to the gitops repository under the branch eda-demo/gse-eda-demos.us-east.containers.appdomain.cloud . In the github repository define secrets environment variables for docker username and password, from your docker hub account. When pushing the repository the gitAction will perform the build. Deploy zipkin from docker image Install it, and expose it with a service oc new-app --docker-image=openzipkin/zipkin oc expose svc/zipkin A new route is created visible with oc get routes . Once the hostname is added to a DNS or /etc/hosts. See zipkin architecture article here Deploy DB2 The Community edition DB2 image on dockerhub . It includes a predefined DB. Clone the DB2 repository to get the helm chart. See readme in this repo, for installation using helm but tiller needs to be installed before. The repository includes a script: db2u-install Create your own docker image with a shell to create the schema: FROM ibmcom/db2 RUN mkdir /var/custom COPY createschema.sh /var/custom RUN chmod a+x /var/custom/createschema.sh Deinstalling configuration oc delete -n jbsandbox sa/db2u role/db2u-role rolebinding/db2u-rolebinding Deploy RabbitMQ First install operator To create a RabbitMQ instance, a RabbitmqCluster resource definition must be created and applied. RabbitMQ Cluster Kubernetes Operator creates the necessary resources, such as Services and StatefulSet, in the same namespace in which the RabbitmqCluster CRD was defined. See some instructions here . Be sure to have kubectl >= 1.14 git clone http://github.com/rabbitmq/cluster-operator.git cd cluster-operator kubectl create -f config/namespace/base/namespace.yaml kubectl create -f config/crd/bases/rabbitmq.com_rabbitmqclusters.yaml # Add cluster roles and roles: rabbitmq-cluster-operator-role and rabbitmq-cluster-leader-election-role kubectl -n rabbitmq-system create --kustomize config/rbac/ kubectl -n rabbitmq-system create --kustomize config/manager/ # Verify CRD installed kubectl get customresourcedefinitions.apiextensions.k8s.io | grep rabbit # Verify the service account oc get sa rabbitmq-cluster-operator # link the service account to security policy oc adm policy add-scc-to-user privileged rabbitmq-cluster-operator If there is an error about user 1000 not in range, change the deployment yaml file for the value securityContext.runAsUser: 1000570000 to a value in range and remove the other runAsGroup and fsGroup. Create a cluster when the operator pod runs, create one instance create a yaml file: apiVersion: rabbitmq.com/v1beta1 kind: RabbitmqCluster metadata: name: eda-rabbitmq spec: replicas: 1 Then do oc apply -f rabbit-cluster.yaml oc get rabbitmqclusters If an error like: \"services \\\"eda-rabbitmq-rabbitmq-headless\\\" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on\" happens, do the following: Deploy sparks Using the operator, see this note Mongodb Define an env file with the following variables: MONGODB_USER=mongo MONGODB_PASSWORD=mongo MONGODB_DATABASE=reeferdb MONGODB_ADMIN_PASSWORD=password then run: oc new-app --env-file=mongo.env --docker-image=openshift/mongodb-24-centos7 See the service: oc describe svc mongodb-24-centos7 For more detail see this note Deploy Jupyter lab See this note to deploy Jupyter lab lastest image to Openshift using the Deploy Image choice. The deployment is done under the project reefer-shipment-solution The environment variable needs to be set to get Jupyter lab. It takes multiple minutes to deploy. For the permission error due to the jovyan user not known, the command was: oc adm policy add-scc-to-user anyuid -z default -n reefer-shipment-solution oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD jupyterlab jupyterlab-reefer-shipment-solution.greencluster-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud all-spark-notebook 8888-tcp None Get the secuity token to login in via the pod logs. oc get pods NAME READY STATUS RESTARTS AGE all-spark-notebook-2-z4dqx oc logs all-spark-notebook-2-z4dqx To avoid loosing the work on the notebook, we need to add PVC to /home/jovyan/work mount point oc get dc oc set volume dc/all-spark-notebook --add --mount-path /home/jovyan/work --claim-size=1G Deploy helm / tiller (DEPRECATED) The goal is to install Tiller server on its own project, and grant it permissions to one or more other projects where Helm Charts will be installed. See the instructions in this blog . Here is a quick summary of the commands performed oc new-project tiller oc project tiller export TILLER_NAMESPACE=tiller oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE=\"${TILLER_NAMESPACE}\" -p HELM_VERSION=v2.16.4 | oc create -f - Once deployed and Tiller server running, create a new project and grant tiller edit role to access this new project, and then use helm CLI to deploy the app: oc new-project myapp oc policy add-role-to-user edit \"system:serviceaccount: ${ TILLER_NAMESPACE } :tiller\"","title":"Deployment examples"},{"location":"deployment-ex/#deployments-examples","text":"Multiple ways to deploy an app to openshift: Deploy an application from an existing Docker image. (Using Deploy Image in the project view.) Note There are two options: from an image imported in the openshift cluster, or built from a dockerfile inside the cluster. by accessing a remote image repository like Dockerhub or quay.io . The image will be pulled down and stored within the internal OpenShift image registry. The image will then be copied to any node in the OpenShift cluster where an instance of the application will be scheduled. Application will, by default, be visible internally to the OpenShift cluster, and usually only to other applications within the same project. Use Create route to make the app public. Build and deploy from source code contained in a Git repository using a Source-to-Image toolkit. See this video to get s2i presentation and this section goes to a simple Flask app deployment using s2i. Build and deploy from source code contained in a Git repository from a Dockerfile. Build the first time oc new-build --binary --name = vertx-greeting-application -l app = vertx-greeting-application mvn dependency:copy-dependencies compile oc start-build vertx-greeting-application --from-dir = . --follow oc new-app vertx-greeting-application -l app = vertx-greeting-application oc expose service vertx-greeting-application To update the application, just update the code and run: mvn dependency:copy-dependencies compile oc start-build vertx-greeting-application --from-dir = . --follow Using Helm charts and helm CLI: Helm can be used as well to define the config files and deploy. Here is a new CI/CD example done from scratch based on the Reefer ML project simulator code . See getting started with helm guide. Create helm chart using the command helm create cd simulator/chart helm create kcontainer-reefer-simulator Change the values.yaml to reflect environment and app settings. Remove Ingress as we will define Openshift route for the app to be visible. In the templates folder modify the deployment.yaml to add env variables section: env : - name : PORT value : \"{{ .Values.service.servicePort }}\" - name : APPLICATION_NAME value : \"{{ .Release.Name }}\" - name : KAFKA_BROKERS valueFrom : configMapKeyRef : name : \"{{ .Values.kafka.brokersConfigMap }}\" key : brokers - name : TELEMETRY_TOPIC value : \"{{ .Values.kafka.telemetryTopicName }}\" - name : CONTAINER_TOPIC value : \"{{ .Values.kafka.containerTopicName }}\" {{ - if .Values.eventstreams.enabled }} - name : KAFKA_APIKEY valueFrom : secretKeyRef : name : \"{{ .Values.eventstreams.apikeyConfigMap }}\" key : binding {{ - end }} Create helm template file for deployment: helm template --output-dir templates --namespace eda-demo chart/kcontainer-reefer-simulator/ Push the service.yaml and deployment.yml template to the gitops repository under the branch eda-demo/gse-eda-demos.us-east.containers.appdomain.cloud . In the github repository define secrets environment variables for docker username and password, from your docker hub account. When pushing the repository the gitAction will perform the build.","title":"Deployments examples"},{"location":"deployment-ex/#deploy-zipkin-from-docker-image","text":"Install it, and expose it with a service oc new-app --docker-image=openzipkin/zipkin oc expose svc/zipkin A new route is created visible with oc get routes . Once the hostname is added to a DNS or /etc/hosts. See zipkin architecture article here","title":"Deploy zipkin from docker image"},{"location":"deployment-ex/#deploy-db2","text":"The Community edition DB2 image on dockerhub . It includes a predefined DB. Clone the DB2 repository to get the helm chart. See readme in this repo, for installation using helm but tiller needs to be installed before. The repository includes a script: db2u-install Create your own docker image with a shell to create the schema: FROM ibmcom/db2 RUN mkdir /var/custom COPY createschema.sh /var/custom RUN chmod a+x /var/custom/createschema.sh Deinstalling configuration oc delete -n jbsandbox sa/db2u role/db2u-role rolebinding/db2u-rolebinding","title":"Deploy DB2"},{"location":"deployment-ex/#deploy-rabbitmq","text":"","title":"Deploy RabbitMQ"},{"location":"deployment-ex/#first-install-operator","text":"To create a RabbitMQ instance, a RabbitmqCluster resource definition must be created and applied. RabbitMQ Cluster Kubernetes Operator creates the necessary resources, such as Services and StatefulSet, in the same namespace in which the RabbitmqCluster CRD was defined. See some instructions here . Be sure to have kubectl >= 1.14 git clone http://github.com/rabbitmq/cluster-operator.git cd cluster-operator kubectl create -f config/namespace/base/namespace.yaml kubectl create -f config/crd/bases/rabbitmq.com_rabbitmqclusters.yaml # Add cluster roles and roles: rabbitmq-cluster-operator-role and rabbitmq-cluster-leader-election-role kubectl -n rabbitmq-system create --kustomize config/rbac/ kubectl -n rabbitmq-system create --kustomize config/manager/ # Verify CRD installed kubectl get customresourcedefinitions.apiextensions.k8s.io | grep rabbit # Verify the service account oc get sa rabbitmq-cluster-operator # link the service account to security policy oc adm policy add-scc-to-user privileged rabbitmq-cluster-operator If there is an error about user 1000 not in range, change the deployment yaml file for the value securityContext.runAsUser: 1000570000 to a value in range and remove the other runAsGroup and fsGroup.","title":"First install operator"},{"location":"deployment-ex/#create-a-cluster","text":"when the operator pod runs, create one instance create a yaml file: apiVersion: rabbitmq.com/v1beta1 kind: RabbitmqCluster metadata: name: eda-rabbitmq spec: replicas: 1 Then do oc apply -f rabbit-cluster.yaml oc get rabbitmqclusters If an error like: \"services \\\"eda-rabbitmq-rabbitmq-headless\\\" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on\" happens, do the following:","title":"Create a cluster"},{"location":"deployment-ex/#deploy-sparks","text":"Using the operator, see this note","title":"Deploy sparks"},{"location":"deployment-ex/#mongodb","text":"Define an env file with the following variables: MONGODB_USER=mongo MONGODB_PASSWORD=mongo MONGODB_DATABASE=reeferdb MONGODB_ADMIN_PASSWORD=password then run: oc new-app --env-file=mongo.env --docker-image=openshift/mongodb-24-centos7 See the service: oc describe svc mongodb-24-centos7 For more detail see this note","title":"Mongodb"},{"location":"deployment-ex/#deploy-jupyter-lab","text":"See this note to deploy Jupyter lab lastest image to Openshift using the Deploy Image choice. The deployment is done under the project reefer-shipment-solution The environment variable needs to be set to get Jupyter lab. It takes multiple minutes to deploy. For the permission error due to the jovyan user not known, the command was: oc adm policy add-scc-to-user anyuid -z default -n reefer-shipment-solution oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD jupyterlab jupyterlab-reefer-shipment-solution.greencluster-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud all-spark-notebook 8888-tcp None Get the secuity token to login in via the pod logs. oc get pods NAME READY STATUS RESTARTS AGE all-spark-notebook-2-z4dqx oc logs all-spark-notebook-2-z4dqx To avoid loosing the work on the notebook, we need to add PVC to /home/jovyan/work mount point oc get dc oc set volume dc/all-spark-notebook --add --mount-path /home/jovyan/work --claim-size=1G","title":"Deploy Jupyter lab"},{"location":"deployment-ex/#deploy-helm-tiller-deprecated","text":"The goal is to install Tiller server on its own project, and grant it permissions to one or more other projects where Helm Charts will be installed. See the instructions in this blog . Here is a quick summary of the commands performed oc new-project tiller oc project tiller export TILLER_NAMESPACE=tiller oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE=\"${TILLER_NAMESPACE}\" -p HELM_VERSION=v2.16.4 | oc create -f - Once deployed and Tiller server running, create a new project and grant tiller edit role to access this new project, and then use helm CLI to deploy the app: oc new-project myapp oc policy add-role-to-user edit \"system:serviceaccount: ${ TILLER_NAMESPACE } :tiller\"","title":"Deploy helm / tiller (DEPRECATED)"},{"location":"docker/","text":"Some docker and docker compose tricks Dockerfile ENTRYPOINT specifies the default command to execute when the image runs in a container CMD provides the default arguments for the ENTRYPOINT instruction Example to build a custom Apache Web Server container image. FROM ubi7/ubi:7.7 ENV PORT 8080 RUN yum install -y httpd && yum clean all RUN sed -ri -e \"/^Listen 80/c\\Listen ${ PORT } \" /etc/httpd/conf/httpd.conf && \\ chown -R apache:apache /etc/httpd/logs/ && \\ chown -R apache:apache /run/httpd/ USER apache EXPOSE ${PORT} COPY ./src/ /var/www/html/ CMD [ \"httpd\" , \"-D\" , \"FOREGROUND\" ] Best practices for writing Dockerfiles Run a ubuntu image This could be a good approach to demonstrate a linux based. docker run --name my-linux --detach ubuntu:20.04 tail -f /dev/null # connect docker exec -ti my-linux bash # Update apt inventory and install jdk, maven, git, curl, ... apt update apt install -y openjdk-11-jre-headless maven git curl vim This project includes a Dockerfile-ubuntu to build a local image with the above tools. docker build -t jbcodeforce/myubuntu:20.04 -f Dockerfile-ubuntu . Docker volume For mounting host directory, the host directory needs to be configured with ownership and permissions allowing access to the container. docker run -v /var/dbfiles:/var/lib/mysql rhmap47/mysql Reclaim disk space docker system df (https://rmoff.net/post/what-to-do-when-docker-runs-out-of-space/) Docker network docker network list # create a network docker network create kafkanet # Assess which network a container is connected to docker inspect 71582654b2f4 -f \"{{json .NetworkSettings.Networks }}\" > \"bridge\" : { \"IPAMConfig\" :null, \"Links\" :null, \"Aliases\" :null, \"NetworkID\" : \"7db...\" # disconnect a container to its network docker network disconnect bridge 71582654b2f4 # Connect an existing container to a network docker network connect docker_default containernameorid Start a docker bypassing entry point or cmd docker run -ti --entrypoint \"/bin/bash\" imagename or use the command after the image name: docker run -ti imagename /bin/bash Docker build image with tests and env variables Inject the environment variables with --build-arg docker build --network host \\ --build-arg KAFKA_BROKERS = ${ KAFKA_BROKERS } \\ --build-arg KAFKA_APIKEY = ${ KAFKA_APIKEY } \\ --build-arg POSTGRESQL_URL = ${ POSTGRESQL_URL } \\ --build-arg POSTGRESQL_USER = ${ POSTGRESQL_USER } \\ --build-arg POSTGRESQL_PWD = ${ POSTGRESQL_PWD } \\ --build-arg JKS_LOCATION = ${ JKS_LOCATION } \\ --build-arg TRUSTSTORE_PWD = ${ TRUSTSTORE_PWD } \\ --build-arg POSTGRESQL_CA_PEM = \" ${ POSTGRESQL_CA_PEM } \" -t ibmcase/ $kname . Define a docker compose to run python env","title":"Docker tricks"},{"location":"docker/#some-docker-and-docker-compose-tricks","text":"","title":"Some docker and docker compose tricks"},{"location":"docker/#dockerfile","text":"ENTRYPOINT specifies the default command to execute when the image runs in a container CMD provides the default arguments for the ENTRYPOINT instruction Example to build a custom Apache Web Server container image. FROM ubi7/ubi:7.7 ENV PORT 8080 RUN yum install -y httpd && yum clean all RUN sed -ri -e \"/^Listen 80/c\\Listen ${ PORT } \" /etc/httpd/conf/httpd.conf && \\ chown -R apache:apache /etc/httpd/logs/ && \\ chown -R apache:apache /run/httpd/ USER apache EXPOSE ${PORT} COPY ./src/ /var/www/html/ CMD [ \"httpd\" , \"-D\" , \"FOREGROUND\" ] Best practices for writing Dockerfiles","title":"Dockerfile"},{"location":"docker/#run-a-ubuntu-image","text":"This could be a good approach to demonstrate a linux based. docker run --name my-linux --detach ubuntu:20.04 tail -f /dev/null # connect docker exec -ti my-linux bash # Update apt inventory and install jdk, maven, git, curl, ... apt update apt install -y openjdk-11-jre-headless maven git curl vim This project includes a Dockerfile-ubuntu to build a local image with the above tools. docker build -t jbcodeforce/myubuntu:20.04 -f Dockerfile-ubuntu .","title":"Run a ubuntu image"},{"location":"docker/#docker-volume","text":"For mounting host directory, the host directory needs to be configured with ownership and permissions allowing access to the container. docker run -v /var/dbfiles:/var/lib/mysql rhmap47/mysql","title":"Docker volume"},{"location":"docker/#reclaim-disk-space","text":"docker system df (https://rmoff.net/post/what-to-do-when-docker-runs-out-of-space/)","title":"Reclaim disk space"},{"location":"docker/#docker-network","text":"docker network list # create a network docker network create kafkanet # Assess which network a container is connected to docker inspect 71582654b2f4 -f \"{{json .NetworkSettings.Networks }}\" > \"bridge\" : { \"IPAMConfig\" :null, \"Links\" :null, \"Aliases\" :null, \"NetworkID\" : \"7db...\" # disconnect a container to its network docker network disconnect bridge 71582654b2f4 # Connect an existing container to a network docker network connect docker_default containernameorid","title":"Docker network"},{"location":"docker/#start-a-docker-bypassing-entry-point-or-cmd","text":"docker run -ti --entrypoint \"/bin/bash\" imagename or use the command after the image name: docker run -ti imagename /bin/bash","title":"Start a docker bypassing entry point or cmd"},{"location":"docker/#docker-build-image-with-tests-and-env-variables","text":"Inject the environment variables with --build-arg docker build --network host \\ --build-arg KAFKA_BROKERS = ${ KAFKA_BROKERS } \\ --build-arg KAFKA_APIKEY = ${ KAFKA_APIKEY } \\ --build-arg POSTGRESQL_URL = ${ POSTGRESQL_URL } \\ --build-arg POSTGRESQL_USER = ${ POSTGRESQL_USER } \\ --build-arg POSTGRESQL_PWD = ${ POSTGRESQL_PWD } \\ --build-arg JKS_LOCATION = ${ JKS_LOCATION } \\ --build-arg TRUSTSTORE_PWD = ${ TRUSTSTORE_PWD } \\ --build-arg POSTGRESQL_CA_PEM = \" ${ POSTGRESQL_CA_PEM } \" -t ibmcase/ $kname .","title":"Docker build image with tests and env variables"},{"location":"docker/#define-a-docker-compose-to-run-python-env","text":"","title":"Define a docker compose to run python env"},{"location":"faq/","text":"OpenShift FAQ Access To get connection details from the config map in the kube-public namespace: kubectl get cm ibmcloud-cluster-info -n kube-public -o yaml The cluster_address value for the master address, and the cluster_router_https_port for the port number. How to get a token for login As soon as a service account is created, two secrets are automatically added to it: an API token credentials for the OpenShift Container Registry To access the token you can go to the Openshift container platform web console, and select the user icon on the top right and the command: copy login command you should have the token. oc login -u apikey -p I173tzup --server=https://c2-e.us-east.containers.cloud.ibm.com:21070 Strange message from login Some oc login command may return a strange message: error: invalid character '<' looking for beginning of value . This is due to the fact that the response is a HTML page. This is a problem of server URL. The Server parameter has to correspond to your OpenShift API server endpoint. When deploy on premise be sure to use the k8s master URL. Login and push image to private registry OpenShift could manage its own image private registry service. The default name and URL is docker-registry-default.apps.... See the product documentation here to install it. Below is the step to push a docker images Login to OpenShift cluster (get secure token from OpenShift console) oc login ... If not done before add registry-viewer role to your user: oc policy add-role-to-user registry-viewer $(oc whoami) and oc policy add-role-to-user registry-editor $(oc whoami) Look up the internal OpenShift Docker registry address by using the following command: kubectl get routes docker-registry -n default Login to docker registry: docker login -u john -p $(oc whoami -t) docker-registry-default.apps.green-with-envy.ocp.csplab.local If you get this message: Error response from daemon: Get https://docker-registry-default.apps.green-with-envy.ocp.csplab.local/v2/: x509: certificate signed by unknown authority , add the certificate to the docker client certificates: * Get the certificate: `oc extract -n default secrets/registry-certificates --keys=registry.crt` * Put the certificate in `~/.docker/certs.d/docker-registry-default.apps.green-with-envy.ocp.csplab.local` * Restart docker desktop Tag the image with registry name: docker tag ibmcase/kc-ordercommandms docker-registry-default.apps.green-with-envy.ocp.csplab.local/reefershipmentsolution/kc-ordercommandms Push the image docker push docker-registry-default.apps.green-with-envy.ocp.csplab.local/reefershipmentsolution/kc-ordercommandms Accessing the registry console: https://registry-console-default.apps.green-with-envy.ocp.csplab.local/ Generate deployment.yaml and services.yaml from helm templates: helm template --set image.repository=docker-registry.default.svc:5000/reefershipmentsolution/kc-ordercommandms --set kafka.brokersConfigMap=kafka-brokers --set eventstreams.enabled=true --set eventstreams.apikeyConfigMap=eventstreams-apikey --set serviceAccountName=kcontainer-runtime --namespace reefershipmentsolution --output-dir templates chart/ordercommandms Refresh an existing pod with the new image oc delete -f templates/ordercommandms/templates/ oc apply -f templates/ordercommandms/templates/ Deployment Deploy any docker image Just reference the docker image name from the dockerhub public repository oc new-app busybox For mongodb for example: oc new-app --env-file=mongo.env --docker-image=openshift/mongodb-24-centos7 Copy a file to an existing running container oc rsync $(pwd) my-connect-connect-54485b7896-k5lsj:/tmp oc rsh my-connect-connect-54485b7896-k5lsj ls /tmp How to setup TLS/SSL certificate The approach is to use secret and mounted volume to inject the SSL certifcate file so the nodejs or python app can use it to connect over TLS. If you have the key and cert certificates as remoteapptoaccess.key and remoteappaccess.crt, you may need to encode them with base64: $ base64 remoteapptoaccess.keys LS0934345DE.... $ base64 remoteapptoaccess.crt SUPERSECRETLONGSTRINGINBASE64FORMAT So then create a TLS secret descriptor for kubernetes: apiVersion : v1 kind : Secret metadata : name : remoteapp-tls-secret type : Opaque data : remoteapptoaccess.key : LS0934345DE... remoteapptoaccess.crt : SUPERSERCRETLONGSTRINGINBASE64FORMAT If you only have the crt file, you just define the data for it. In the client app deployment.yaml set a mount point: (the deployment is not complete, there are missing arguments linked to the app itself) apiVersion : apps/v1 kind : Deployment metadata : labels : app : clientApp name : clientApp spec : replicas : 1 spec : containers : - image : yournamespace/imagename name : clientapp volumeMounts : - mountPath : \"/client/path/inside/container/ssl\" name : ssl-path readOnly : true ports : - containerPort : 80 volumes : - name : ssl-path secret : secretName : remoteapp-tls-secret This declaration will add two files (remoteapptoaccess.key, remoteapptoaccess.crt) under the /client/path/inside/container/ssl folder.","title":"FAQ"},{"location":"faq/#openshift-faq","text":"","title":"OpenShift FAQ"},{"location":"faq/#access","text":"To get connection details from the config map in the kube-public namespace: kubectl get cm ibmcloud-cluster-info -n kube-public -o yaml The cluster_address value for the master address, and the cluster_router_https_port for the port number.","title":"Access"},{"location":"faq/#how-to-get-a-token-for-login","text":"As soon as a service account is created, two secrets are automatically added to it: an API token credentials for the OpenShift Container Registry To access the token you can go to the Openshift container platform web console, and select the user icon on the top right and the command: copy login command you should have the token. oc login -u apikey -p I173tzup --server=https://c2-e.us-east.containers.cloud.ibm.com:21070","title":"How to get a token for login"},{"location":"faq/#strange-message-from-login","text":"Some oc login command may return a strange message: error: invalid character '<' looking for beginning of value . This is due to the fact that the response is a HTML page. This is a problem of server URL. The Server parameter has to correspond to your OpenShift API server endpoint. When deploy on premise be sure to use the k8s master URL.","title":"Strange message from login"},{"location":"faq/#login-and-push-image-to-private-registry","text":"OpenShift could manage its own image private registry service. The default name and URL is docker-registry-default.apps.... See the product documentation here to install it. Below is the step to push a docker images Login to OpenShift cluster (get secure token from OpenShift console) oc login ... If not done before add registry-viewer role to your user: oc policy add-role-to-user registry-viewer $(oc whoami) and oc policy add-role-to-user registry-editor $(oc whoami) Look up the internal OpenShift Docker registry address by using the following command: kubectl get routes docker-registry -n default Login to docker registry: docker login -u john -p $(oc whoami -t) docker-registry-default.apps.green-with-envy.ocp.csplab.local If you get this message: Error response from daemon: Get https://docker-registry-default.apps.green-with-envy.ocp.csplab.local/v2/: x509: certificate signed by unknown authority , add the certificate to the docker client certificates: * Get the certificate: `oc extract -n default secrets/registry-certificates --keys=registry.crt` * Put the certificate in `~/.docker/certs.d/docker-registry-default.apps.green-with-envy.ocp.csplab.local` * Restart docker desktop Tag the image with registry name: docker tag ibmcase/kc-ordercommandms docker-registry-default.apps.green-with-envy.ocp.csplab.local/reefershipmentsolution/kc-ordercommandms Push the image docker push docker-registry-default.apps.green-with-envy.ocp.csplab.local/reefershipmentsolution/kc-ordercommandms Accessing the registry console: https://registry-console-default.apps.green-with-envy.ocp.csplab.local/ Generate deployment.yaml and services.yaml from helm templates: helm template --set image.repository=docker-registry.default.svc:5000/reefershipmentsolution/kc-ordercommandms --set kafka.brokersConfigMap=kafka-brokers --set eventstreams.enabled=true --set eventstreams.apikeyConfigMap=eventstreams-apikey --set serviceAccountName=kcontainer-runtime --namespace reefershipmentsolution --output-dir templates chart/ordercommandms Refresh an existing pod with the new image oc delete -f templates/ordercommandms/templates/ oc apply -f templates/ordercommandms/templates/","title":"Login and push image to private registry"},{"location":"faq/#deployment","text":"","title":"Deployment"},{"location":"faq/#deploy-any-docker-image","text":"Just reference the docker image name from the dockerhub public repository oc new-app busybox For mongodb for example: oc new-app --env-file=mongo.env --docker-image=openshift/mongodb-24-centos7","title":"Deploy any docker image"},{"location":"faq/#copy-a-file-to-an-existing-running-container","text":"oc rsync $(pwd) my-connect-connect-54485b7896-k5lsj:/tmp oc rsh my-connect-connect-54485b7896-k5lsj ls /tmp","title":"Copy a file to an existing running container"},{"location":"faq/#how-to-setup-tlsssl-certificate","text":"The approach is to use secret and mounted volume to inject the SSL certifcate file so the nodejs or python app can use it to connect over TLS. If you have the key and cert certificates as remoteapptoaccess.key and remoteappaccess.crt, you may need to encode them with base64: $ base64 remoteapptoaccess.keys LS0934345DE.... $ base64 remoteapptoaccess.crt SUPERSECRETLONGSTRINGINBASE64FORMAT So then create a TLS secret descriptor for kubernetes: apiVersion : v1 kind : Secret metadata : name : remoteapp-tls-secret type : Opaque data : remoteapptoaccess.key : LS0934345DE... remoteapptoaccess.crt : SUPERSERCRETLONGSTRINGINBASE64FORMAT If you only have the crt file, you just define the data for it. In the client app deployment.yaml set a mount point: (the deployment is not complete, there are missing arguments linked to the app itself) apiVersion : apps/v1 kind : Deployment metadata : labels : app : clientApp name : clientApp spec : replicas : 1 spec : containers : - image : yournamespace/imagename name : clientapp volumeMounts : - mountPath : \"/client/path/inside/container/ssl\" name : ssl-path readOnly : true ports : - containerPort : 80 volumes : - name : ssl-path secret : secretName : remoteapp-tls-secret This declaration will add two files (remoteapptoaccess.key, remoteapptoaccess.crt) under the /client/path/inside/container/ssl folder.","title":"How to setup TLS/SSL certificate"},{"location":"knative/","text":"Knative Knative is Kubernetes based platform to develop serverless. Major value proposition is a simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. Other interesting characteristics: adjust the traffic distribution amongst the service revisions, to support blue/green deployment and canary release able to scale to zero : After a defined time of idleness (the so called stable-window) a revision is considered inactive. Now, all routes pointing to the now inactive revision will be pointed to the so-called activator. By default the scale-to-zero-grace-period is 30s, and the stable-window is 60s. auto-scaling : By default Knative Serving allows 100 concurrent requests into a pod. This is defined by the container-concurrency-target-default setting in the configmap config-autoscaler in the knative-serving namespace. Knative consists of the following components: Eventing - Management and delivery of events Serving - Request-driven compute that can scale to zero Redhat knative cookbook . Value propositions Like any serverless (AWS lambda, openwhisk..) the benefits are: instantly spin up enough server resources to handle tens of thousands of incoming requests more expensive on compute time, but cost zero after spin up a completely separate version of the site to do some prototyping, no need to worry about forgetting a test running forever No Kubernetes configurations, no load balancers, no auto scaling rules Challenges (aws lambda that may apply to knative) web socket support, SMTP session With AWS no control of the OS, so difficult to bring your libraries. Knative as container fixes this. AWS: functions with less TAM have slower CPU speed. This can swing a lot between 5ms to 500ms response time. Charged by 100ms increment Using SPA app like React or Angular, the page will be downloaded from a Content CDN server, which leads to have a delay between this web server and the function. If you want <50ms response times, then hosting your backend behind API Gateway is not for function. Need non serverless services like VPC, NAT gateways to access other service like storage. to get a granular record of what Lambdas are running, how many times and for how long, measurement and Logging products are not serverless, so cost a lot. Specially as you may not control the information sent to the logger (Cloudwatch). Idempotence: AWS Lambda, can execute the same requests more than once. Which will generate multiple records written into the DB. This is due to retries on errors, and event source set at least once delivery. Use request identifiers to implement idempotent Lambda functions that do not break if a function invocation needs to be retried. Make sure to pass the original request id to all subsequent calls. The original request id is generated as early as possible in the chain. If possible on the client side. Avoid generating your own ids. Time limit on execution to prevent deadlocked function. Difficult to see those functions as they disappear. COLD start: if your function hasn\u2019t been run in a while, a pending request needs to wait for it to be initialized before it can be served: 3 to 5 seconds. nothing the end-user touches could be hosted on Lambda after all view Lambda mainly in terms of what it can do as a high-end parallel architecture Getting started Install Knative CLI with brew . It will be accessible via kn Or use Knative CLI running in docker, see doc here To prepare OpenShift see the serverless install instructions . The minimum requirement to use OpenShift Serverless is a cluster with 10 CPUs and 40GB memory. Use operator hub to install Knative serving and eventing servers and brokers. OpenShift Serverless Operator eventually shows up and its Status ultimately resolves to InstallSucceeded in the openshift-operators namespace. Creating the knative-serving namespace: oc create namespace knative-serving , and then within the project itself, create an instance. Verify the conditions: oc get knativeserving.operator.knative.dev/knative-serving -n knative-serving --template='{{range .status.conditions}}{{printf \"%s=%s\\n\" .type .status}}{{end}}' You should get: DependenciesInstalled=True DeploymentsAvailable=True InstallSucceeded=True Ready=True * With a yaml file: and oc apply it. apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 2 *We can do the same for knative-eventing: create a namespace and then an instance using the serverless operator. * Verify with: oc get knativeeventing.operator.knative.dev/knative-eventing -n knative-eventing --template='{{range .status.conditions}}{{printf \"%s=%s\\n\" .type .status}}{{end}}' Define a service for a given image oc apply the following definition: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : greeter spec : template : metadata : name : greeter-v2 spec : containers : - image : quay.io/rhdevelopers/knative-tutorial-greeter:quarkus livenessProbe : httpGet : path : /healthz readinessProbe : httpGet : path : /healthz Get the detail of the configuration: oc get configurations.serving.knative.dev greeter Knative and Quarkus app Be sure to have the kubernetes or openshit plugin: ./mvnw quarkus:add-extension -Dextensions=\"openshift\" Add the following property: quarkus.kubernetes.deployment-target = knative When doing mvn package a knative.yaml file is created under target/kubernetes Other example of creating deployment: mvn -Dcontainer.registry.url='https://index.docker.io/v1/' \\ > -Dcontainer.registry.user='jbcodefore' \\ > -Dcontainer.registry.password='XXXXXXXYYYYYYYZZZZZZZZ' \\ > -Dgit.source.revision='master' \\ > -Dgit.source.repo.url='https://github.com/quarkusio/quarkus-quickstarts.git' \\ > -Dapp.container.image='quay.io/jbcodefore/quarkus-greetings' package This command creates the resource files in target/kubernetes directory Deploy the service oc apply --recursive --filename target/kubernetes/ Some RedHat article . Knative Eventing Some CLI commands # get revisions for a service oc get rev --selector = serving.knative.dev/service = greeter --sort-by = \"{.metadata.creationTimestamp}\" oc delete services.serving.knative.dev greeter # create a service from image kn service create greeter --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus # create a revision kn service update greeter --env \"MESSAGE_PREFIX=Namaste\" kn service describe greeter kn service delete greeter # get revisions kn revision list kn revision describe greeter-xhwyt-1 # route kn route list Blue/green deployment Knative offers a simple way of switching 100% of the traffic from one Knative service revision (blue) to another newly rolled out revision (green). By default new revision receives 100% of the new traffic. To rollback we need to create a yaml with the template.metadata.name to the expected revision and add the following spec.traffic traffic : - tag : v1 revisionName : greeter-v1 percent : 100 - tag : v2 revisionName : greeter-v2 percent : 0 - tag : latest latestRevision : true percent : 0 Canary release Knative allows you to split the traffic between revisions traffic : - tag : v1 revisionName : greeter-v1 percent : 80 - tag : v2 revisionName : greeter-v2 percent : 20 - tag : latest latestRevision : true percent : 0 Knative eventing There are three primary usage patterns with Knative Eventing: Source to Sink : It provides single Sink\u2009\u2014\u2009that is, event receiving service --, with no queuing, back-pressure, and filtering. The Source to Service does not support replies, which means the response from the Sink service is ignored Channel and subscription : the Knative Eventing system defines a Channel, which can connect to various backends such as In-Memory, Kafka and GCP PubSub for sourcing the events. Each Channel can have one or more subscribers in the form of Sink services Broker and Trigger : supports filtering of events so subscribers specify interest on certain set of messages that flows into the Broker. For each Broker, Knative Eventing will implicitly create a Knative Eventing Channel. The Trigger gets itself subscribed to the Broker and applies the filter on the messages on its subscribed broker. The filters are applied on the on the Cloud Event attributes of the messages, before delivering it to the interested Sink Services(subscribers). To make a project using knative eventing label the namespace with: kubectl label namespace jbsandbox knative-eventing-injection=enabled . This will start the filter and ingress pods. Broker and Trigger See this tutorial for basics trigger and services. This tutorial defines filtering on CloudEvent type named greeting so the two different services can subscribe to. When eventing is set the filter and ingress pods are started. To get the address of the broker url: oc get broker default -o jsonpath='{.status.address.url}' Then the approach is to create different sinks, define triggers for each sink on what kind of event attribute to subscribe too, so filtering can occur. The sink will respond depending of the cloud event 'type' attribute for example. Troubleshooting Source of knowledge Debugging issues with your application . Revision failed oc get configurations.serving.knative.dev item-kafka-producer NAME LATESTCREATED LATESTREADY READY REASON item-kafka-producer item-kafka-producer-65kbv False RevisionFailed","title":"Knative"},{"location":"knative/#knative","text":"Knative is Kubernetes based platform to develop serverless. Major value proposition is a simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. Other interesting characteristics: adjust the traffic distribution amongst the service revisions, to support blue/green deployment and canary release able to scale to zero : After a defined time of idleness (the so called stable-window) a revision is considered inactive. Now, all routes pointing to the now inactive revision will be pointed to the so-called activator. By default the scale-to-zero-grace-period is 30s, and the stable-window is 60s. auto-scaling : By default Knative Serving allows 100 concurrent requests into a pod. This is defined by the container-concurrency-target-default setting in the configmap config-autoscaler in the knative-serving namespace. Knative consists of the following components: Eventing - Management and delivery of events Serving - Request-driven compute that can scale to zero Redhat knative cookbook .","title":"Knative"},{"location":"knative/#value-propositions","text":"Like any serverless (AWS lambda, openwhisk..) the benefits are: instantly spin up enough server resources to handle tens of thousands of incoming requests more expensive on compute time, but cost zero after spin up a completely separate version of the site to do some prototyping, no need to worry about forgetting a test running forever No Kubernetes configurations, no load balancers, no auto scaling rules","title":"Value propositions"},{"location":"knative/#challenges-aws-lambda-that-may-apply-to-knative","text":"web socket support, SMTP session With AWS no control of the OS, so difficult to bring your libraries. Knative as container fixes this. AWS: functions with less TAM have slower CPU speed. This can swing a lot between 5ms to 500ms response time. Charged by 100ms increment Using SPA app like React or Angular, the page will be downloaded from a Content CDN server, which leads to have a delay between this web server and the function. If you want <50ms response times, then hosting your backend behind API Gateway is not for function. Need non serverless services like VPC, NAT gateways to access other service like storage. to get a granular record of what Lambdas are running, how many times and for how long, measurement and Logging products are not serverless, so cost a lot. Specially as you may not control the information sent to the logger (Cloudwatch). Idempotence: AWS Lambda, can execute the same requests more than once. Which will generate multiple records written into the DB. This is due to retries on errors, and event source set at least once delivery. Use request identifiers to implement idempotent Lambda functions that do not break if a function invocation needs to be retried. Make sure to pass the original request id to all subsequent calls. The original request id is generated as early as possible in the chain. If possible on the client side. Avoid generating your own ids. Time limit on execution to prevent deadlocked function. Difficult to see those functions as they disappear. COLD start: if your function hasn\u2019t been run in a while, a pending request needs to wait for it to be initialized before it can be served: 3 to 5 seconds. nothing the end-user touches could be hosted on Lambda after all view Lambda mainly in terms of what it can do as a high-end parallel architecture","title":"Challenges (aws lambda that may apply to knative)"},{"location":"knative/#getting-started","text":"Install Knative CLI with brew . It will be accessible via kn Or use Knative CLI running in docker, see doc here To prepare OpenShift see the serverless install instructions . The minimum requirement to use OpenShift Serverless is a cluster with 10 CPUs and 40GB memory. Use operator hub to install Knative serving and eventing servers and brokers. OpenShift Serverless Operator eventually shows up and its Status ultimately resolves to InstallSucceeded in the openshift-operators namespace. Creating the knative-serving namespace: oc create namespace knative-serving , and then within the project itself, create an instance. Verify the conditions: oc get knativeserving.operator.knative.dev/knative-serving -n knative-serving --template='{{range .status.conditions}}{{printf \"%s=%s\\n\" .type .status}}{{end}}' You should get: DependenciesInstalled=True DeploymentsAvailable=True InstallSucceeded=True Ready=True * With a yaml file: and oc apply it. apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 2 *We can do the same for knative-eventing: create a namespace and then an instance using the serverless operator. * Verify with: oc get knativeeventing.operator.knative.dev/knative-eventing -n knative-eventing --template='{{range .status.conditions}}{{printf \"%s=%s\\n\" .type .status}}{{end}}'","title":"Getting started"},{"location":"knative/#define-a-service-for-a-given-image","text":"oc apply the following definition: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : greeter spec : template : metadata : name : greeter-v2 spec : containers : - image : quay.io/rhdevelopers/knative-tutorial-greeter:quarkus livenessProbe : httpGet : path : /healthz readinessProbe : httpGet : path : /healthz Get the detail of the configuration: oc get configurations.serving.knative.dev greeter","title":"Define a service for a given image"},{"location":"knative/#knative-and-quarkus-app","text":"Be sure to have the kubernetes or openshit plugin: ./mvnw quarkus:add-extension -Dextensions=\"openshift\" Add the following property: quarkus.kubernetes.deployment-target = knative When doing mvn package a knative.yaml file is created under target/kubernetes Other example of creating deployment: mvn -Dcontainer.registry.url='https://index.docker.io/v1/' \\ > -Dcontainer.registry.user='jbcodefore' \\ > -Dcontainer.registry.password='XXXXXXXYYYYYYYZZZZZZZZ' \\ > -Dgit.source.revision='master' \\ > -Dgit.source.repo.url='https://github.com/quarkusio/quarkus-quickstarts.git' \\ > -Dapp.container.image='quay.io/jbcodefore/quarkus-greetings' package This command creates the resource files in target/kubernetes directory Deploy the service oc apply --recursive --filename target/kubernetes/ Some RedHat article .","title":"Knative and Quarkus app"},{"location":"knative/#knative-eventing","text":"","title":"Knative Eventing"},{"location":"knative/#some-cli-commands","text":"# get revisions for a service oc get rev --selector = serving.knative.dev/service = greeter --sort-by = \"{.metadata.creationTimestamp}\" oc delete services.serving.knative.dev greeter # create a service from image kn service create greeter --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus # create a revision kn service update greeter --env \"MESSAGE_PREFIX=Namaste\" kn service describe greeter kn service delete greeter # get revisions kn revision list kn revision describe greeter-xhwyt-1 # route kn route list","title":"Some CLI commands"},{"location":"knative/#bluegreen-deployment","text":"Knative offers a simple way of switching 100% of the traffic from one Knative service revision (blue) to another newly rolled out revision (green). By default new revision receives 100% of the new traffic. To rollback we need to create a yaml with the template.metadata.name to the expected revision and add the following spec.traffic traffic : - tag : v1 revisionName : greeter-v1 percent : 100 - tag : v2 revisionName : greeter-v2 percent : 0 - tag : latest latestRevision : true percent : 0","title":"Blue/green deployment"},{"location":"knative/#canary-release","text":"Knative allows you to split the traffic between revisions traffic : - tag : v1 revisionName : greeter-v1 percent : 80 - tag : v2 revisionName : greeter-v2 percent : 20 - tag : latest latestRevision : true percent : 0","title":"Canary release"},{"location":"knative/#knative-eventing_1","text":"There are three primary usage patterns with Knative Eventing: Source to Sink : It provides single Sink\u2009\u2014\u2009that is, event receiving service --, with no queuing, back-pressure, and filtering. The Source to Service does not support replies, which means the response from the Sink service is ignored Channel and subscription : the Knative Eventing system defines a Channel, which can connect to various backends such as In-Memory, Kafka and GCP PubSub for sourcing the events. Each Channel can have one or more subscribers in the form of Sink services Broker and Trigger : supports filtering of events so subscribers specify interest on certain set of messages that flows into the Broker. For each Broker, Knative Eventing will implicitly create a Knative Eventing Channel. The Trigger gets itself subscribed to the Broker and applies the filter on the messages on its subscribed broker. The filters are applied on the on the Cloud Event attributes of the messages, before delivering it to the interested Sink Services(subscribers). To make a project using knative eventing label the namespace with: kubectl label namespace jbsandbox knative-eventing-injection=enabled . This will start the filter and ingress pods.","title":"Knative eventing"},{"location":"knative/#broker-and-trigger","text":"See this tutorial for basics trigger and services. This tutorial defines filtering on CloudEvent type named greeting so the two different services can subscribe to. When eventing is set the filter and ingress pods are started. To get the address of the broker url: oc get broker default -o jsonpath='{.status.address.url}' Then the approach is to create different sinks, define triggers for each sink on what kind of event attribute to subscribe too, so filtering can occur. The sink will respond depending of the cloud event 'type' attribute for example.","title":"Broker and Trigger"},{"location":"knative/#troubleshooting","text":"Source of knowledge Debugging issues with your application . Revision failed oc get configurations.serving.knative.dev item-kafka-producer NAME LATESTCREATED LATESTREADY READY REASON item-kafka-producer item-kafka-producer-65kbv False RevisionFailed","title":"Troubleshooting"},{"location":"notes-ocp-training/","text":"Notes on training DO180 student, which has the password student, root redhat Student workstation: workstation.lab.example.com http://rol.redhat.com Students also have access to a MySQL and a Nexus server hosted by either the OpenShift cluster or by AWS github.com jbcodeforce quay.io jbcodeforce Container technology Difference between container applications and traditional deployments The major drawback to traditionally deployed software application is that the application's dependencies are entangled with the runtime environment a traditionally deployed application must be stopped before updating the associated dependencies * complex systems to provide high availability A container is a set of one or more processes that are isolated from the rest of the system. * security, storage, and network isolation * isolate dependent libraries and run time resources * less resources than VM, start quickly. * helps with the efficiency, elasticity, and reusability of the hosted applications, and portability * [Open Container initiative](https://www.opencontainers.org/) Container image: bundle of files and metadata Container engine: Rocket, Drawbridge, LXC, Docker, and Podman Started in 2001 with VServer, then move to isolated process which leverages the linux features: * **Namespaces**: The kernel can isolate specific system resources, usually visible to all processes, by placing the resources within a namespace. Namespaces can include resources like network interfaces, the process ID list, mount points, IPC resources, and the system's host name information. * **cgroups**: Control groups partition sets of processes and their children into groups to manage and limit the resources they consume. * **Seccomp** defines a security profile for processes, whitelisting the system calls, parameters and file descriptors they are allowed to use * SELinux (Security-Enhanced Linux) is a mandatory access control system for processes. Protect processes from each other and to protect the host system from its running processes Example of Dockerfile OpenShift RHOCP adds the capabilities to provide a production PaaS platform such as remote management, multi tenancy, increased security, monitoring and auditing, application life-cycle management, and self-service interfaces for developers. Username RHT_OCP4_DEV_USER boyerje-us Password RHT_OCP4_DEV_PASSWORD <> API Endpoint RHT_OCP4_MASTER_API https://api.ocp-na2.prod.nextcle.com:6443 Console Web Application https://console-openshift-console.apps.ocp-na2.prod.nextcle.com Cluster ID ... Container images are named based on the following syntax: registry_name/user_name/image_name:tag Example of images used: # login to a registry sudo podman login -u username -p password registry.access.redhat.com sudo podman run ubi7/ubi:7.7 echo \"Hello!\" # Apache http server sudo podman run -d rhscl/httpd-24-rhel7:2.4-36.8 # Get IP address of a last container started podman inspect -l -f \"{{.NetworkSettings.IPAddress}}\" # mysql sudo podman run --name mysql-basic -v /var/local/mysql:/var/lib/mysql/data \\ > -e MYSQL_USER = user1 -e MYSQL_PASSWORD = mypa55 \\ > -e MYSQL_DATABASE = items -e MYSQL_ROOT_PASSWORD = r00tpa55 \\ > -d rhscl/mysql-57-rhel7:5.7-3.14 # Stop and start a container sudo podman stop my-httpd-container sudo podman restart my-httpd-container # Send a SIGKILL sudo podman kill my-httpd-container Quay.io introduces several features, such as server-side image building, fine-grained access controls, and automatic scanning of images for known vulnerabilities. To configure registries for the podman command, you need to update the /etc/containers/registries.conf . The podman search command finds images from all the registries listed in this file. [registries.search] registries = [\"registry.access.redhat.com\", \"quay.io\"] Use an FQDN and port number (5000 default) to identify a registry. By default, Podman stores container images in the /var/lib/containers/storage/overlay-images directory. Existing images from the Podman local storage can be saved to a .tar file using the podman save command. # Retrieve the list of external files and directories that Podman mounts to the running container sudo podman inspect -f \"{{range .Mounts}}{{println .Destination}}{{end}}\" official-httpd # list of modified files in the container file system sudo podman diff official-httpd # Commit the changes to a new container image with a new name sudo podman commit -a 'Jerome' official-httpd do180-custom-httpd sudo podman save [ -o FILE_NAME ] IMAGE_NAME [ :TAG ] sudo podman tag quay.io/jbcodeforce/do180-custom-httpd:v1.0 sudo podman push quay.io/jbcodeforce/do180-custom-httpd:v1.0 sudo podman build -t NAME:TAG DIR # examine the content of the environment variable of a container sudo podman exec todoapi env Red Hat Software Collections Library is the source of most container images Deploying Containerized Applications on OpenShift Lab: # login to cluster oc login -u ${ RHT_OCP4_DEV_USER } -p ${ RHT_OCP4_DEV_PASSWORD } ${ RHT_OCP4_MASTER_API } # Create a new project named \"youruser-ocp\" oc new-project ${ RHT_OCP4_DEV_USER } -ocp # Create a temperature converter application written in PHP using the php:7.1 image stream tag. The source code is in the Git repository at https://github.com/RedHatTraining/DO180-apps/ c new-app php:7.1~https://github.com/RedHatTraining/DO180-apps --context-dir temps --name temps # Monitor progress of the build oc logs -f bc/temps # Verify that the application is deployed. oc get pods -w # Expose the temps service to create an external route for the application. oc expose svc/temps # Determine route URL oc get route/temps Deploy multi-container app Podman uses Container Network Interface (CNI) to create a software-defined network (SDN) between all containers in the host. Unless stated otherwise, CNI assigns a new IP address to a container when it starts. Each container exposes all ports to other containers in the same SDN. As such, services are readily accessible within the same network. The containers expose ports to external networks only by explicit configuration. Using environment variables allows you to share information between containers with Podman. However, there are still some limitations and some manual work involved in ensuring that all environment variables stay in sync, especially when working with many containers Any service defined on Kubernetes generates environment variables for the IP address and port number where the service is available. Kubernetes automatically injects these environment variables into the containers from pods in the same namespace Get the list of templates from where to create k8s resources like Secret, a Service, a PersistentVolumeClaim, and a DeploymentConfig: oc get template -n openshift and then from one of the template: oc get template mysql-persistent -n openshift -o yaml . With template, you can publish a new template to the OpenShift cluster so that other developers can build an application from the template. Get the parameters of a template: process --parameters mysql-persistent -n openshift . From the template create the app resources file and then deploy the app: # first export the template oc get template mysql-persistent -o yaml -n openshift > mysql-persistent-template.yaml # identify appropriate values for the template parameters and process the template oc process -f mysql-persistent-template.yaml -p MYSQL_USER = dev -p MYSQL_PASSWORD = $P4SSD -p MYSQL_DATABASE = bank -p VOLUME_CAPACITY = 10Gi | oc create -f - # or using new-app oc new-app --template = mysql-persistent -p MYSQL_USER = dev -p MYSQL_PASSWORD = $P4SSD -p MYSQL_DATABASE = bank -p VOLUME_CAPACITY = 10Gi Lab: # login to cluster oc login -u ${ RHT_OCP4_DEV_USER } -p ${ RHT_OCP4_DEV_PASSWORD } ${ RHT_OCP4_MASTER_API } # Create a new project named \"youruser-\" oc new-project ${ RHT_OCP4_DEV_USER } -deploy # Build the MySQL Database image sudo podman build -t do180-mysql-57-rhel7 . # Push the MySQL image to the your Quay.io repository. sudo podman login quay.io -u ${ RHT_OCP4_QUAY_USER } sudo podman tag do180-mysql-57-rhel7 quay.io/ ${ RHT_OCP4_QUAY_USER } /do180-mysql-57-rhel7 sudo podman push quay.io/ ${ RHT_OCP4_QUAY_USER } /do180-mysql-57-rhel7 Troubleshooting S2I The S2I image creation process is composed of the build (The BuildConfig (BC) OpenShift resources drive it) and deploy ( The DeploymentConfig (OpenShift resources) steps. retrieve the logs from a build configuration: oc logs bc/<application-name> request a new build oc start-build <application-name> Deployment logs: oc logs dc/<appname> If a container could not access a file system, it may come to the container running under a specific user so we need to authorize by : oc adm policy add-scc-to-user anyuid -z default . Make sure the ownership and permissions of the directory are set according to the USER directive in the Dockerfile that was used to build the container being deployed To avoid file system permission issues, local folders used for container volume mounts must satisfy the following: * The user executing the container processes must be the owner of the folder * The local folder must satisfy the SELinux requirements to be used as a container volume. Assign the container_file_t group to the folder by using the semanage fcontext -a -t container_file_t command, then refresh the permissions with the restorecon -R command. * Run the oc adm prune command for an automated way to remove obsolete images and other resources. Containerized applications To connect to a admin console of a pod, we can use: oc port-forward for forwarding a local port to a pod port. If the image enable remote debugging by exposing a port number, then port-forwarding, will let the IDE access the JVM to debug step by step via the Java Debug Wire Protocol (JDWP). Use openshift events to see the problem at a higher level: oc get events . To add tools like ping, telnet, iputils, dig ... into a pod, just mount local file to volume and directory like /bin, /sbin, /lib... To copy file to or from a container, podman cp is useful, or use exec: sudo podman exec -i <container> mysql -uroot -proot < /path/on/host/db.sql < db.sql # or sudo podman exec -it <containerName> sh -c 'exec mysqldump -h\"$MYSQL_PORT_3306_TCP_ADDR\" \\ -P\"$MYSQL_PORT_3306_TCP_PORT\" -uroot -p\"$MYSQL_ENV_MYSQL_ROOT_PASSWORD\" items' > db_dump.sql Compendium Open Container initiave Podman, the daemonless container engine for developing, managing, and running OCI Containers on your Linux System Quay image registry kubernetes openshift Git commands summary","title":"notes certification"},{"location":"notes-ocp-training/#notes-on-training","text":"","title":"Notes on training"},{"location":"notes-ocp-training/#do180","text":"student, which has the password student, root redhat Student workstation: workstation.lab.example.com http://rol.redhat.com Students also have access to a MySQL and a Nexus server hosted by either the OpenShift cluster or by AWS github.com jbcodeforce quay.io jbcodeforce","title":"DO180"},{"location":"notes-ocp-training/#container-technology","text":"Difference between container applications and traditional deployments The major drawback to traditionally deployed software application is that the application's dependencies are entangled with the runtime environment a traditionally deployed application must be stopped before updating the associated dependencies * complex systems to provide high availability A container is a set of one or more processes that are isolated from the rest of the system. * security, storage, and network isolation * isolate dependent libraries and run time resources * less resources than VM, start quickly. * helps with the efficiency, elasticity, and reusability of the hosted applications, and portability * [Open Container initiative](https://www.opencontainers.org/) Container image: bundle of files and metadata Container engine: Rocket, Drawbridge, LXC, Docker, and Podman Started in 2001 with VServer, then move to isolated process which leverages the linux features: * **Namespaces**: The kernel can isolate specific system resources, usually visible to all processes, by placing the resources within a namespace. Namespaces can include resources like network interfaces, the process ID list, mount points, IPC resources, and the system's host name information. * **cgroups**: Control groups partition sets of processes and their children into groups to manage and limit the resources they consume. * **Seccomp** defines a security profile for processes, whitelisting the system calls, parameters and file descriptors they are allowed to use * SELinux (Security-Enhanced Linux) is a mandatory access control system for processes. Protect processes from each other and to protect the host system from its running processes Example of Dockerfile","title":"Container technology"},{"location":"notes-ocp-training/#openshift","text":"RHOCP adds the capabilities to provide a production PaaS platform such as remote management, multi tenancy, increased security, monitoring and auditing, application life-cycle management, and self-service interfaces for developers. Username RHT_OCP4_DEV_USER boyerje-us Password RHT_OCP4_DEV_PASSWORD <> API Endpoint RHT_OCP4_MASTER_API https://api.ocp-na2.prod.nextcle.com:6443 Console Web Application https://console-openshift-console.apps.ocp-na2.prod.nextcle.com Cluster ID ... Container images are named based on the following syntax: registry_name/user_name/image_name:tag Example of images used: # login to a registry sudo podman login -u username -p password registry.access.redhat.com sudo podman run ubi7/ubi:7.7 echo \"Hello!\" # Apache http server sudo podman run -d rhscl/httpd-24-rhel7:2.4-36.8 # Get IP address of a last container started podman inspect -l -f \"{{.NetworkSettings.IPAddress}}\" # mysql sudo podman run --name mysql-basic -v /var/local/mysql:/var/lib/mysql/data \\ > -e MYSQL_USER = user1 -e MYSQL_PASSWORD = mypa55 \\ > -e MYSQL_DATABASE = items -e MYSQL_ROOT_PASSWORD = r00tpa55 \\ > -d rhscl/mysql-57-rhel7:5.7-3.14 # Stop and start a container sudo podman stop my-httpd-container sudo podman restart my-httpd-container # Send a SIGKILL sudo podman kill my-httpd-container Quay.io introduces several features, such as server-side image building, fine-grained access controls, and automatic scanning of images for known vulnerabilities. To configure registries for the podman command, you need to update the /etc/containers/registries.conf . The podman search command finds images from all the registries listed in this file. [registries.search] registries = [\"registry.access.redhat.com\", \"quay.io\"] Use an FQDN and port number (5000 default) to identify a registry. By default, Podman stores container images in the /var/lib/containers/storage/overlay-images directory. Existing images from the Podman local storage can be saved to a .tar file using the podman save command. # Retrieve the list of external files and directories that Podman mounts to the running container sudo podman inspect -f \"{{range .Mounts}}{{println .Destination}}{{end}}\" official-httpd # list of modified files in the container file system sudo podman diff official-httpd # Commit the changes to a new container image with a new name sudo podman commit -a 'Jerome' official-httpd do180-custom-httpd sudo podman save [ -o FILE_NAME ] IMAGE_NAME [ :TAG ] sudo podman tag quay.io/jbcodeforce/do180-custom-httpd:v1.0 sudo podman push quay.io/jbcodeforce/do180-custom-httpd:v1.0 sudo podman build -t NAME:TAG DIR # examine the content of the environment variable of a container sudo podman exec todoapi env Red Hat Software Collections Library is the source of most container images","title":"OpenShift"},{"location":"notes-ocp-training/#deploying-containerized-applications-on-openshift","text":"Lab: # login to cluster oc login -u ${ RHT_OCP4_DEV_USER } -p ${ RHT_OCP4_DEV_PASSWORD } ${ RHT_OCP4_MASTER_API } # Create a new project named \"youruser-ocp\" oc new-project ${ RHT_OCP4_DEV_USER } -ocp # Create a temperature converter application written in PHP using the php:7.1 image stream tag. The source code is in the Git repository at https://github.com/RedHatTraining/DO180-apps/ c new-app php:7.1~https://github.com/RedHatTraining/DO180-apps --context-dir temps --name temps # Monitor progress of the build oc logs -f bc/temps # Verify that the application is deployed. oc get pods -w # Expose the temps service to create an external route for the application. oc expose svc/temps # Determine route URL oc get route/temps","title":"Deploying Containerized Applications on OpenShift"},{"location":"notes-ocp-training/#deploy-multi-container-app","text":"Podman uses Container Network Interface (CNI) to create a software-defined network (SDN) between all containers in the host. Unless stated otherwise, CNI assigns a new IP address to a container when it starts. Each container exposes all ports to other containers in the same SDN. As such, services are readily accessible within the same network. The containers expose ports to external networks only by explicit configuration. Using environment variables allows you to share information between containers with Podman. However, there are still some limitations and some manual work involved in ensuring that all environment variables stay in sync, especially when working with many containers Any service defined on Kubernetes generates environment variables for the IP address and port number where the service is available. Kubernetes automatically injects these environment variables into the containers from pods in the same namespace Get the list of templates from where to create k8s resources like Secret, a Service, a PersistentVolumeClaim, and a DeploymentConfig: oc get template -n openshift and then from one of the template: oc get template mysql-persistent -n openshift -o yaml . With template, you can publish a new template to the OpenShift cluster so that other developers can build an application from the template. Get the parameters of a template: process --parameters mysql-persistent -n openshift . From the template create the app resources file and then deploy the app: # first export the template oc get template mysql-persistent -o yaml -n openshift > mysql-persistent-template.yaml # identify appropriate values for the template parameters and process the template oc process -f mysql-persistent-template.yaml -p MYSQL_USER = dev -p MYSQL_PASSWORD = $P4SSD -p MYSQL_DATABASE = bank -p VOLUME_CAPACITY = 10Gi | oc create -f - # or using new-app oc new-app --template = mysql-persistent -p MYSQL_USER = dev -p MYSQL_PASSWORD = $P4SSD -p MYSQL_DATABASE = bank -p VOLUME_CAPACITY = 10Gi Lab: # login to cluster oc login -u ${ RHT_OCP4_DEV_USER } -p ${ RHT_OCP4_DEV_PASSWORD } ${ RHT_OCP4_MASTER_API } # Create a new project named \"youruser-\" oc new-project ${ RHT_OCP4_DEV_USER } -deploy # Build the MySQL Database image sudo podman build -t do180-mysql-57-rhel7 . # Push the MySQL image to the your Quay.io repository. sudo podman login quay.io -u ${ RHT_OCP4_QUAY_USER } sudo podman tag do180-mysql-57-rhel7 quay.io/ ${ RHT_OCP4_QUAY_USER } /do180-mysql-57-rhel7 sudo podman push quay.io/ ${ RHT_OCP4_QUAY_USER } /do180-mysql-57-rhel7","title":"Deploy multi-container app"},{"location":"notes-ocp-training/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"notes-ocp-training/#s2i","text":"The S2I image creation process is composed of the build (The BuildConfig (BC) OpenShift resources drive it) and deploy ( The DeploymentConfig (OpenShift resources) steps. retrieve the logs from a build configuration: oc logs bc/<application-name> request a new build oc start-build <application-name> Deployment logs: oc logs dc/<appname> If a container could not access a file system, it may come to the container running under a specific user so we need to authorize by : oc adm policy add-scc-to-user anyuid -z default . Make sure the ownership and permissions of the directory are set according to the USER directive in the Dockerfile that was used to build the container being deployed To avoid file system permission issues, local folders used for container volume mounts must satisfy the following: * The user executing the container processes must be the owner of the folder * The local folder must satisfy the SELinux requirements to be used as a container volume. Assign the container_file_t group to the folder by using the semanage fcontext -a -t container_file_t command, then refresh the permissions with the restorecon -R command. * Run the oc adm prune command for an automated way to remove obsolete images and other resources.","title":"S2I"},{"location":"notes-ocp-training/#containerized-applications","text":"To connect to a admin console of a pod, we can use: oc port-forward for forwarding a local port to a pod port. If the image enable remote debugging by exposing a port number, then port-forwarding, will let the IDE access the JVM to debug step by step via the Java Debug Wire Protocol (JDWP). Use openshift events to see the problem at a higher level: oc get events . To add tools like ping, telnet, iputils, dig ... into a pod, just mount local file to volume and directory like /bin, /sbin, /lib... To copy file to or from a container, podman cp is useful, or use exec: sudo podman exec -i <container> mysql -uroot -proot < /path/on/host/db.sql < db.sql # or sudo podman exec -it <containerName> sh -c 'exec mysqldump -h\"$MYSQL_PORT_3306_TCP_ADDR\" \\ -P\"$MYSQL_PORT_3306_TCP_PORT\" -uroot -p\"$MYSQL_ENV_MYSQL_ROOT_PASSWORD\" items' > db_dump.sql","title":"Containerized applications"},{"location":"notes-ocp-training/#compendium","text":"Open Container initiave Podman, the daemonless container engine for developing, managing, and running OCI Containers on your Linux System Quay image registry kubernetes openshift Git commands summary","title":"Compendium"},{"location":"oc-cli/","text":"oc cheat sheet oc is a tool written in Go to interact with an openshift cluster (one at a time). State about the current login session is stored in the home directory of the local user running the oc command. Login oc login --username collaborator --password collaborator Log in to your server using a token for an existing session. oc login --token <token> --server=https://<>.us-east.containers.cloud.ibm.com:21070 Get api version oc api-versions To assess who you are and get login userid: oc whoami See what the current context: oc whoami --show-context verify which server you are logged into oc whoami --show-server Even if logged as a non admin user, we can still execute some command as admin, if the user is a sudoer: oc get projects --as system:admin Cluster See cluster: oc config get-clusters Show a list of contexts for all sessions ever created. For each context listed, this will include details about the project, server and name of user, in that order oc config get-contexts Proxy the API server oc proxy --port=8001 curl -X GET http://localhost:8001/api/v1/namespaces/myproject/pods Persistence volume oc get pv --as system:admin Add a storage class Add access for a user to one of your project oc adm policy add-role-to-user view developer -n myproject > role \"view\" added: \"developer\" The role could be edit | view | admin Add user Project commands Select a project once logged to openshift: oc project <projectname> Get the list of projects oc get projects Get the list of supported app templates: oc new-app -L To create a project, that is mapped to a kubernetes namespace: $ oc new-project <project_name> --description=\"<description>\" --display-name=\"<display_name>\" To see a list of all the resources that have been created in the project: oc get all -o name If you need to run a container from an image that needs to be run as root, then grant additional provilieges to the project: oc adm policy add-scc-to-user anyuid -z default -n myproject --as system:admin Verify the Deployment has been created: oc get deploy Verify the ReplicaSet has been created: oc get replicaset Verify the pods are running: oc get pods Custom Resource Definition See this training oc get crd Get service account: Service accounts provide a flexible way to control API access without sharing a regular user\u2019s credentials. Every service account has an associated user name that can be granted roles. Default, builder, deployer are defined in each project) oc get sa > builder default deployer jb-kafka-cluster-entity-operator jb-kafka-cluster-kafka jb-kafka-cluster-zookeeper strimzi-cluster-operator strimzi-topic-operator As soon as a service account is created ( oc create sa strimzi-cluster-operator ), two secrets are automatically added to it: an API token credentials for the OpenShift Container Registry Access to the roles defined in a project oc get roles > strimzi-topic-operator > oc get rolebindings Work with images Search a docker image and see if it is valid: oc new-app --search openshiftkatacoda/blog-django-py Deploy an image to a project and link it to github: oc new-app --docker-image=<docker-image> [--code=<source>] --name myapp The image will be pulled down and stored within the internal OpenShift image registry. You can list what image stream resources have been created within a project by running the command: oc get imagestream -o name Expose app To expose the application created so it is available outside of the OpenShift cluster, you can run the command: oc expose service/blog-django-py To view the hostname assigned to the route created from the command line, you can run the command: oc get route/blog-django-py Or by using a label selector: oc get all --selector app=blog-django-py -o name Get detail of a resource: oc describe route/blog-django-py Deleting an application and all its resources, using label: oc delete all --selector app=blog-django-py To import an image without creating a container: oc import-image openshiftkatacoda/blog-django-py --confirm Then to deploy an instance of the application from the image stream which has been created, run the command: oc new-app blog-django-py --name blog-1 List what image stream resources have been created within a project by running the command: oc get imagestream -o name Create an app from the source code, and use source to image build process to deploy the app: oc new-app python:latest~https://github.com/jbcodeforce/order-producer-python -name appname other example with context directory: oc new-app https://github.com/jbcodeforce/refarch-kc-order-ms --context-dir=order-command-ms/ Then to track the deployment progress: oc logs -f bc/<appname> The dependencies are loaded, the build is scheduled and executed, the image is uploaded to the registry, and started. See the workflow in the diagram below. The first time the application is deployed, if you want to expose it to internet do: oc expose service/<appname> When using source to image approach, it is possible to trigger the build via: oc start-build app-name You can use oc logs to monitor the log output as the build runs. You can also monitor the progress of any builds in a project by running the command: oc get builds --watch To trigger a binary build, without committing to github, you can use the local folder to provide the source code: oc start-build app-name --from-dir=. --wait The --wait option is supplied to indicate that the command should only return when the build has completed. Switch back to minishift context: oc config use-context minishift","title":"oc CLI"},{"location":"oc-cli/#oc-cheat-sheet","text":"oc is a tool written in Go to interact with an openshift cluster (one at a time). State about the current login session is stored in the home directory of the local user running the oc command.","title":"oc cheat sheet"},{"location":"oc-cli/#login","text":"oc login --username collaborator --password collaborator Log in to your server using a token for an existing session. oc login --token <token> --server=https://<>.us-east.containers.cloud.ibm.com:21070 Get api version oc api-versions To assess who you are and get login userid: oc whoami See what the current context: oc whoami --show-context verify which server you are logged into oc whoami --show-server Even if logged as a non admin user, we can still execute some command as admin, if the user is a sudoer: oc get projects --as system:admin","title":"Login"},{"location":"oc-cli/#cluster","text":"See cluster: oc config get-clusters Show a list of contexts for all sessions ever created. For each context listed, this will include details about the project, server and name of user, in that order oc config get-contexts Proxy the API server oc proxy --port=8001 curl -X GET http://localhost:8001/api/v1/namespaces/myproject/pods","title":"Cluster"},{"location":"oc-cli/#persistence-volume","text":"oc get pv --as system:admin Add a storage class","title":"Persistence volume"},{"location":"oc-cli/#add-access-for-a-user-to-one-of-your-project","text":"oc adm policy add-role-to-user view developer -n myproject > role \"view\" added: \"developer\" The role could be edit | view | admin","title":"Add access for a user to one of your project"},{"location":"oc-cli/#add-user","text":"","title":"Add user"},{"location":"oc-cli/#project-commands","text":"Select a project once logged to openshift: oc project <projectname> Get the list of projects oc get projects Get the list of supported app templates: oc new-app -L To create a project, that is mapped to a kubernetes namespace: $ oc new-project <project_name> --description=\"<description>\" --display-name=\"<display_name>\" To see a list of all the resources that have been created in the project: oc get all -o name If you need to run a container from an image that needs to be run as root, then grant additional provilieges to the project: oc adm policy add-scc-to-user anyuid -z default -n myproject --as system:admin Verify the Deployment has been created: oc get deploy Verify the ReplicaSet has been created: oc get replicaset Verify the pods are running: oc get pods","title":"Project commands"},{"location":"oc-cli/#custom-resource-definition","text":"See this training oc get crd Get service account: Service accounts provide a flexible way to control API access without sharing a regular user\u2019s credentials. Every service account has an associated user name that can be granted roles. Default, builder, deployer are defined in each project) oc get sa > builder default deployer jb-kafka-cluster-entity-operator jb-kafka-cluster-kafka jb-kafka-cluster-zookeeper strimzi-cluster-operator strimzi-topic-operator As soon as a service account is created ( oc create sa strimzi-cluster-operator ), two secrets are automatically added to it: an API token credentials for the OpenShift Container Registry Access to the roles defined in a project oc get roles > strimzi-topic-operator > oc get rolebindings","title":"Custom Resource Definition"},{"location":"oc-cli/#work-with-images","text":"Search a docker image and see if it is valid: oc new-app --search openshiftkatacoda/blog-django-py Deploy an image to a project and link it to github: oc new-app --docker-image=<docker-image> [--code=<source>] --name myapp The image will be pulled down and stored within the internal OpenShift image registry. You can list what image stream resources have been created within a project by running the command: oc get imagestream -o name","title":"Work with images"},{"location":"oc-cli/#expose-app","text":"To expose the application created so it is available outside of the OpenShift cluster, you can run the command: oc expose service/blog-django-py To view the hostname assigned to the route created from the command line, you can run the command: oc get route/blog-django-py Or by using a label selector: oc get all --selector app=blog-django-py -o name Get detail of a resource: oc describe route/blog-django-py Deleting an application and all its resources, using label: oc delete all --selector app=blog-django-py To import an image without creating a container: oc import-image openshiftkatacoda/blog-django-py --confirm Then to deploy an instance of the application from the image stream which has been created, run the command: oc new-app blog-django-py --name blog-1 List what image stream resources have been created within a project by running the command: oc get imagestream -o name Create an app from the source code, and use source to image build process to deploy the app: oc new-app python:latest~https://github.com/jbcodeforce/order-producer-python -name appname other example with context directory: oc new-app https://github.com/jbcodeforce/refarch-kc-order-ms --context-dir=order-command-ms/ Then to track the deployment progress: oc logs -f bc/<appname> The dependencies are loaded, the build is scheduled and executed, the image is uploaded to the registry, and started. See the workflow in the diagram below. The first time the application is deployed, if you want to expose it to internet do: oc expose service/<appname> When using source to image approach, it is possible to trigger the build via: oc start-build app-name You can use oc logs to monitor the log output as the build runs. You can also monitor the progress of any builds in a project by running the command: oc get builds --watch To trigger a binary build, without committing to github, you can use the local folder to provide the source code: oc start-build app-name --from-dir=. --wait The --wait option is supplied to indicate that the command should only return when the build has completed. Switch back to minishift context: oc config use-context minishift","title":"Expose app"},{"location":"odo-appsody/","text":"ODO and Appsody Summary Openshift DO ODO CLI creating applications on OpenShift . The main value propositions are: Abstracts away complex Kubernetes and OpenShift. Detects changes to local code and deploys it to the cluster automatically, giving instant feedback to validate changes in real time Leverage source 2 images to produce ready-to-run images by building source code without the need of a Dockerfile The basic commands : # login to a ROKS odo login --token = s....vA c100-e.us-south.containers.cloud.ibm.com:30040 # Create a new project in OCP odo project create jbsandbox # change project inside OCP odo project set jbsandbox # Create a component (from an existing project for ex)... then following the question odo component create # list available component odo catalog list components # push to ocp odo push # delete an app odo app list odo app delete myapp Push a component creates 2 pods: one ...app-deploy and one ...-app Important concepts See details in odo architecture section . Init containers are specialized containers that run before the application container starts and configure the necessary environment for the application containers to run. Init containers can have files that application images do not have, for example setup scripts Application container is the main container inside of which the user-source code executes. It uses two volumes: emptyDir and PersistentVolume. The data on the PersistentVolume persists across Pod restarts. odo creates a Service for every application Pod to make it accessible for communication odo push workflow: create resources like deployment config, service, secrets, PVC index source code files push code into the application container execute assemble and restart. Appsody Appsody provides pre-configured container images (stacks) and project templates for a growing set of popular open source runtimes and frameworks, providing a foundation on which to build applications for Kubernetes and Knative deployments. On k8s it uses the appsody operator to automate the installation and maintenance of a special type of Custom Resource Definitions (CRDs), called AppsodyApplication. On Mac install appsody with home brew: brew install appsody/appsody/appsody . To update to new version: brew upgrade appsody How it works Appsody includes a CLI and a daemon to control the life cycle of the application. Developers use the CLI to create a new application from an existing \"stack\" (1). Stacks are defined in a repository. Repositories can be referenced from remote sources (e.g., GitHub) or can be stored locally on a filesystem. First we can get the list of templates available via the command, which list stacks from all known repo: appsody list Then create our own application using one of the template: mkdir projectname appsody init java-openliberty # another useful one appsody init quarkus # or ours appsody init ibmcase/ibm-gse-eda-quarkus In general the command is appsody init <repository-name>/<stack> . It is possible to initialize an existing project using the command: appsody init <stackname> --no-template . Appsody helps developer to do not worry about the details of k8s deployment and build. During a Appsody run, debug or test step (2), Appsody creates a Docker container based on the parent stack Dockerfile, and combines application code with the source code in the template. When a source code project is initialized with Appsody, you get a local Appsody development container where you can do the following commands: appsody run appsody test appsody debug One of the above command creates a daemon which monitors changes to any files and build and start a new docker container continuously. # The daemon ps -ef | grep appsody 501 4156 93070 appsody run # the docker container 501 56789 4156 docker run --rm -p 7777 :7777 -p 9080 :9080 -p 9443 :9443 --name scoring-mp-dev -v /Users/jeromeboyer/.m2/repository:/mvn/repository -v /Users/jeromeboyer/Code/jbcodeforce/myEDA/refarch-reefer-ml/scoring-mp/src:/project/user-app/src -v /Users/jeromeboyer/Code/jbcodeforce/myEDA/refarch-reefer-ml/scoring-mp/pom.xml:/project/user-app/pom.xml -v appsody-controller-0.3.3:/.appsody -t --entrypoint /.appsody/appsody-controller docker.io/appsody/java-microprofile:0.2 --mode = run The other basic commands are: Run , to run. But you can use docker options like: appsody run --docker-options = \"--env-file=postgresql.properties\" # connect to a local docker network appsody run --network kafkanet Build : You can use the appsody build command to generate a deployment Docker image on your local Docker registry, and then manually deploy that image to your runtime platform of choice. Deploy : You can use the appsody deploy command (3) to deploy the same deployment Docker image directly to a Kubernetes cluster that you are using for testing or staging. See next section. The full command template is: appsody deploy -t <mynamespace/myrepository [ :tag ] > --push-url $IMAGE_REGISTRY --push --namespace mynamespace [ --knative ] # example appsody deploy -t jbsandbox/eda-coldchain-agent:0.0.1 --push-url $IMAGE_REGISTRY --push --namespace jbsandbox To undeploy: appsody deploy delete You can delegate the build and deployment steps to an external pipeline, such as a Tekton pipeline that consumes the source code of your Appsody project after you push it to a GitHub repository. appsody stop See Appsody CLI commands. See this tutorial how to deploy on openshift App Deployment Log to the cluster Get the name of the available registry oc get route --all-namespaces | grep registry . Keep it in an env var: export IMAGE_REGISTRY=default-route-openshift-image-registry.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud Login to this registry: docker login -u $(oc whoami) -p $(oc whoami -t) $IMAGE_REGISTRY Create config map via yaml descriptor or command for all the dependent env variables, properties,... See notes in this repo Deploy using a command like: appsody deploy -t jbsandbox/eda-coldchain-agent:0.0.1 --push-url $IMAGE_REGISTRY --push --namespace jbsandbox appsody deploy -t dockerhub/imagename --push -n yournamespace (3) will do the following: deploy Appsody operator into the given namespace if no operator found. (you can install it manually too see one of the instruction depending of the release ) call appsody build and create a deployment image with the given tag push the image to docker hub or other repository create the app-deploy.yaml deployment manifest Apply it with: kubectl apply -f app-deploy.yaml within the given namespace Verify the operator is deployed: oc get pods NAME READY STATUS RESTARTS AGE appsody-operator-d8dfb4f5f-4dpwk 1 /1 Running 0 4m34s As part of the deployment manifest a service and a route are created. For example using a microprofile app the following command will verify everything went well. curl http://scoring-mp-eda-sandbox.apps.green.ocp.csplab.local/health appsody deploy delete -n yournamespace To ensure that the latest version of your app is pushed to the cluster, use the -t flag to add a unique tag every time you redeploy your app. Kubernetes then detects a change in the deployment manifest, and pushes your app to the cluster again. 3 mn Tekton Appsody tekton example Tekton CLI Appsody app creation examples Create a python flask app The stack is not for production and is not fully supported. Here is an example of creating a simple webapp with flask, flask cli and gunicorn # Get the default template from the stack appsody init incubator/python-flask # build an image with a name = the folder name based on the dockerfile from the stack appsody build # or run it directly appsody run You can add your own dockerfile to extend existing one. With docker images you can see what appsody build created, then you can use this image as source for your own docker image FROM <nameofyourapp> ADD stuff CMD change the command To add your own code. Create quarkus knative app appsody init quarkus Note The container image will not be pushed to a remote container registry, and hence the container image url has to be dev.local, to make Knative deploy it without trying to pull it from external container registry. Then do the following steps: update the index.html to provide some simple doc of the app add health, and openapi xml <dependency> <groupId>io.quarkus</groupId> <artifactId>quarkus-smallrye-health</artifactId> </dependency> <dependency> <groupId>io.quarkus</groupId> <artifactId>quarkus-smallrye-openapi</artifactId> </dependency> * If using kafka to support event driven solution add: <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-smallrye-reactive-messaging-kafka </artifactId> </dependency> Create a microprofile 3.0 app appsody init java-openliberty Defining your own stack See code in appsody-stacks/experimental/ibm-gse-eda-quarkus for one example of a kafka quarkus stack and Appsody tutorial to get detail instructions. Stack has one dockerfile to help building the application and control the build, run and test steps of Appsody . And a second Dockerfile in the image/project folder to \"dockerize\" the final app. This Dockerfile is responsible for ensuring the combined dependencies are installed in the final image. It hosts the target app-deploy.yaml file used for kubernetes deployment. When designing a stack, we need to decide who controls the application: a web server in which the developer, user of the stack, is adding new end points, or the developer is controlling how the app starts and runs. See details in this note . See appsody environment variables description in the product documentation See this other create appsody tutorial here . Some considerations to address: select the technologies and libraries to use address how to verify dependencies define what kind of sample starter application address how to enable adding new libraries define what docker image repository to use, and what credentials Here is a summary of the steps to create a Kafka java stack for consumer and producer: Look to local cache for appsody stack $ export APPSODY_PULL_POLICY = IFNOTPRESENT Create a starter stack, as a scaffold. $ appsody stack create ibm-gse-eda-quarkus --copy incubator/java-openliberty * Update the template folder, and then the stack.yml to define version number (e.g. 1.7.1), name, ... * Update the pom.xml under image folder. * Under the experimental folder build the stack using the Dockerfile-stack file with the following command $ appsody stack package --image-namespace ibmcase Your local stack is available as part of ` dev.local ` repo. This mean a file ibm-gse-eda-quarkus.v1.7.1.templates.kafka.tar.gz is created in .appsody/stacks/dev.local/ Test your stack scaffold $ appsody init dev.local/ibm-gse-eda-quarkus kafka Successfully initialized Appsody project with the dev.local/gse-eda-java-stack stack and the kafka template. Start the application scaffold using appsody run . If you are running with a remote kafka broker set the scripts/appsody.env variables accordingly. Modify the Dockerfile-stack file to include the base image and dependencies for the server and other predefined code. Package your stack to create a docker images that will be pushed to dockerhub registry appsody stack package --image-namespace ibmcase --image-registry docker.io # this command builds a docker image but also creates files under ~/.appsody/stacks/dev.local ibm-gse-eda-quarkus.v0.4.1.source.tar.gz ibm-gse-eda-quarkus.v0.4.1.templates.default.tar.gz ibm-gse-eda-quarkus.v0.4.1.templates.kafka.tar.gz # push the docker image created docker push ibmcase/ibm-gse-eda-quarkus If not done create a git release in the appsody-stack github repository. See the latest release Redefined the repository index, so from the source of all the stacks do appsody stack add-to-repo ibmcase --release-url https://github.com/ibm-cloud-architecture/appsody-stacks/releases/download/0.4.1/ # this command updates the following files ibmcase-index.json ibmcase-index.yaml * copy those file into root folder of the stack project Upload the source code and template archives to the release using drag and drop. The files are ibm-gse-eda-quarkus.v0.4.1.source.tar.gz ibm-gse-eda-quarkus.v0.4.1.templates.default.tar.gz ibm-gse-eda-quarkus.v0.4.1.templates.kafka.tar.gz ibmcase-index.json ibmcase-index.yaml then publish the release. Which can be see at the URL: https://github.com/ibm-cloud-architecture/appsody-stacks/releases . appsody repo add ibmcase https://raw.githubusercontent.com/ibm-cloud-architecture/appsody-stacks/master/ibmcase-index.yaml Future readings Introduction to Appsody: Developing containerized applications for the cloud just got easier Video Appsody overview Kabanero - Appsody - Tekton - Openshift","title":"ODO - Appsody Summary"},{"location":"odo-appsody/#odo-and-appsody-summary","text":"","title":"ODO and Appsody Summary"},{"location":"odo-appsody/#openshift-do","text":"ODO CLI creating applications on OpenShift . The main value propositions are: Abstracts away complex Kubernetes and OpenShift. Detects changes to local code and deploys it to the cluster automatically, giving instant feedback to validate changes in real time Leverage source 2 images to produce ready-to-run images by building source code without the need of a Dockerfile The basic commands : # login to a ROKS odo login --token = s....vA c100-e.us-south.containers.cloud.ibm.com:30040 # Create a new project in OCP odo project create jbsandbox # change project inside OCP odo project set jbsandbox # Create a component (from an existing project for ex)... then following the question odo component create # list available component odo catalog list components # push to ocp odo push # delete an app odo app list odo app delete myapp Push a component creates 2 pods: one ...app-deploy and one ...-app","title":"Openshift DO"},{"location":"odo-appsody/#important-concepts","text":"See details in odo architecture section . Init containers are specialized containers that run before the application container starts and configure the necessary environment for the application containers to run. Init containers can have files that application images do not have, for example setup scripts Application container is the main container inside of which the user-source code executes. It uses two volumes: emptyDir and PersistentVolume. The data on the PersistentVolume persists across Pod restarts. odo creates a Service for every application Pod to make it accessible for communication odo push workflow: create resources like deployment config, service, secrets, PVC index source code files push code into the application container execute assemble and restart.","title":"Important concepts"},{"location":"odo-appsody/#appsody","text":"Appsody provides pre-configured container images (stacks) and project templates for a growing set of popular open source runtimes and frameworks, providing a foundation on which to build applications for Kubernetes and Knative deployments. On k8s it uses the appsody operator to automate the installation and maintenance of a special type of Custom Resource Definitions (CRDs), called AppsodyApplication. On Mac install appsody with home brew: brew install appsody/appsody/appsody . To update to new version: brew upgrade appsody","title":"Appsody"},{"location":"odo-appsody/#how-it-works","text":"Appsody includes a CLI and a daemon to control the life cycle of the application. Developers use the CLI to create a new application from an existing \"stack\" (1). Stacks are defined in a repository. Repositories can be referenced from remote sources (e.g., GitHub) or can be stored locally on a filesystem. First we can get the list of templates available via the command, which list stacks from all known repo: appsody list Then create our own application using one of the template: mkdir projectname appsody init java-openliberty # another useful one appsody init quarkus # or ours appsody init ibmcase/ibm-gse-eda-quarkus In general the command is appsody init <repository-name>/<stack> . It is possible to initialize an existing project using the command: appsody init <stackname> --no-template . Appsody helps developer to do not worry about the details of k8s deployment and build. During a Appsody run, debug or test step (2), Appsody creates a Docker container based on the parent stack Dockerfile, and combines application code with the source code in the template. When a source code project is initialized with Appsody, you get a local Appsody development container where you can do the following commands: appsody run appsody test appsody debug One of the above command creates a daemon which monitors changes to any files and build and start a new docker container continuously. # The daemon ps -ef | grep appsody 501 4156 93070 appsody run # the docker container 501 56789 4156 docker run --rm -p 7777 :7777 -p 9080 :9080 -p 9443 :9443 --name scoring-mp-dev -v /Users/jeromeboyer/.m2/repository:/mvn/repository -v /Users/jeromeboyer/Code/jbcodeforce/myEDA/refarch-reefer-ml/scoring-mp/src:/project/user-app/src -v /Users/jeromeboyer/Code/jbcodeforce/myEDA/refarch-reefer-ml/scoring-mp/pom.xml:/project/user-app/pom.xml -v appsody-controller-0.3.3:/.appsody -t --entrypoint /.appsody/appsody-controller docker.io/appsody/java-microprofile:0.2 --mode = run The other basic commands are: Run , to run. But you can use docker options like: appsody run --docker-options = \"--env-file=postgresql.properties\" # connect to a local docker network appsody run --network kafkanet Build : You can use the appsody build command to generate a deployment Docker image on your local Docker registry, and then manually deploy that image to your runtime platform of choice. Deploy : You can use the appsody deploy command (3) to deploy the same deployment Docker image directly to a Kubernetes cluster that you are using for testing or staging. See next section. The full command template is: appsody deploy -t <mynamespace/myrepository [ :tag ] > --push-url $IMAGE_REGISTRY --push --namespace mynamespace [ --knative ] # example appsody deploy -t jbsandbox/eda-coldchain-agent:0.0.1 --push-url $IMAGE_REGISTRY --push --namespace jbsandbox To undeploy: appsody deploy delete You can delegate the build and deployment steps to an external pipeline, such as a Tekton pipeline that consumes the source code of your Appsody project after you push it to a GitHub repository. appsody stop See Appsody CLI commands. See this tutorial how to deploy on openshift","title":"How it works"},{"location":"odo-appsody/#app-deployment","text":"Log to the cluster Get the name of the available registry oc get route --all-namespaces | grep registry . Keep it in an env var: export IMAGE_REGISTRY=default-route-openshift-image-registry.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud Login to this registry: docker login -u $(oc whoami) -p $(oc whoami -t) $IMAGE_REGISTRY Create config map via yaml descriptor or command for all the dependent env variables, properties,... See notes in this repo Deploy using a command like: appsody deploy -t jbsandbox/eda-coldchain-agent:0.0.1 --push-url $IMAGE_REGISTRY --push --namespace jbsandbox appsody deploy -t dockerhub/imagename --push -n yournamespace (3) will do the following: deploy Appsody operator into the given namespace if no operator found. (you can install it manually too see one of the instruction depending of the release ) call appsody build and create a deployment image with the given tag push the image to docker hub or other repository create the app-deploy.yaml deployment manifest Apply it with: kubectl apply -f app-deploy.yaml within the given namespace Verify the operator is deployed: oc get pods NAME READY STATUS RESTARTS AGE appsody-operator-d8dfb4f5f-4dpwk 1 /1 Running 0 4m34s As part of the deployment manifest a service and a route are created. For example using a microprofile app the following command will verify everything went well. curl http://scoring-mp-eda-sandbox.apps.green.ocp.csplab.local/health appsody deploy delete -n yournamespace To ensure that the latest version of your app is pushed to the cluster, use the -t flag to add a unique tag every time you redeploy your app. Kubernetes then detects a change in the deployment manifest, and pushes your app to the cluster again.","title":"App Deployment"},{"location":"odo-appsody/#3-mn-tekton","text":"Appsody tekton example Tekton CLI","title":"3 mn Tekton"},{"location":"odo-appsody/#appsody-app-creation-examples","text":"","title":"Appsody app creation examples"},{"location":"odo-appsody/#create-a-python-flask-app","text":"The stack is not for production and is not fully supported. Here is an example of creating a simple webapp with flask, flask cli and gunicorn # Get the default template from the stack appsody init incubator/python-flask # build an image with a name = the folder name based on the dockerfile from the stack appsody build # or run it directly appsody run You can add your own dockerfile to extend existing one. With docker images you can see what appsody build created, then you can use this image as source for your own docker image FROM <nameofyourapp> ADD stuff CMD change the command To add your own code.","title":"Create a python flask app"},{"location":"odo-appsody/#create-quarkus-knative-app","text":"appsody init quarkus Note The container image will not be pushed to a remote container registry, and hence the container image url has to be dev.local, to make Knative deploy it without trying to pull it from external container registry. Then do the following steps: update the index.html to provide some simple doc of the app add health, and openapi xml <dependency> <groupId>io.quarkus</groupId> <artifactId>quarkus-smallrye-health</artifactId> </dependency> <dependency> <groupId>io.quarkus</groupId> <artifactId>quarkus-smallrye-openapi</artifactId> </dependency> * If using kafka to support event driven solution add: <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-smallrye-reactive-messaging-kafka </artifactId> </dependency>","title":"Create quarkus knative app"},{"location":"odo-appsody/#create-a-microprofile-30-app","text":"appsody init java-openliberty","title":"Create a microprofile 3.0 app"},{"location":"odo-appsody/#defining-your-own-stack","text":"See code in appsody-stacks/experimental/ibm-gse-eda-quarkus for one example of a kafka quarkus stack and Appsody tutorial to get detail instructions. Stack has one dockerfile to help building the application and control the build, run and test steps of Appsody . And a second Dockerfile in the image/project folder to \"dockerize\" the final app. This Dockerfile is responsible for ensuring the combined dependencies are installed in the final image. It hosts the target app-deploy.yaml file used for kubernetes deployment. When designing a stack, we need to decide who controls the application: a web server in which the developer, user of the stack, is adding new end points, or the developer is controlling how the app starts and runs. See details in this note . See appsody environment variables description in the product documentation See this other create appsody tutorial here . Some considerations to address: select the technologies and libraries to use address how to verify dependencies define what kind of sample starter application address how to enable adding new libraries define what docker image repository to use, and what credentials Here is a summary of the steps to create a Kafka java stack for consumer and producer: Look to local cache for appsody stack $ export APPSODY_PULL_POLICY = IFNOTPRESENT Create a starter stack, as a scaffold. $ appsody stack create ibm-gse-eda-quarkus --copy incubator/java-openliberty * Update the template folder, and then the stack.yml to define version number (e.g. 1.7.1), name, ... * Update the pom.xml under image folder. * Under the experimental folder build the stack using the Dockerfile-stack file with the following command $ appsody stack package --image-namespace ibmcase Your local stack is available as part of ` dev.local ` repo. This mean a file ibm-gse-eda-quarkus.v1.7.1.templates.kafka.tar.gz is created in .appsody/stacks/dev.local/ Test your stack scaffold $ appsody init dev.local/ibm-gse-eda-quarkus kafka Successfully initialized Appsody project with the dev.local/gse-eda-java-stack stack and the kafka template. Start the application scaffold using appsody run . If you are running with a remote kafka broker set the scripts/appsody.env variables accordingly. Modify the Dockerfile-stack file to include the base image and dependencies for the server and other predefined code. Package your stack to create a docker images that will be pushed to dockerhub registry appsody stack package --image-namespace ibmcase --image-registry docker.io # this command builds a docker image but also creates files under ~/.appsody/stacks/dev.local ibm-gse-eda-quarkus.v0.4.1.source.tar.gz ibm-gse-eda-quarkus.v0.4.1.templates.default.tar.gz ibm-gse-eda-quarkus.v0.4.1.templates.kafka.tar.gz # push the docker image created docker push ibmcase/ibm-gse-eda-quarkus If not done create a git release in the appsody-stack github repository. See the latest release Redefined the repository index, so from the source of all the stacks do appsody stack add-to-repo ibmcase --release-url https://github.com/ibm-cloud-architecture/appsody-stacks/releases/download/0.4.1/ # this command updates the following files ibmcase-index.json ibmcase-index.yaml * copy those file into root folder of the stack project Upload the source code and template archives to the release using drag and drop. The files are ibm-gse-eda-quarkus.v0.4.1.source.tar.gz ibm-gse-eda-quarkus.v0.4.1.templates.default.tar.gz ibm-gse-eda-quarkus.v0.4.1.templates.kafka.tar.gz ibmcase-index.json ibmcase-index.yaml then publish the release. Which can be see at the URL: https://github.com/ibm-cloud-architecture/appsody-stacks/releases . appsody repo add ibmcase https://raw.githubusercontent.com/ibm-cloud-architecture/appsody-stacks/master/ibmcase-index.yaml","title":"Defining your own stack"},{"location":"odo-appsody/#future-readings","text":"Introduction to Appsody: Developing containerized applications for the cloud just got easier Video Appsody overview Kabanero - Appsody - Tekton - Openshift","title":"Future readings"},{"location":"opendatahub/","text":"Open Data Hub Studies Open Data Hub main site Summary The goal of Open Data Hub is to provide open source AI tools for running large and distributed AI workloads on Openshift Container platform. While AI Library is to provide ML models as a service on Openshift. In general, an AI workflow includes most of the steps shown in figure below: For data storage and availability, ODH provides Ceph, with multi protocol support including block, file and S3 object API support, both for persistent storage within the containers and as a scalable object storage data lake that AI applications can store and access data from. Notes Ceph delivers a self-managed, self-scaling, and self-healing storage infrastructure using storage cluster. A key Ceph architectural tenet is to have no single point of failure (SPoF) in the system. Installation on Openshift The latest version of the Open Data Hub operator project is located here: https://gitlab.com/opendatahub/opendatahub-operator Install Ceph with the rook operator using these instructions . The following github has the configurations: https://github.com/rook To validate pods oc get pods -n rook-ceph-system oc get pods -n rook-ceph","title":"Open Data Hub"},{"location":"opendatahub/#open-data-hub-studies","text":"Open Data Hub main site","title":"Open Data Hub Studies"},{"location":"opendatahub/#summary","text":"The goal of Open Data Hub is to provide open source AI tools for running large and distributed AI workloads on Openshift Container platform. While AI Library is to provide ML models as a service on Openshift. In general, an AI workflow includes most of the steps shown in figure below: For data storage and availability, ODH provides Ceph, with multi protocol support including block, file and S3 object API support, both for persistent storage within the containers and as a scalable object storage data lake that AI applications can store and access data from. Notes Ceph delivers a self-managed, self-scaling, and self-healing storage infrastructure using storage cluster. A key Ceph architectural tenet is to have no single point of failure (SPoF) in the system.","title":"Summary"},{"location":"opendatahub/#installation-on-openshift","text":"The latest version of the Open Data Hub operator project is located here: https://gitlab.com/opendatahub/opendatahub-operator Install Ceph with the rook operator using these instructions . The following github has the configurations: https://github.com/rook To validate pods oc get pods -n rook-ceph-system oc get pods -n rook-ceph","title":"Installation on Openshift"},{"location":"spark-on-os/","text":"Kubernetes Operators Operators Operators make it easy to manage complex stateful applications on top of Kubernetes. They are custom resource definition in kubernetes. See the framework here . Operator SDK is a framework to expose higher level APIs to write operational logic. Here is a workflow for a new Go-based Operator using the Operator SDK: Create a new Operator project using the SDK CLI. operator-sdk new podset-operator --type=go --skip-git-init Create a new Custom Resource Definition API Type using the SDK CLI. operator-sdk add api --api-version=app.example.com/v1alpha1 --kind=PodSet Add your Custom Resource Definition (CRD) to your live Kubernetes cluster. Define your Custom Resource Spec and Status. Create a new Controller for your Custom Resource Definition API. Write the reconciling logic for your Controller. Run the Operator locally to test your code against your live Kubernetes cluster. Add your Custom Resource (CR) to your live Kubernetes cluster and watch your Operator in action! After you are satisifed with your work, use the SDK CLI to build and generate the Operator Deployment manifests. Optionally add additional APIs and Controllers using the SDK CLI. A namespace-scoped operator (the default) watches and manages resources in a single namespace, whereas a cluster-scoped operator watches and manages resources cluster-wide. Note TBC Spark on openshift using operator The following spark operator from : git clone https://github.com/radanalyticsio/spark-operator.git supports both config map or custom resource definition to deploy spark cluster on kubernetes. It works well in environments where a user has a limited role-based access to Kubernetes, such as OpenShift. See also this note on user identity . You need a custom spark docker image to set userid and pod security policies. Spark operator To deploy the spark operator, use one of the yaml file (with CRD or with configMap) kubectl apply -f manifest/operator.yaml or kubectl apply -f manifest/operator-cm.yaml Spark cluster Once the operator is deployed, configure your spark cluster using one of the yaml examples cd examples kubectl apply -f cluster.yaml or kubectl apply -f cluster-cm.yaml Once you don't need the cluster anymore, you can delete it by deleting the custom resource by: kubectl delete sparkcluster my-spark-cluster Exposing the spark UI Create a route for the Spark UI: go to the service to get the exposed port for the my-spark-cluster-ui service. In network> routes add the roule with the following yaml: apiVersion : route.openshift.io/v1 kind : Route metadata : name : expose-spark-ui namespace : greencompute spec : path : / to : kind : Service name : my-spark-cluster-ui port : targetPort : 8080 Then going to the exposed URL will bring the basic spark user interface, helpful to get visibility on the running and completed application, and the workers state. Test with spark terminal and scala Go to one of the spark worker pod, in the Workloads menu, then go to the Terminal and enter: spark-shell you should now be in scala interpretor. Spark\u2019s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. The simple exercise will be to count the number of word occurence in a file. First connect to one of the spark pod using kubectl or oc. oc get pods NAME READY STATUS RESTARTS AGE my-spark-cluster-m-4gmdb 1/1 Running 0 1d my-spark-cluster-w-jh9pl 1/1 Running 1 1d my-spark-cluster-w-mnvkz 1/1 Running 0 1d oc exec -ti my-spark-cluster-m-4gmdb bash bash-4.2$ cd /tmp; vi input.txt enter some sentences In the spark-shell terminal, enter the following scala line of code to connect to spark context (sc) variable, read the file scala> val inputfile = sc.textFile(\"input.txt\") inputfile: org.apache.spark.rdd.RDD[String] = input.txt MapPartitionsRDD[1] at textFile at <console>:24 scala> val counts = inputfile. flatMap (line => line. split (\" \")).map (word => (word, 1)).reduceByKey (_+_) counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at <console>:25 scala> counts.saveAsTextFile (\"output\") The output is a folder under the /tmp directory. Inside the cluster executor container, we can see the file cat part-00000 that contains the word count. Spark works!. See my other repo on spark studies The most common way to launch spark applications on the cluster is to use the shell command spark-submit.","title":"Operators (with Spark)"},{"location":"spark-on-os/#kubernetes-operators","text":"","title":"Kubernetes Operators"},{"location":"spark-on-os/#operators","text":"Operators make it easy to manage complex stateful applications on top of Kubernetes. They are custom resource definition in kubernetes. See the framework here . Operator SDK is a framework to expose higher level APIs to write operational logic. Here is a workflow for a new Go-based Operator using the Operator SDK: Create a new Operator project using the SDK CLI. operator-sdk new podset-operator --type=go --skip-git-init Create a new Custom Resource Definition API Type using the SDK CLI. operator-sdk add api --api-version=app.example.com/v1alpha1 --kind=PodSet Add your Custom Resource Definition (CRD) to your live Kubernetes cluster. Define your Custom Resource Spec and Status. Create a new Controller for your Custom Resource Definition API. Write the reconciling logic for your Controller. Run the Operator locally to test your code against your live Kubernetes cluster. Add your Custom Resource (CR) to your live Kubernetes cluster and watch your Operator in action! After you are satisifed with your work, use the SDK CLI to build and generate the Operator Deployment manifests. Optionally add additional APIs and Controllers using the SDK CLI. A namespace-scoped operator (the default) watches and manages resources in a single namespace, whereas a cluster-scoped operator watches and manages resources cluster-wide. Note TBC","title":"Operators"},{"location":"spark-on-os/#spark-on-openshift-using-operator","text":"The following spark operator from : git clone https://github.com/radanalyticsio/spark-operator.git supports both config map or custom resource definition to deploy spark cluster on kubernetes. It works well in environments where a user has a limited role-based access to Kubernetes, such as OpenShift. See also this note on user identity . You need a custom spark docker image to set userid and pod security policies.","title":"Spark on openshift using operator"},{"location":"spark-on-os/#spark-operator","text":"To deploy the spark operator, use one of the yaml file (with CRD or with configMap) kubectl apply -f manifest/operator.yaml or kubectl apply -f manifest/operator-cm.yaml","title":"Spark operator"},{"location":"spark-on-os/#spark-cluster","text":"Once the operator is deployed, configure your spark cluster using one of the yaml examples cd examples kubectl apply -f cluster.yaml or kubectl apply -f cluster-cm.yaml Once you don't need the cluster anymore, you can delete it by deleting the custom resource by: kubectl delete sparkcluster my-spark-cluster","title":"Spark cluster"},{"location":"spark-on-os/#exposing-the-spark-ui","text":"Create a route for the Spark UI: go to the service to get the exposed port for the my-spark-cluster-ui service. In network> routes add the roule with the following yaml: apiVersion : route.openshift.io/v1 kind : Route metadata : name : expose-spark-ui namespace : greencompute spec : path : / to : kind : Service name : my-spark-cluster-ui port : targetPort : 8080 Then going to the exposed URL will bring the basic spark user interface, helpful to get visibility on the running and completed application, and the workers state.","title":"Exposing the spark UI"},{"location":"spark-on-os/#test-with-spark-terminal-and-scala","text":"Go to one of the spark worker pod, in the Workloads menu, then go to the Terminal and enter: spark-shell you should now be in scala interpretor. Spark\u2019s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. The simple exercise will be to count the number of word occurence in a file. First connect to one of the spark pod using kubectl or oc. oc get pods NAME READY STATUS RESTARTS AGE my-spark-cluster-m-4gmdb 1/1 Running 0 1d my-spark-cluster-w-jh9pl 1/1 Running 1 1d my-spark-cluster-w-mnvkz 1/1 Running 0 1d oc exec -ti my-spark-cluster-m-4gmdb bash bash-4.2$ cd /tmp; vi input.txt enter some sentences In the spark-shell terminal, enter the following scala line of code to connect to spark context (sc) variable, read the file scala> val inputfile = sc.textFile(\"input.txt\") inputfile: org.apache.spark.rdd.RDD[String] = input.txt MapPartitionsRDD[1] at textFile at <console>:24 scala> val counts = inputfile. flatMap (line => line. split (\" \")).map (word => (word, 1)).reduceByKey (_+_) counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at <console>:25 scala> counts.saveAsTextFile (\"output\") The output is a folder under the /tmp directory. Inside the cluster executor container, we can see the file cat part-00000 that contains the word count. Spark works!. See my other repo on spark studies The most common way to launch spark applications on the cluster is to use the shell command spark-submit.","title":"Test with spark terminal and scala"},{"location":"sre/","text":"System Reliability Engineering Some potential failures to consider for each deployment: Availability Always ask what are the SLAs Queue full and transaction not being processed API backend not available Network failure DNS problems Update for new version fails to deploy Hitting unknown limits, like number of VPC interfaces Certificate not auto renewed on expiry, and authentication fails Performance Cold start of the function cause deploy Unreliable response time Consumer lag behind Event backbone not responding Functions lock an external resource which becomes a bottleneck Backend DB starts to throttle at high load Serverless stars to throttle at high volume Front end times out, no response from the back end Latency measurement Meaningful throughput measurement Batch job impacting main real time processing No more in SLA Retry logic missing jitter causes bursts of retries Message Queues are filled too quickly, causing unhandled messages Network latency impacting call / response - and front end doesn't handle Can't scale up fast enough to support demand spikes Transaction chain scales at different rates or hard limits on one link break scalability Data Loss / Durability Data at rest gets corrupted Error while archiving central logs Every nth transaction fails silently, loses data DR site down Data out of synch between two data sources Can't find a particular orderID in the logs Lack of log files in serverless world Data replicated among active cluster is taking longer than expected Queue exceeds visibility limit Losing messages in kafka Duplicated messages Quality and correctness Idempotency failure causing multiple transactions in error Dual records created in database See unexpected records Version error after code upgrade Data recovery can cause ID duplicates Error reporting not reporting actual problem Are the runbooks autodated? Does simulated transaction bring clear understanding of the process flow and error flow? Inconsistent view between caller and callee? Security Privacy laws enforcement GDPR How to identify data breach Data at rest not encrypted and protected Elevation of privileges on PROD accounts Access to private data Cannot trace who make changes Keep up to date with security patches Open source component with security vulnerability goes undetected","title":"SRE"},{"location":"sre/#system-reliability-engineering","text":"Some potential failures to consider for each deployment:","title":"System Reliability Engineering"},{"location":"sre/#availability","text":"Always ask what are the SLAs Queue full and transaction not being processed API backend not available Network failure DNS problems Update for new version fails to deploy Hitting unknown limits, like number of VPC interfaces Certificate not auto renewed on expiry, and authentication fails","title":"Availability"},{"location":"sre/#performance","text":"Cold start of the function cause deploy Unreliable response time Consumer lag behind Event backbone not responding Functions lock an external resource which becomes a bottleneck Backend DB starts to throttle at high load Serverless stars to throttle at high volume Front end times out, no response from the back end Latency measurement Meaningful throughput measurement Batch job impacting main real time processing No more in SLA Retry logic missing jitter causes bursts of retries Message Queues are filled too quickly, causing unhandled messages Network latency impacting call / response - and front end doesn't handle Can't scale up fast enough to support demand spikes Transaction chain scales at different rates or hard limits on one link break scalability","title":"Performance"},{"location":"sre/#data-loss-durability","text":"Data at rest gets corrupted Error while archiving central logs Every nth transaction fails silently, loses data DR site down Data out of synch between two data sources Can't find a particular orderID in the logs Lack of log files in serverless world Data replicated among active cluster is taking longer than expected Queue exceeds visibility limit Losing messages in kafka Duplicated messages","title":"Data Loss / Durability"},{"location":"sre/#quality-and-correctness","text":"Idempotency failure causing multiple transactions in error Dual records created in database See unexpected records Version error after code upgrade Data recovery can cause ID duplicates Error reporting not reporting actual problem Are the runbooks autodated? Does simulated transaction bring clear understanding of the process flow and error flow? Inconsistent view between caller and callee?","title":"Quality and correctness"},{"location":"sre/#security","text":"Privacy laws enforcement GDPR How to identify data breach Data at rest not encrypted and protected Elevation of privileges on PROD accounts Access to private data Cannot trace who make changes Keep up to date with security patches Open source component with security vulnerability goes undetected","title":"Security"},{"location":"istio/readme/","text":"Use ISTIO for service mesh Some Istio quick summary, and notes from personal study. Last tested on release 1.3.3 on openshift 3.11 under istio-system namespace. Summary The main concepts are presented in the istio main page . Istio helps operators to connect, secure, control and observe services and microservices. It helps managing the service mesh (network of microservices constituting applications). Value propositions Support the cloud native requirements: service discovery, load balancing, failure recovery, metrics, and monitoring, A/B testing, canary rollouts, rate limiting, access control, and end-to-end authentication Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic. Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection. A pluggable policy layer and configuration API supporting access controls, rate limits and quotas. Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress. Secure service-to-service communication in a cluster Architecture The istio service mesh is composed of the data plane and control plane . The data plane groups Envoy proxies deployed as sidecars to the microservice to mediate and control communication. Mixer is a policy and telemetry hub. The control plane includes Pilot and Citadel , and is responsible for managing and configuring proxies to route traffic, and configuring Mixers to enforce policies and collect telemetry. The following is an example of pod assignment within kubernetes. Egress gateway and servicegraph run on a proxy, while the other components run in the worker nodes. The command used to get this assignment are: $ kubectl get nodes $ kubectl describe node <ipaddress> To get the pods: kubectl get pods -n istio-system . The component roles: Component Role Envoy Proxy to mediate all inbound and outbound traffic for all services in the service mesh. It is deployed as a sidecar container inside the same pod as a service. It supports load balancing, circuit breakers, health checks, fault injection, metrics... Mixer Enforces access control and usage policies across the service mesh and collects telemetry data from the Envoy proxy. Pilot Config Envoy and Mixer. Supports service discovery, traffic management, resiliency (timeouts, retries, circuit breakers), intelligent routing (A/B testingm canary deployment..). Citadel Used for service-to-service and end-user authentication. Enforce security policy based on service identity. Ingress/Egress Configure path based routing for inbound and outbound external traffic Control Plane API Underlying Orchestrator such as Kubernetes or Hashicorp Nomad. Installation As Istio is using custom resources like virtualService and destination rules, we need to apply the CRD templates. oc apply -f istio-1.3.3/install/kubernetes/helm/istio/templates/crds.yaml Installing a simple Istio deployment for demonstration purpose use: oc apply -f istio-1.0.5/install/kubernetes/istio-demo.yaml On OpenShift we need to define Route to the Istio services: oc expose svc istio-ingressgateway -n istio-system; \\ oc expose svc servicegraph -n istio-system; \\ oc expose svc grafana -n istio-system; \\ oc expose svc prometheus -n istio-system; \\ oc expose svc tracing -n istio-system CLI istioctl version Deploy an app and inject Envoy sidecar into the same pod. oc apply -f <(istioctl kube-inject -f ../../kubernetes/Deployment.yml) -n tutorial Others Service Graph displays a high-level overview of how systems are connected, a tool called Weave Scope provides a powerful visualisation and debugging tool for the entire cluster kubectl create -f https://cloud.weave.works/launch/k8s/weavescope.yaml A/B testing with routing rules Route 50% traffic to one image Route based on http header argument Compendium Installation on kubernetes Troubleshooting Istio Tutorial on Katacoda Istio on openshift training A repo for demo","title":"Istio"},{"location":"istio/readme/#use-istio-for-service-mesh","text":"Some Istio quick summary, and notes from personal study. Last tested on release 1.3.3 on openshift 3.11 under istio-system namespace.","title":"Use ISTIO for service mesh"},{"location":"istio/readme/#summary","text":"The main concepts are presented in the istio main page . Istio helps operators to connect, secure, control and observe services and microservices. It helps managing the service mesh (network of microservices constituting applications).","title":"Summary"},{"location":"istio/readme/#value-propositions","text":"Support the cloud native requirements: service discovery, load balancing, failure recovery, metrics, and monitoring, A/B testing, canary rollouts, rate limiting, access control, and end-to-end authentication Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic. Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection. A pluggable policy layer and configuration API supporting access controls, rate limits and quotas. Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress. Secure service-to-service communication in a cluster","title":"Value propositions"},{"location":"istio/readme/#architecture","text":"The istio service mesh is composed of the data plane and control plane . The data plane groups Envoy proxies deployed as sidecars to the microservice to mediate and control communication. Mixer is a policy and telemetry hub. The control plane includes Pilot and Citadel , and is responsible for managing and configuring proxies to route traffic, and configuring Mixers to enforce policies and collect telemetry. The following is an example of pod assignment within kubernetes. Egress gateway and servicegraph run on a proxy, while the other components run in the worker nodes. The command used to get this assignment are: $ kubectl get nodes $ kubectl describe node <ipaddress> To get the pods: kubectl get pods -n istio-system . The component roles: Component Role Envoy Proxy to mediate all inbound and outbound traffic for all services in the service mesh. It is deployed as a sidecar container inside the same pod as a service. It supports load balancing, circuit breakers, health checks, fault injection, metrics... Mixer Enforces access control and usage policies across the service mesh and collects telemetry data from the Envoy proxy. Pilot Config Envoy and Mixer. Supports service discovery, traffic management, resiliency (timeouts, retries, circuit breakers), intelligent routing (A/B testingm canary deployment..). Citadel Used for service-to-service and end-user authentication. Enforce security policy based on service identity. Ingress/Egress Configure path based routing for inbound and outbound external traffic Control Plane API Underlying Orchestrator such as Kubernetes or Hashicorp Nomad.","title":"Architecture"},{"location":"istio/readme/#installation","text":"As Istio is using custom resources like virtualService and destination rules, we need to apply the CRD templates. oc apply -f istio-1.3.3/install/kubernetes/helm/istio/templates/crds.yaml Installing a simple Istio deployment for demonstration purpose use: oc apply -f istio-1.0.5/install/kubernetes/istio-demo.yaml On OpenShift we need to define Route to the Istio services: oc expose svc istio-ingressgateway -n istio-system; \\ oc expose svc servicegraph -n istio-system; \\ oc expose svc grafana -n istio-system; \\ oc expose svc prometheus -n istio-system; \\ oc expose svc tracing -n istio-system","title":"Installation"},{"location":"istio/readme/#cli","text":"istioctl version Deploy an app and inject Envoy sidecar into the same pod. oc apply -f <(istioctl kube-inject -f ../../kubernetes/Deployment.yml) -n tutorial","title":"CLI"},{"location":"istio/readme/#others","text":"Service Graph displays a high-level overview of how systems are connected, a tool called Weave Scope provides a powerful visualisation and debugging tool for the entire cluster kubectl create -f https://cloud.weave.works/launch/k8s/weavescope.yaml","title":"Others"},{"location":"istio/readme/#ab-testing-with-routing-rules","text":"Route 50% traffic to one image Route based on http header argument","title":"A/B testing with routing rules"},{"location":"istio/readme/#compendium","text":"Installation on kubernetes Troubleshooting Istio Tutorial on Katacoda Istio on openshift training A repo for demo","title":"Compendium"},{"location":"k8s/k8s-0/","text":"Introduction Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. A Kubernetes cluster consists of one or more physical or virtual machines, also known as worker nodes, that are loosely coupled, extensible, and centrally monitored and managed by the Kubernetes master nodes. A cluster defines a set of resources, nodes, networks, and storage devices that keep applications highly available. Key features Service discovery by assigning a single DNS entry to each set of containers. This permits load-balancing the request across the pool of containers providing the service Horizontal scaling Self-healing use user-defined health checks to monitor containers to restart and reschedule them in case of failure. Automated rollout gradually roll updates out to your application's containers while checking their status. If something goes wrong during the rollout, Kubernetes can roll back to the previous iteration of the deployment. Secrets and configuration management Operators packaged Kubernetes applications that also bring the knowledge of the application's life cycle into the Kubernetes cluster. Applications packaged as Operators use the Kubernetes API to update the cluster's state reacting to changes in the application state. Components The following diagram lists the important components of the cluster (Based on IBM Cloud Private architecture). Master nodes control nodes, and schedule pods. They persist states and configuration in etcd . API server exposes API for the CLI and Web App to validate and support configuration injection kube-control manager is a daemon that embeds other controllers: node, replications, endpoints, and service account controllers. etcd : is a distributed key-value pair datastore to persist configuration, to do service discovery and coordinate distributed work. Backup it. kube-proxy , is present in each node and perform TCP/UDP packet forwarding across the backend network. It is a network proxy for the services defined in the cluster. kubelet is to schedule pods in a node Container images confine the application code, its runtime, and all of its dependencies in a pre-defined format. Container runtime uses those pre-packaged images, to create one or more containers. They run in one host. To have a fault-tolerant and scalable solution we need multiple nodes connected together and controlled by a container orchestrator. It ensures that applications: are fault-tolerant can do horizontal scaling, and do this on-demand. Scale applications based on resource usage like CPU and memory. support automatic binpacking: schedules the containers based on resource usage and constraints, without sacrifying the availability are self-healed: automatically replaces and reschedules the containers from failed node can discover other applications automatically, and communicate with each other groups sets of containers and refers to them via a DNS name (called a service). It can discover these services automatically, and load-balance requests between containers of a given service are accessible from the external world can update/rollback, without any downtime, new versions/configurations of an application access storage orchestrated via Software Defined Storage support batch execution support VMs, bare-metal, or public/private/hybrid/multi-cloud setups Value Propositions The key paradigm of kubernetes is it\u2019s Declarative model : you provide the \"desired state\" and Kubernetes will do it's best to make it happens. high availability 24/7 deploy new version multiple times a day containerization of apps and business services helps you make sure those containerized applications run where and when you want, and helps them find the resources and tools they need to work Single-tenant Kubernetes clusters with compute, network and storage infrastructure isolation Automatic scaling of apps Use the cluster dashboard to quickly see and manage the health of your cluster, worker nodes, and container deployments. Automatic re-creation of containers in case of failures Polyglote application Value propositions for container Just to recall the value of using container for the cloud native application are the following: Docker ensures consistent environments from development to production. Docker containers are configured to maintain all configurations and dependencies internally. Docker containers allows you to commit changes to your Docker image and version control them. It is very easy to rollback to a previous version of your Docker image. This whole process can be tested in a few minutes. Docker is fast, allowing you to quickly make replications and achieve redundancy. Isolation: Docker makes sure each container has its own resources that are isolated from other containers Removing an app/ container is easy and won\u2019t leave any temporary or configuration files on your host OS. Docker ensures that applications that are running on containers are completely segregated and isolated from each other, granting you complete control over traffic flow and management The container filesystem is represented as a list of read-only layers stacked on top of each other using a storage driver. The layers are generated when commands are executed during the Docker image build process. The top layer has read-write permissions. Docker daemon configuration is managed by the Docker configuration file (/etc/docker/daemon.json) and Docker daemon startup options are usually controlled by the systemd unit: Docker . With environment variables you can control one container, while using linked containers docker automatically copies all environment variables from one container to another. IKS value propositions Simplified Cluster Management CLI and API Intuitive UI Fully managed master nodes. Always 3 masters. User controlled worker node management: Worker node auto-recovery Worker node on GPU Design your own cluster Tunable capacity Edge nodes support Integrated VPN in-cluster providing IPSec tunnels Configurable network Security Vulnaribility advisor All secrets are encrypted. Concepts Kubernetes is a system to run docker containers spanning multiple physical machines. It groups containers that make up an application into logical units for easy management and discovery. Master Nodes are responsible for managing the Kubernetes cluster, and are the entry points for all administrative tasks. We can communicate to the Master Node via the CLI, the GUI (Dashboard), or via APIs. For fault tolerance purposes, there can be more than one Master Node in the cluster. To manage the cluster state, Kubernetes uses etcd , and all Master Nodes connect to it. A worker node is the machine (VM or physical), which runs the applications using Pods and is controlled by the Master Node. A Pod is the scheduling unit in Kubernetes. An app in production runs replicas of the app across multiple worker nodes to provide higher availability for your app. Every containerized app that is deployed into a Kubernetes cluster is deployed, run, and managed by a pod . An app might require a container and other helper containers to be deployed into one pod, so that those containers can be addressed by using the same private IP address Etcd Kubelet : a node agent that runs on each node. It ensures that the containers described by PodSpecs are running and healthy Kube-proxy : Runs on each node, perform UDP and TCP stream forwarding for services defined in the cluster. It manages a collection of iptables rules, to implement a form of virtual IP for Services . When a container image runs, it is also executed using namespaces in the operating system. These namespaces contain the process and provide isolation: running container has its own separated file system, own network, own process identifier namespace (PID). Control Group (CGROUP) allows isolation of hardware resource. The Container Runtime offloads the IP assignment to CNI Container Network Interface: https://github.com/containernetworking/cni. CNI is a specification to define how network interfaces are set for container runtime. In cluster, pod to pod communication should happen across nodes, without any Network Address Translation. So k8s uses Software Define Networking like Calico to support networking https://www.projectcalico.org/ Kubernetes Objects: With each object, you declare the intent or desired state using the spec attribute. To create an object, we need to provide the spec field to the Kubernetes API Server. The spec field describes the desired state, along with some basic information, like the name. Pod : A Pod is a logical collection of one or more containers Labels : are key-value pairs that can be attached to any Kubernetes objects. Labels are used to organize and select a subset of objects Label Selectors , we can select a subset of objects. Two types: Equality-Based Selectors allow filtering of objects based on label keys and values Set-Based Selectors allow filtering of objects based on a set of values ( in, notin, and exist operators) A ReplicationController (rc) is a controller that is part of the Master Node's Controller Manager. It makes sure the specified number of replicas for a Pod is running at any given point in time. A ReplicaSet (rs) is the next-generation ReplicationController. ReplicaSets support both equality- and set-based Selectors, whereas ReplicationControllers only support equality-based Selectors. A Deployment automatically creates the ReplicaSets. Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of the Master Node's Controller Manager, and it makes sure that the current state always matches the desired state. Deployments include the definitions for the app to run, it references the docker image to use and which port number exposed to access the app. When you create a deployment, a Kubernetes pod is created for each container that you defined in the deployment. To make your app more resilient, you can define multiple instances of the same app in your deployment and let Kubernetes automatically create a Replica set for you. The kubernetes Deployment is responsible for creating and updating instances of your application. Once you've created a Deployment, the Kubernetes master schedules the application instances that the Deployment creates onto individual Nodes in the cluster. When a version of the container image change it is possible to deploy it, and it will create a new replication set. This process is referred to as a Deployment rollout. Once ReplicaSet B is ready, the Deployment starts pointing to it. On top of ReplicaSets, Deployments provide features like Deployment recording, with which, if something goes wrong, we can rollback to a previously known state. namespace : If we have numerous users whom we would like to organize into teams/projects, we can partition the Kubernetes cluster into sub-clusters using Namespaces. The names of the resources/objects created inside a Namespace are unique, but not across Namespaces. Generally, Kubernetes creates two default namespaces: kube-system and default. Using Resource Quotas, we can divide the cluster resources within Namespaces Services : allow containers in one pod to open network connections to containers in another pod. In the declaration, the targetPort attribute has to match a containerPort from a pod container definition, and the port attribute is the port that is exposed by the service. selector is how the service finds pods to forward packets to. Each service is dynamically assigned an SRV record with an FQDN of the form: SVC_NAME.PROJECT_NAME.svc.cluster.local kubelet is an agent which runs on each Worker Node and communicates with the Master Node. It makes sure the containers which are part of the Pods are healthy at all times Kube-proxy is the network proxy which runs on each Worker Node and listens to the API Server for each Service endpoint creation/deletion. Running as a daemon inside worker node, it watches the API Server on the Master Node for the addition and removal of Services and Endpoints. For each new Service, on each node, kube-proxy configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints. Volumes All data stored inside a container is deleted if the container crashes. A Volume is essentially a directory backed by a storage medium. The storage medium and its content are determined by the Volume Type. A Volume is attached to a Pod and shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts. A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator. It is a resource in the cluster A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume PV resources. A StorageClass provides a way for administrators to describe the \u201cclasses\u201d of storage they offer. Different classes might map to quality-of-service levels, and to backup policies. Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy. vSphere, minio, GlusterFS, NFS are storage class. IBM summary on PV, PVC Some concepts: Platform Storage: use hostPath storage. Consider to make those paths separate, expandable disks so they can be extended as needed. Block Storage: PV is a block of storage. There are two ways PVs may be provisioned: statically or dynamically. Static : A cluster administrator creates a number of PVs. They carry the details of the real storage which is available for use by cluster users. Cluster administrators must create their disks and export their NFS shares in order for Kubernetes to mount them. Create a PersistentVolume yaml file Dynamic : When none of the static PVs the administrator created matches a user\u2019s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses : the PVC must request a class and the administrator must have created and configured that class in order for dynamic provisioning to occur. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a pod. Master nodes need to run on fast disks because they do use a lot of IO because of ETCD. SSD type. Do not run with thin provisioning for VM Separate SAN per cluster. When the cluster runs long time, the amount data persisted will become bigger Do not use hostPath storage for user's workload? NFS shared storage is used for the docker trusted registry imave repository, but when using a load balancer in fron of multiple master nodes rather than VIP. NFS can be a problem due to simultaneous pushes to different nodes of the images. If NFS sync does happend gast enough you can get 'unknown blob' errors. NFS shared storage can add significant load to the data network at times of high usage. Openshift deprecated NFS for the cluster. With SAN the bottle neck is the sas controller. KVM. kernel based VM is a linux feature to do virutalization. It is faster than VM. NVMe non-volatile memory express. Do not assume IOPS because of the hardware used, verify configuration. The tool to test io is fio Read more ConfigMaps Pass configuration parameter into the runtime pods without creating different docker images. ConfigMaps allow us to decouple the configuration details from the container image. Create a ConfigMap with command: kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2 or from yaml in which we mentioned the kind, metadata, and data fields, which are targeted to connect with the v1 endpoint of the API Server apiVersion : v1 kind : ConfigMap metadata : name : customer1 data : TEXT1 : Customer1_Company TEXT2 : Welcomes You ConfigMaps should reference properties files, not replace them. Secret With Secrets, we can share sensitive information like passwords, tokens, or keys in the form of key-value pairs. In Deployments or other system components, the Secret object is referenced, without exposing its content. create a single line file, for example to include a password. kubectl create secret generic my-password --from-file=password.txt We can get Secrets to be used by containers in a Pod by mounting them as data volumes >> NEXT","title":"Introduction"},{"location":"k8s/k8s-0/#introduction","text":"Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. A Kubernetes cluster consists of one or more physical or virtual machines, also known as worker nodes, that are loosely coupled, extensible, and centrally monitored and managed by the Kubernetes master nodes. A cluster defines a set of resources, nodes, networks, and storage devices that keep applications highly available.","title":"Introduction"},{"location":"k8s/k8s-0/#key-features","text":"Service discovery by assigning a single DNS entry to each set of containers. This permits load-balancing the request across the pool of containers providing the service Horizontal scaling Self-healing use user-defined health checks to monitor containers to restart and reschedule them in case of failure. Automated rollout gradually roll updates out to your application's containers while checking their status. If something goes wrong during the rollout, Kubernetes can roll back to the previous iteration of the deployment. Secrets and configuration management Operators packaged Kubernetes applications that also bring the knowledge of the application's life cycle into the Kubernetes cluster. Applications packaged as Operators use the Kubernetes API to update the cluster's state reacting to changes in the application state.","title":"Key features"},{"location":"k8s/k8s-0/#components","text":"The following diagram lists the important components of the cluster (Based on IBM Cloud Private architecture). Master nodes control nodes, and schedule pods. They persist states and configuration in etcd . API server exposes API for the CLI and Web App to validate and support configuration injection kube-control manager is a daemon that embeds other controllers: node, replications, endpoints, and service account controllers. etcd : is a distributed key-value pair datastore to persist configuration, to do service discovery and coordinate distributed work. Backup it. kube-proxy , is present in each node and perform TCP/UDP packet forwarding across the backend network. It is a network proxy for the services defined in the cluster. kubelet is to schedule pods in a node Container images confine the application code, its runtime, and all of its dependencies in a pre-defined format. Container runtime uses those pre-packaged images, to create one or more containers. They run in one host. To have a fault-tolerant and scalable solution we need multiple nodes connected together and controlled by a container orchestrator. It ensures that applications: are fault-tolerant can do horizontal scaling, and do this on-demand. Scale applications based on resource usage like CPU and memory. support automatic binpacking: schedules the containers based on resource usage and constraints, without sacrifying the availability are self-healed: automatically replaces and reschedules the containers from failed node can discover other applications automatically, and communicate with each other groups sets of containers and refers to them via a DNS name (called a service). It can discover these services automatically, and load-balance requests between containers of a given service are accessible from the external world can update/rollback, without any downtime, new versions/configurations of an application access storage orchestrated via Software Defined Storage support batch execution support VMs, bare-metal, or public/private/hybrid/multi-cloud setups","title":"Components"},{"location":"k8s/k8s-0/#value-propositions","text":"The key paradigm of kubernetes is it\u2019s Declarative model : you provide the \"desired state\" and Kubernetes will do it's best to make it happens. high availability 24/7 deploy new version multiple times a day containerization of apps and business services helps you make sure those containerized applications run where and when you want, and helps them find the resources and tools they need to work Single-tenant Kubernetes clusters with compute, network and storage infrastructure isolation Automatic scaling of apps Use the cluster dashboard to quickly see and manage the health of your cluster, worker nodes, and container deployments. Automatic re-creation of containers in case of failures Polyglote application","title":"Value Propositions"},{"location":"k8s/k8s-0/#value-propositions-for-container","text":"Just to recall the value of using container for the cloud native application are the following: Docker ensures consistent environments from development to production. Docker containers are configured to maintain all configurations and dependencies internally. Docker containers allows you to commit changes to your Docker image and version control them. It is very easy to rollback to a previous version of your Docker image. This whole process can be tested in a few minutes. Docker is fast, allowing you to quickly make replications and achieve redundancy. Isolation: Docker makes sure each container has its own resources that are isolated from other containers Removing an app/ container is easy and won\u2019t leave any temporary or configuration files on your host OS. Docker ensures that applications that are running on containers are completely segregated and isolated from each other, granting you complete control over traffic flow and management The container filesystem is represented as a list of read-only layers stacked on top of each other using a storage driver. The layers are generated when commands are executed during the Docker image build process. The top layer has read-write permissions. Docker daemon configuration is managed by the Docker configuration file (/etc/docker/daemon.json) and Docker daemon startup options are usually controlled by the systemd unit: Docker . With environment variables you can control one container, while using linked containers docker automatically copies all environment variables from one container to another.","title":"Value propositions for container"},{"location":"k8s/k8s-0/#iks-value-propositions","text":"Simplified Cluster Management CLI and API Intuitive UI Fully managed master nodes. Always 3 masters. User controlled worker node management: Worker node auto-recovery Worker node on GPU Design your own cluster Tunable capacity Edge nodes support Integrated VPN in-cluster providing IPSec tunnels Configurable network Security Vulnaribility advisor All secrets are encrypted.","title":"IKS value propositions"},{"location":"k8s/k8s-0/#concepts","text":"Kubernetes is a system to run docker containers spanning multiple physical machines. It groups containers that make up an application into logical units for easy management and discovery. Master Nodes are responsible for managing the Kubernetes cluster, and are the entry points for all administrative tasks. We can communicate to the Master Node via the CLI, the GUI (Dashboard), or via APIs. For fault tolerance purposes, there can be more than one Master Node in the cluster. To manage the cluster state, Kubernetes uses etcd , and all Master Nodes connect to it. A worker node is the machine (VM or physical), which runs the applications using Pods and is controlled by the Master Node. A Pod is the scheduling unit in Kubernetes. An app in production runs replicas of the app across multiple worker nodes to provide higher availability for your app. Every containerized app that is deployed into a Kubernetes cluster is deployed, run, and managed by a pod . An app might require a container and other helper containers to be deployed into one pod, so that those containers can be addressed by using the same private IP address Etcd Kubelet : a node agent that runs on each node. It ensures that the containers described by PodSpecs are running and healthy Kube-proxy : Runs on each node, perform UDP and TCP stream forwarding for services defined in the cluster. It manages a collection of iptables rules, to implement a form of virtual IP for Services . When a container image runs, it is also executed using namespaces in the operating system. These namespaces contain the process and provide isolation: running container has its own separated file system, own network, own process identifier namespace (PID). Control Group (CGROUP) allows isolation of hardware resource. The Container Runtime offloads the IP assignment to CNI Container Network Interface: https://github.com/containernetworking/cni. CNI is a specification to define how network interfaces are set for container runtime. In cluster, pod to pod communication should happen across nodes, without any Network Address Translation. So k8s uses Software Define Networking like Calico to support networking https://www.projectcalico.org/","title":"Concepts"},{"location":"k8s/k8s-0/#kubernetes-objects","text":"With each object, you declare the intent or desired state using the spec attribute. To create an object, we need to provide the spec field to the Kubernetes API Server. The spec field describes the desired state, along with some basic information, like the name. Pod : A Pod is a logical collection of one or more containers Labels : are key-value pairs that can be attached to any Kubernetes objects. Labels are used to organize and select a subset of objects Label Selectors , we can select a subset of objects. Two types: Equality-Based Selectors allow filtering of objects based on label keys and values Set-Based Selectors allow filtering of objects based on a set of values ( in, notin, and exist operators) A ReplicationController (rc) is a controller that is part of the Master Node's Controller Manager. It makes sure the specified number of replicas for a Pod is running at any given point in time. A ReplicaSet (rs) is the next-generation ReplicationController. ReplicaSets support both equality- and set-based Selectors, whereas ReplicationControllers only support equality-based Selectors. A Deployment automatically creates the ReplicaSets. Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of the Master Node's Controller Manager, and it makes sure that the current state always matches the desired state. Deployments include the definitions for the app to run, it references the docker image to use and which port number exposed to access the app. When you create a deployment, a Kubernetes pod is created for each container that you defined in the deployment. To make your app more resilient, you can define multiple instances of the same app in your deployment and let Kubernetes automatically create a Replica set for you. The kubernetes Deployment is responsible for creating and updating instances of your application. Once you've created a Deployment, the Kubernetes master schedules the application instances that the Deployment creates onto individual Nodes in the cluster. When a version of the container image change it is possible to deploy it, and it will create a new replication set. This process is referred to as a Deployment rollout. Once ReplicaSet B is ready, the Deployment starts pointing to it. On top of ReplicaSets, Deployments provide features like Deployment recording, with which, if something goes wrong, we can rollback to a previously known state. namespace : If we have numerous users whom we would like to organize into teams/projects, we can partition the Kubernetes cluster into sub-clusters using Namespaces. The names of the resources/objects created inside a Namespace are unique, but not across Namespaces. Generally, Kubernetes creates two default namespaces: kube-system and default. Using Resource Quotas, we can divide the cluster resources within Namespaces Services : allow containers in one pod to open network connections to containers in another pod. In the declaration, the targetPort attribute has to match a containerPort from a pod container definition, and the port attribute is the port that is exposed by the service. selector is how the service finds pods to forward packets to. Each service is dynamically assigned an SRV record with an FQDN of the form: SVC_NAME.PROJECT_NAME.svc.cluster.local kubelet is an agent which runs on each Worker Node and communicates with the Master Node. It makes sure the containers which are part of the Pods are healthy at all times Kube-proxy is the network proxy which runs on each Worker Node and listens to the API Server for each Service endpoint creation/deletion. Running as a daemon inside worker node, it watches the API Server on the Master Node for the addition and removal of Services and Endpoints. For each new Service, on each node, kube-proxy configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints.","title":"Kubernetes Objects:"},{"location":"k8s/k8s-0/#volumes","text":"All data stored inside a container is deleted if the container crashes. A Volume is essentially a directory backed by a storage medium. The storage medium and its content are determined by the Volume Type. A Volume is attached to a Pod and shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts. A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator. It is a resource in the cluster A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume PV resources. A StorageClass provides a way for administrators to describe the \u201cclasses\u201d of storage they offer. Different classes might map to quality-of-service levels, and to backup policies. Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy. vSphere, minio, GlusterFS, NFS are storage class. IBM summary on PV, PVC Some concepts: Platform Storage: use hostPath storage. Consider to make those paths separate, expandable disks so they can be extended as needed. Block Storage: PV is a block of storage. There are two ways PVs may be provisioned: statically or dynamically. Static : A cluster administrator creates a number of PVs. They carry the details of the real storage which is available for use by cluster users. Cluster administrators must create their disks and export their NFS shares in order for Kubernetes to mount them. Create a PersistentVolume yaml file Dynamic : When none of the static PVs the administrator created matches a user\u2019s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses : the PVC must request a class and the administrator must have created and configured that class in order for dynamic provisioning to occur. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a pod. Master nodes need to run on fast disks because they do use a lot of IO because of ETCD. SSD type. Do not run with thin provisioning for VM Separate SAN per cluster. When the cluster runs long time, the amount data persisted will become bigger Do not use hostPath storage for user's workload? NFS shared storage is used for the docker trusted registry imave repository, but when using a load balancer in fron of multiple master nodes rather than VIP. NFS can be a problem due to simultaneous pushes to different nodes of the images. If NFS sync does happend gast enough you can get 'unknown blob' errors. NFS shared storage can add significant load to the data network at times of high usage. Openshift deprecated NFS for the cluster. With SAN the bottle neck is the sas controller. KVM. kernel based VM is a linux feature to do virutalization. It is faster than VM. NVMe non-volatile memory express. Do not assume IOPS because of the hardware used, verify configuration. The tool to test io is fio Read more","title":"Volumes"},{"location":"k8s/k8s-0/#configmaps","text":"Pass configuration parameter into the runtime pods without creating different docker images. ConfigMaps allow us to decouple the configuration details from the container image. Create a ConfigMap with command: kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2 or from yaml in which we mentioned the kind, metadata, and data fields, which are targeted to connect with the v1 endpoint of the API Server apiVersion : v1 kind : ConfigMap metadata : name : customer1 data : TEXT1 : Customer1_Company TEXT2 : Welcomes You ConfigMaps should reference properties files, not replace them.","title":"ConfigMaps"},{"location":"k8s/k8s-0/#secret","text":"With Secrets, we can share sensitive information like passwords, tokens, or keys in the form of key-value pairs. In Deployments or other system components, the Secret object is referenced, without exposing its content. create a single line file, for example to include a password. kubectl create secret generic my-password --from-file=password.txt We can get Secrets to be used by containers in a Pod by mounting them as data volumes >> NEXT","title":"Secret"},{"location":"k8s/k8s-1/","text":"Networking With kubernetes it is important to understand how the networking, load balancing, security and service communication, work. In multi-host deployment, you need to understand how containers are communicating within a host and between different hosts. The Cloud Native Computing Foundation (CNCF) sponsors the Container Networking Interface (CNI) open source project. The CNI project aims to standardize the network interface for containers in cloud native environments, such as Kubernetes. Docker uses the CNI project to implement a software-defined network (SDN) for containers on each host. Docker networking With Docker there are four networking mode approaches: bridge, host, container or no networking mode. With bridge mode docker daemon creates a docker0 virtual ethernet bridge to forward packets to any interface attached to it. All containers running on the host are attached to this bridge, and peer attached to the container eth0 interface, They got an IP address and are part of a subnet. The default bridge network is present on all Docker hosts. If you do not specify a different network, new containers are automatically connected to the default bridge network: Containers are isolated from the host network. Docker containers can talk to other containers only if they are on the same machine. Because of network isolation, a container in one SDN can have the same IP address as a container in a different SDN. With Host mode ( docker run -d --net=host yourimagehere ) the containers share IP and namespace, and you need to take care of the port numbering schema. There is no routing overhead, but this approach exposes the container to the network. Docker does not support automatic service discovery on the default bridge network. If you want containers to be able to resolve IP addresses by container name, you should use user-defined networks instead. Exposing ports in a Dockerfile is a way of documenting which ports are used, but does not actually map or open any ports. When starting the container the exposed port is published. So for previous figure, we have: $ docker run \u2013d -p 8081:8080 imagename When using nodePort a dynamic published port number is generated by kubernetes. When a pod contains two containers, they run their own cgroup , but shares IPC, Network, and UTC (hostname) namespaces. This can be proven using the following commands: oc exec -it my-two-container-pod -c side-car -- /bin/sh ip address netstat -ntlp hostname Kubernetes networking Kubernetes uses the container mode , where other containers share the same IP address as a previously created containers within the pod: it is set at the pod scope level. It requires each pod to have an IP in a flat networking namespace with full connectivity to other nodes and pods across the network. The Pod IP address is allocated by Calico IPAM and the networking interface definition within the pod and host is done by Calico CNI plugin. The following diagram illustrates the process: The following commands help to see these IP schema: $ kubectl get node -o wide NAME STATUS VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION 172.16.40.137 Ready v1.10.0+icp-ee <none> Ubuntu 16.04.3 LTS 4.4.0-112-generic 172.16.40.139 Ready v1.10.0+icp-ee <none> Ubuntu 16.04.3 LTS 4.4.0-112-generic # The 172.16.40.137 is the IP address of the HOST $ kubectl describe node 172.16.40.137 # but ssh to the host/node and looking at the interface, we can see the ethernet and docker interfaces: $ ifconfig ens160 Link encap:Ethernet HWaddr 00:50:56:a5:cf:34 inet addr:172.16.40.137 Bcast:172.16.255.255 Mask:255.255.0.0 docker0 Link encap:Ethernet HWaddr 02:42:73:1c:72:ca inet addr:172.17.0.1 Bcast:172.17.255.255 Mask:255.255.0.0 This IP-per-pod model yields a backward-compatible way for you to treat a pod almost identically to a VM or a physical host, in the context of naming, service discovery, or port allocations. Containers inside a pod can communicate with each other using localhost . Kubernetes using Calico as Container Network Interface is configured with a large flat subnet (e.g. 172.30.0.0/16) that is considered internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node. When a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node. Kube-proxy intercepts and controls where to forward the traffic, internal to the node, or to another worker node running the destination pod, or outside of the cluster. For inter-pod networking as pod has its own routable IP address there is no need for proxies or NAT. Except that with Kubernetes, services use a completely separate set of IP addresses. The IP addresses are not known by Calico, and are not directly routable. However, they are still reachable by pods since the kube-proxy performs NAT on those IPs (NAT from svc IP@ to Pod IP @). Any pod can communicate with any another pod using its assigned IP address, even if it\u2019s on a different worker node. K8s Services are used as virtual IP address (it is k8s concept), that are discovered via DNS and allow clients to reliably discover and connect to containers. Services Services group a set of pods and provide network connection to these pods for other services in the cluster without exposing the actual private IP address of each pod. https://kubernetes.io/docs/concepts/services-networking/service/ A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. As Pods are ephemeral in nature, resources like IP addresses allocated to it cannot be static. As an example the pods running for the BFF apps are: $ kubectl get pods -n greencompute -o wide NAME READY STATUS RESTARTS AGE IP NODE gc-caseportal-bc-caseportal-app-698f98d787-56qz5 1/1 Running 0 5d 192.168.130.74 172.16.40.138 gc-caseportal-bc-caseportal-app-698f98d787-bhl5f 1/1 Running 0 5d 192.168.223.17 172.16.40.137 The 192.168.x.x is a flat, cluster wide, address space. As illustrated in the figure below: The command shows the eth0 interface in the container is map to this IP address. ``` $ kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- ip addr .... 4: eth0@if16: mtu 1500 qdisc noqueue state UP link/ether c2:74:08:4a:69:3f brd ff:ff:ff:ff:ff:ff inet 192.168.130.74/32 scope global eth0 You can use Kubernetes services to make an app available to other pods inside the cluster or to expose an app to the internet by abstracting the dynamic IP address assignment. The yaml below creates the casewebportal-svc: `kubectl apply -f svc.yml` ```yaml apiVersion: v1 kind: Service metadata: name: casewebportal-svc namespace: greencompute labels: app: casewebportal spec: type: NodePort ports: - port: 6100 targetPort: 6100 protocol: TCP name: http selector: app: casewebportal By default, each Service also gets an IP address (known as ClusterIP ), which is routable only inside the cluster. The target port 6100 is visible as 31560 on the node $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE casewebportal-svc NodePort 10.10.10.197 <none> 6100:31560/TCP 6d A Service is backed by a group of Pods. These Pods are exposed through endpoints. The command below displays the endpoints. The port number stays the same on all worker nodes and event Proxy node. $ kubectl get ep casewebportal-svc NAME ENDPOINTS AGE casewebportal-svc 192.168.130.74:6100,192.168.223.17:6100 6d When a Pod runs on a Node, the kubelet adds a set of environment variables for each active Service. You can see those variables with the command: $ kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- printenv A Service does the load balancing while selecting the Pods for forwarding the data/traffic. It uses the label and label selector to get access to the pods. Kubernetes offers a DNS cluster addon Service that automatically assigns dns names to other Services: $ kubectl get services kube-dns --namespace=kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.10.10.10 <none> 53/UDP,53/TCP 10d So we can do a nslookup within any pod to assess where to find the service: $ kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- nslookup casewebportal-svc nslookup: can't resolve '(null)': Name does not resolve Name: casewebportal-svc Address 1: 10.10.10.197 casewebportal-svc.greencompute.svc.cluster.local You can access ClusterIP service using the cluster local proxy: * Start the kubernetes local proxy: kubectl proxy --port=8080 * Access the service via url like: http://localhost:8080/api/v1/proxy/namespaces/NAMESPACE/services/SERVICE-NAME:PORT-NAME/ We can define a service without selectors, so no Endpoint object will be created, and we can map to your endpoint via the subset.addresses.ip spec. This is useful to abstract other kinds of backend component like DB server, a service in another namespace... The NodePort ServiceType is useful when we want to make the services accessible from the external world. BUT if the IP address of the VM change you need to take care of this problem in your client code. The end-user connects to the Worker Nodes on the specified port, which forwards the traffic to the applications running inside the cluster. To access the application from the external world, administrators can configure a reverse proxy outside the Kubernetes cluster and map the specific endpoint to the respective port on the Worker Nodes. See this article: https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0 Using NGINX server as front end and load balancer, the routing can be done outside the cluster. The following conf file an be deployed in the /etc/nginx/conf.d folder and will help to load balance between two pods: upstream caseportal { server 172.16.40.137:31560; server 172.16.40.138:31560; } server { location / { proxy_pass http://caseportal; } } Alternate is to use LoadBalancer service. With the LoadBalancer ServiceType: NodePort and ClusterIP Services are automatically created, and the external load balancer will route to them. The Services are exposed at a static port on each Worker Node. The Service is exposed externally using the underlying Cloud provider's load balancer feature. If you want to directly expose a service, this is the default method. All traffic on the port you specify will be forwarded to the service. Finally,the last model is by using ingress. Ingress Ingress is another method (from LoadBalancer and NodePort) we can use to access our applications from the external world. It works at layer 7 traffic. The Kubernetes cluster must have an Ingress controller deployed to be able to create Ingress resources. The Ingress controller is deployed as a Docker container. Its Docker image contains a load balancer like NGInx and a controller daemon. The controller daemon receives the desired Ingress configuration from Kubernetes, it generates an NGInx configuration file and restarts the load balancer process for changes to take effect. Ingress controller is a load balancer managed by Kubernetes. Ingress helps to decouple the routing rules from the application, we can then update our application without worrying about its external access. With Ingress, users don't connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the respective Service. You can define one ingress for all the component of your application you want to expose to external world. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : web-ingress spec : rules : - host : myservice.mydomain.com http : paths : - path : /dashboardbff - backend : serviceName : dashboard-bff-svc servicePort : 8080 - host : green.myweb.com http : paths : - backend : serviceName : green-service servicePort : 80 Service Discovery As soon as the Pod starts on any Worker Node, the kubelet daemon running on that node adds a set of environment variables in the Pod for all active Services. Be careful while ordering Services, as the Pods will not have the environment variables set for Services which are created after the Pods are created. The DNS add-on, creates a DNS record for each normal Service and its format is like my-svc.my-namespace.svc.cluster.local within the same namespace can reach to other services with just their name. The following diagram groups all those networking concepts to illustrates, Ingress, service, kube-proxy and label selectors: Kubernetes DNS Kubelets resolve hostnames for pods through a Service named kube-dns . kube-dns runs as a pod in the kube-system namespace. Every Service that is created is automatically assigned a DNS name. By default, this hostname is ServiceName.Namespace . All Services that are in the same namespace may omit Namespace and just use ServiceName . Here is an example: To understand the default kubernetes DNS pod, you can run k describe pod kube-dns-nw8gf -n kube-system It watches the Kubernetes master for changes in Services and Endpoints, and maintains in-memory lookup structures to serve DNS requests. It has a static address, and kubelet passes DNS to each container with the --cluster-dns= flag. The default domain is cluster.local . Virtual Private Cloud VPC delivers networking functions for cloud deployments in logically isolated segments using Software Defined Networking. It is a virtual networking platform tied to a tenant account. It provides logical isolation within the same tenant and other tenants. Zones are created using high bandwidth and low latency connections (3 ms). A VPC spans multiple zones at each region for high availability. It does not span regions. With VPC, operators can customize network topologies, sizes, IP addressing without NAT or tunnels. It supports bring your own IP address plan. The security controls are done at the subnet and server level. Some important features: A region has three or more zones. Multi-zone regions is designed to support enterprise workloads accross application, data, security and solution domains. Availability zone is an independent fault domains that do not share physical infrastructure. It has a latency requirement of <500 micro second intra-zone and <2 ms inter-zone. Security With kubernetes deployed internally, companies have control over their physical environment, network, operating system patches, access to authentication and authorization internal registry. They control data access, access to administration console for the platform and the workload deployed. As part of security is the Identity and access management to control access to resource and connection protocol like OpenID . A team is a logical grouping of resources, users, and user groups. Teams can be restricted to all resources within a namespace. Access control gateway enforces role-based access control (RBAC) for all registered services. Network segmentation to provide isolation, using DMZ VLAN for specific workload, so deploying cluster in DMZ. And use VLAN to segment the physical network, subnets, clusters and namespaces to add more segmentation. Segmentation will depend of the application requirements. For securing kubernetes network control, you will have Edge nodes that are worker nodes with only ingress workloads and network policies. Traffic reaches application through private network. Backend services may not have access from public network. K8s network policy specifies how pods can communicate with each other, and Calico , the IP based networking, implements the policies (rule per IP addresses). Rules are ORe'ed, if any ingress /egress rule matches connection is allowed. Their are scoped at namespace level. We can isolate application using labels. THe srv1 backend can be access only from the frontend kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: backend-allow spec: podSelector: matchLabels: app: Srv1 Tier: backend ingress: - from: - podSelector: matchLabels: app: Srv1 Tier: frontend and the db backend can only be access from the srv1 backend. kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: db-allow spec: podSelector: matchLabels: app: Srv1 Tier: db ingress: - from: - podSelector: matchLabels: app: Srv1 Tier: backend >> NEXT","title":"Networking"},{"location":"k8s/k8s-1/#networking","text":"With kubernetes it is important to understand how the networking, load balancing, security and service communication, work. In multi-host deployment, you need to understand how containers are communicating within a host and between different hosts. The Cloud Native Computing Foundation (CNCF) sponsors the Container Networking Interface (CNI) open source project. The CNI project aims to standardize the network interface for containers in cloud native environments, such as Kubernetes. Docker uses the CNI project to implement a software-defined network (SDN) for containers on each host.","title":"Networking"},{"location":"k8s/k8s-1/#docker-networking","text":"With Docker there are four networking mode approaches: bridge, host, container or no networking mode. With bridge mode docker daemon creates a docker0 virtual ethernet bridge to forward packets to any interface attached to it. All containers running on the host are attached to this bridge, and peer attached to the container eth0 interface, They got an IP address and are part of a subnet. The default bridge network is present on all Docker hosts. If you do not specify a different network, new containers are automatically connected to the default bridge network: Containers are isolated from the host network. Docker containers can talk to other containers only if they are on the same machine. Because of network isolation, a container in one SDN can have the same IP address as a container in a different SDN. With Host mode ( docker run -d --net=host yourimagehere ) the containers share IP and namespace, and you need to take care of the port numbering schema. There is no routing overhead, but this approach exposes the container to the network. Docker does not support automatic service discovery on the default bridge network. If you want containers to be able to resolve IP addresses by container name, you should use user-defined networks instead. Exposing ports in a Dockerfile is a way of documenting which ports are used, but does not actually map or open any ports. When starting the container the exposed port is published. So for previous figure, we have: $ docker run \u2013d -p 8081:8080 imagename When using nodePort a dynamic published port number is generated by kubernetes. When a pod contains two containers, they run their own cgroup , but shares IPC, Network, and UTC (hostname) namespaces. This can be proven using the following commands: oc exec -it my-two-container-pod -c side-car -- /bin/sh ip address netstat -ntlp hostname","title":"Docker networking"},{"location":"k8s/k8s-1/#kubernetes-networking","text":"Kubernetes uses the container mode , where other containers share the same IP address as a previously created containers within the pod: it is set at the pod scope level. It requires each pod to have an IP in a flat networking namespace with full connectivity to other nodes and pods across the network. The Pod IP address is allocated by Calico IPAM and the networking interface definition within the pod and host is done by Calico CNI plugin. The following diagram illustrates the process: The following commands help to see these IP schema: $ kubectl get node -o wide NAME STATUS VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION 172.16.40.137 Ready v1.10.0+icp-ee <none> Ubuntu 16.04.3 LTS 4.4.0-112-generic 172.16.40.139 Ready v1.10.0+icp-ee <none> Ubuntu 16.04.3 LTS 4.4.0-112-generic # The 172.16.40.137 is the IP address of the HOST $ kubectl describe node 172.16.40.137 # but ssh to the host/node and looking at the interface, we can see the ethernet and docker interfaces: $ ifconfig ens160 Link encap:Ethernet HWaddr 00:50:56:a5:cf:34 inet addr:172.16.40.137 Bcast:172.16.255.255 Mask:255.255.0.0 docker0 Link encap:Ethernet HWaddr 02:42:73:1c:72:ca inet addr:172.17.0.1 Bcast:172.17.255.255 Mask:255.255.0.0 This IP-per-pod model yields a backward-compatible way for you to treat a pod almost identically to a VM or a physical host, in the context of naming, service discovery, or port allocations. Containers inside a pod can communicate with each other using localhost . Kubernetes using Calico as Container Network Interface is configured with a large flat subnet (e.g. 172.30.0.0/16) that is considered internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node. When a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node. Kube-proxy intercepts and controls where to forward the traffic, internal to the node, or to another worker node running the destination pod, or outside of the cluster. For inter-pod networking as pod has its own routable IP address there is no need for proxies or NAT. Except that with Kubernetes, services use a completely separate set of IP addresses. The IP addresses are not known by Calico, and are not directly routable. However, they are still reachable by pods since the kube-proxy performs NAT on those IPs (NAT from svc IP@ to Pod IP @). Any pod can communicate with any another pod using its assigned IP address, even if it\u2019s on a different worker node. K8s Services are used as virtual IP address (it is k8s concept), that are discovered via DNS and allow clients to reliably discover and connect to containers.","title":"Kubernetes networking"},{"location":"k8s/k8s-1/#services","text":"Services group a set of pods and provide network connection to these pods for other services in the cluster without exposing the actual private IP address of each pod. https://kubernetes.io/docs/concepts/services-networking/service/ A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. As Pods are ephemeral in nature, resources like IP addresses allocated to it cannot be static. As an example the pods running for the BFF apps are: $ kubectl get pods -n greencompute -o wide NAME READY STATUS RESTARTS AGE IP NODE gc-caseportal-bc-caseportal-app-698f98d787-56qz5 1/1 Running 0 5d 192.168.130.74 172.16.40.138 gc-caseportal-bc-caseportal-app-698f98d787-bhl5f 1/1 Running 0 5d 192.168.223.17 172.16.40.137 The 192.168.x.x is a flat, cluster wide, address space. As illustrated in the figure below: The command shows the eth0 interface in the container is map to this IP address. ``` $ kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- ip addr .... 4: eth0@if16: mtu 1500 qdisc noqueue state UP link/ether c2:74:08:4a:69:3f brd ff:ff:ff:ff:ff:ff inet 192.168.130.74/32 scope global eth0 You can use Kubernetes services to make an app available to other pods inside the cluster or to expose an app to the internet by abstracting the dynamic IP address assignment. The yaml below creates the casewebportal-svc: `kubectl apply -f svc.yml` ```yaml apiVersion: v1 kind: Service metadata: name: casewebportal-svc namespace: greencompute labels: app: casewebportal spec: type: NodePort ports: - port: 6100 targetPort: 6100 protocol: TCP name: http selector: app: casewebportal By default, each Service also gets an IP address (known as ClusterIP ), which is routable only inside the cluster. The target port 6100 is visible as 31560 on the node $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE casewebportal-svc NodePort 10.10.10.197 <none> 6100:31560/TCP 6d A Service is backed by a group of Pods. These Pods are exposed through endpoints. The command below displays the endpoints. The port number stays the same on all worker nodes and event Proxy node. $ kubectl get ep casewebportal-svc NAME ENDPOINTS AGE casewebportal-svc 192.168.130.74:6100,192.168.223.17:6100 6d When a Pod runs on a Node, the kubelet adds a set of environment variables for each active Service. You can see those variables with the command: $ kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- printenv A Service does the load balancing while selecting the Pods for forwarding the data/traffic. It uses the label and label selector to get access to the pods. Kubernetes offers a DNS cluster addon Service that automatically assigns dns names to other Services: $ kubectl get services kube-dns --namespace=kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.10.10.10 <none> 53/UDP,53/TCP 10d So we can do a nslookup within any pod to assess where to find the service: $ kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- nslookup casewebportal-svc nslookup: can't resolve '(null)': Name does not resolve Name: casewebportal-svc Address 1: 10.10.10.197 casewebportal-svc.greencompute.svc.cluster.local You can access ClusterIP service using the cluster local proxy: * Start the kubernetes local proxy: kubectl proxy --port=8080 * Access the service via url like: http://localhost:8080/api/v1/proxy/namespaces/NAMESPACE/services/SERVICE-NAME:PORT-NAME/ We can define a service without selectors, so no Endpoint object will be created, and we can map to your endpoint via the subset.addresses.ip spec. This is useful to abstract other kinds of backend component like DB server, a service in another namespace... The NodePort ServiceType is useful when we want to make the services accessible from the external world. BUT if the IP address of the VM change you need to take care of this problem in your client code. The end-user connects to the Worker Nodes on the specified port, which forwards the traffic to the applications running inside the cluster. To access the application from the external world, administrators can configure a reverse proxy outside the Kubernetes cluster and map the specific endpoint to the respective port on the Worker Nodes. See this article: https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0 Using NGINX server as front end and load balancer, the routing can be done outside the cluster. The following conf file an be deployed in the /etc/nginx/conf.d folder and will help to load balance between two pods: upstream caseportal { server 172.16.40.137:31560; server 172.16.40.138:31560; } server { location / { proxy_pass http://caseportal; } } Alternate is to use LoadBalancer service. With the LoadBalancer ServiceType: NodePort and ClusterIP Services are automatically created, and the external load balancer will route to them. The Services are exposed at a static port on each Worker Node. The Service is exposed externally using the underlying Cloud provider's load balancer feature. If you want to directly expose a service, this is the default method. All traffic on the port you specify will be forwarded to the service. Finally,the last model is by using ingress.","title":"Services"},{"location":"k8s/k8s-1/#ingress","text":"Ingress is another method (from LoadBalancer and NodePort) we can use to access our applications from the external world. It works at layer 7 traffic. The Kubernetes cluster must have an Ingress controller deployed to be able to create Ingress resources. The Ingress controller is deployed as a Docker container. Its Docker image contains a load balancer like NGInx and a controller daemon. The controller daemon receives the desired Ingress configuration from Kubernetes, it generates an NGInx configuration file and restarts the load balancer process for changes to take effect. Ingress controller is a load balancer managed by Kubernetes. Ingress helps to decouple the routing rules from the application, we can then update our application without worrying about its external access. With Ingress, users don't connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the respective Service. You can define one ingress for all the component of your application you want to expose to external world. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : web-ingress spec : rules : - host : myservice.mydomain.com http : paths : - path : /dashboardbff - backend : serviceName : dashboard-bff-svc servicePort : 8080 - host : green.myweb.com http : paths : - backend : serviceName : green-service servicePort : 80","title":"Ingress"},{"location":"k8s/k8s-1/#service-discovery","text":"As soon as the Pod starts on any Worker Node, the kubelet daemon running on that node adds a set of environment variables in the Pod for all active Services. Be careful while ordering Services, as the Pods will not have the environment variables set for Services which are created after the Pods are created. The DNS add-on, creates a DNS record for each normal Service and its format is like my-svc.my-namespace.svc.cluster.local within the same namespace can reach to other services with just their name. The following diagram groups all those networking concepts to illustrates, Ingress, service, kube-proxy and label selectors:","title":"Service Discovery"},{"location":"k8s/k8s-1/#kubernetes-dns","text":"Kubelets resolve hostnames for pods through a Service named kube-dns . kube-dns runs as a pod in the kube-system namespace. Every Service that is created is automatically assigned a DNS name. By default, this hostname is ServiceName.Namespace . All Services that are in the same namespace may omit Namespace and just use ServiceName . Here is an example: To understand the default kubernetes DNS pod, you can run k describe pod kube-dns-nw8gf -n kube-system It watches the Kubernetes master for changes in Services and Endpoints, and maintains in-memory lookup structures to serve DNS requests. It has a static address, and kubelet passes DNS to each container with the --cluster-dns= flag. The default domain is cluster.local .","title":"Kubernetes DNS"},{"location":"k8s/k8s-1/#virtual-private-cloud","text":"VPC delivers networking functions for cloud deployments in logically isolated segments using Software Defined Networking. It is a virtual networking platform tied to a tenant account. It provides logical isolation within the same tenant and other tenants. Zones are created using high bandwidth and low latency connections (3 ms). A VPC spans multiple zones at each region for high availability. It does not span regions. With VPC, operators can customize network topologies, sizes, IP addressing without NAT or tunnels. It supports bring your own IP address plan. The security controls are done at the subnet and server level. Some important features: A region has three or more zones. Multi-zone regions is designed to support enterprise workloads accross application, data, security and solution domains. Availability zone is an independent fault domains that do not share physical infrastructure. It has a latency requirement of <500 micro second intra-zone and <2 ms inter-zone.","title":"Virtual Private Cloud"},{"location":"k8s/k8s-1/#security","text":"With kubernetes deployed internally, companies have control over their physical environment, network, operating system patches, access to authentication and authorization internal registry. They control data access, access to administration console for the platform and the workload deployed. As part of security is the Identity and access management to control access to resource and connection protocol like OpenID . A team is a logical grouping of resources, users, and user groups. Teams can be restricted to all resources within a namespace. Access control gateway enforces role-based access control (RBAC) for all registered services. Network segmentation to provide isolation, using DMZ VLAN for specific workload, so deploying cluster in DMZ. And use VLAN to segment the physical network, subnets, clusters and namespaces to add more segmentation. Segmentation will depend of the application requirements. For securing kubernetes network control, you will have Edge nodes that are worker nodes with only ingress workloads and network policies. Traffic reaches application through private network. Backend services may not have access from public network. K8s network policy specifies how pods can communicate with each other, and Calico , the IP based networking, implements the policies (rule per IP addresses). Rules are ORe'ed, if any ingress /egress rule matches connection is allowed. Their are scoped at namespace level. We can isolate application using labels. THe srv1 backend can be access only from the frontend kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: backend-allow spec: podSelector: matchLabels: app: Srv1 Tier: backend ingress: - from: - podSelector: matchLabels: app: Srv1 Tier: frontend and the db backend can only be access from the srv1 backend. kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: db-allow spec: podSelector: matchLabels: app: Srv1 Tier: db ingress: - from: - podSelector: matchLabels: app: Srv1 Tier: backend >> NEXT","title":"Security"},{"location":"k8s/k8s-2/","text":"Security Role Based Access Control Add a user to a role: oc adm policy add-cluster-role-to-user strimzi-cluster-operator-namespaced IAM#emailasuserid@something.com Security Context Constraints Security Context Constraints (SCCs) to control permissions a pod can perform and what resources it can access Access to existing SCC: kubectl get scc Get security policies for a scc: oc describe scc privileged You can specify SCCs as resources that are handled by RBAC. This allows you to scope access to your SCCs to a certain project or to the entire cluster. Assigning users, groups, or service accounts directly to an SCC retains cluster-wide scope See explanations here","title":"Security"},{"location":"k8s/k8s-2/#security","text":"","title":"Security"},{"location":"k8s/k8s-2/#role-based-access-control","text":"Add a user to a role: oc adm policy add-cluster-role-to-user strimzi-cluster-operator-namespaced IAM#emailasuserid@something.com","title":"Role Based Access Control"},{"location":"k8s/k8s-2/#security-context-constraints","text":"Security Context Constraints (SCCs) to control permissions a pod can perform and what resources it can access Access to existing SCC: kubectl get scc Get security policies for a scc: oc describe scc privileged You can specify SCCs as resources that are handled by RBAC. This allows you to scope access to your SCCs to a certain project or to the entire cluster. Assigning users, groups, or service accounts directly to an SCC retains cluster-wide scope See explanations here","title":"Security Context Constraints"},{"location":"k8s/k8s-comp/","text":"Kubernetes Compendium Kubernetes product documentation kubectl cheatsheet IKS Docker install on redhat or centos with some basic docker commands Kubernetes official tutorial Helm tutorial in garage web site Our CASE team troubleshooting CASE private cloud repo interesting for the different installations and terraform work. CASE Storage best practice CASE Backup and restore for k8s workload Deploying a Ceph cluster for ICP PV How To Run OpenVPN in a Docker Container on Ubuntu Calico as network layer Splendors and Miseries of Docker Network List of deployments done in the CASE solution implementations CLI summary in CASE repo Technical Dive into StatefulSets and Deployments in Kubernetes ] Kubectl tricks Explore security in IBM Cloud Private Get familiar with the security capabilities of IBM Cloud\u2122 Private and compare them with the capabilities of other private and public clouds. Set of demo from IBM Cloud team Securing Containers Workload Patterns in IKS explains how to run containers workload securely in a Kubernetes cluster in IBM Cloud Kubernetes Service (IKS) without using any gateway router or firewall appliance for security aspects. Update the user name or password for admin in ICP Deep dive on IKS from Lionel Mace","title":"Compendium"},{"location":"k8s/k8s-comp/#kubernetes-compendium","text":"Kubernetes product documentation kubectl cheatsheet IKS Docker install on redhat or centos with some basic docker commands Kubernetes official tutorial Helm tutorial in garage web site Our CASE team troubleshooting CASE private cloud repo interesting for the different installations and terraform work. CASE Storage best practice CASE Backup and restore for k8s workload Deploying a Ceph cluster for ICP PV How To Run OpenVPN in a Docker Container on Ubuntu Calico as network layer Splendors and Miseries of Docker Network List of deployments done in the CASE solution implementations CLI summary in CASE repo Technical Dive into StatefulSets and Deployments in Kubernetes ] Kubectl tricks Explore security in IBM Cloud Private Get familiar with the security capabilities of IBM Cloud\u2122 Private and compare them with the capabilities of other private and public clouds. Set of demo from IBM Cloud team Securing Containers Workload Patterns in IKS explains how to run containers workload securely in a Kubernetes cluster in IBM Cloud Kubernetes Service (IKS) without using any gateway router or firewall appliance for security aspects. Update the user name or password for admin in ICP Deep dive on IKS from Lionel Mace","title":"Kubernetes Compendium"},{"location":"k8s/k8s-faq/","text":"Kubernetes FAQ what do you understand by kubernetes? k8s is open source container management / orchestration tool to deploy scaling and descaling of contrainers. Writing in GO and it is part of the cloud foundation. The key features: * automated scheduling to launch container on cluster nodes * self healing: identify container dying and reschedule them * automated rollouts and rollbacks * horizontal scaling and load balancing Difference between deploying an app on host versus containers? With host you share OS, libraries, and apps co-exist. Packaging with container includes OS, config, the libraries and app binary. Containers are made of: * namespaces: process PID, network, UTS host and domain name isolation, Inter Process Communication isolation, user (container UID are isolated from OS ones), mount tables, * cgroups: Control group is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes. * layered filesystem: some base layers (e.g linux base layers) can be shared by multiple containers. Layers are usually read-only, and containers make copy if they have to modify the layer * content: app, libraries... So containers are more portable. How k8s simplify container deployment? By externalizing configuration and requirements, and defining relation between container via configurations: deploymnet, service, secrets, config map... Developers focus on desired state of the application and defined configuration for k8s to manage. What is a Heapster? Heapster is a cluster-wide aggregator of monitoring and event data. It supports Kubernetes natively and works on all Kubernetes setups. It is deprecated. Consider using metrics-server and a third party metrics pipeline to gather Prometheus-format metrics instead. Which process runs on Kubernetes master node? kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.40.132 Kube-apiserver etcd when not clustered prometheus node exporter logging ELK Heapster Calico node Auth API keys Vulnerability advisor How to do multiple deployment a day? Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Updates are versioned and any Deployment update can be reverted to previous. The approach is to deploy the image to a docker registry and then use: kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 then we can see the new pods scheduled: kubectl get pods . To rollback to a previous use kubectl rollout undo deployment/deployment-name Using helm we can do helm rollback <release-name> What is the best practices for pod to pod communications? Don\u2019t specify a hostPort for a Pod unless it is absolutely necessary. When you bind a Pod to a hostPort, it limits the number of places the Pod can be scheduled, because each combination must be unique. Services are assigned a DNS A record for a name of the form my-svc.my-namespace.svc.cluster.local . This resolves to the cluster IP of the Service. So prefer to use DNS name to support this communication. How to isolate pod to pod communication? By default, pods are non-isolated; they accept traffic from any source. Use network policy resource to group pods allowed to communicate with each other and other network endpoints. You need a network solution that support network policies. Like Calico. A simple example of network policy with nginx . How to secure internal traffic ? The first answer is to use to istio to simplify the management of certificates and pod to pod communication. A more manual is to add a reverse-proxy like nginx in each pod. The SSL proxy will terminate HTTPS connection, using the same nginx config. App in container listen to localhost. Security practices to Consider apply security to environments like host OS of the different nodes restrict access to etcd implement network segmentation: define network policy use continuous security Vulnerability scanning limit access to nodes define resource quota to control overusage use private repository and verified images How RBAC works? Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users. A role contains rules that represent a set of permissions. Two types: Role at the namespace level, or ClusterRole. There is no deny rules. A role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted To assess if RBAC is enabled: kubectl get clusterroles | grep -i rbac RBAC documentation How to do A/B testing ? A/B testing deployments consists of routing a subset of users to a new functionality under specific conditions, like HTTP headers, cookie, weight... You need Istio to support A/B testing. How to support multiple version of the same app? See A/B testing for one approach and use one of the following deployment strategies: Blue/green release a new version alongside the old version then switch traffic at the load balancer level (example two different hostnames in the Ingress). Canary is used when releasing a new version to a subset of users, then proceed to a full rollout. It can be done using two Deployments with common pod labels. But the use of service mesh like istio will define greater control over traffic. A ramped deployment updates pods in a rolling update fashion, a secondary ReplicaSet is created with the new version of the application, then the number of replicas of the old version is decreased and the new version is increased until the correct number of replicas is reached. kubernetes-deployment-strategies examples What are the best practices around Logging? Logs should have a separate storage and lifecycle independent of nodes, pods, or containers. This concept is called cluster-level-logging. To get container logs use one of: kubectl logs <podname> kubectl logs <podname> --previous By default, if a container restarts, the kubelet keeps one terminated container with its logs. Everything a containerized application writes to stdout and stderr is handled and redirected somewhere by a container engine to a logging driver. On machines with systemd, the kubelet and container runtime write to journald. f systemd is not present, they write to .log files in the /var/log directory. For cluster-level logging three choices: * Use a node-level logging agent that runs on every node: another container which access logs at node level and exposes logs or pushes logs to a backend. Better define a DaemonSet for that. This is the most common and encouraged approach for a Kubernetes cluster. For GKE Stackdriver Logging , or elasticsearch and kibana * Include a dedicated sidecar container for logging in an application pod. The sidecar streams application logs to its own stdout. It runs a logging agent, which is configured to pick up logs from an application container. Each sidecar container could tail a particular log file from a shared volume and then redirect the logs to its own stdout stream * Push logs directly to a backend from within an application. If you have an application that writes to a single file, it\u2019s generally better to set /dev/stdout as destination rather than implementing the streaming sidecar container approach. How is persistent volume allocated? Before you can deploy most Kubernetes-based application, you must first create system-level storage from the underlying infrastructure. hostPath Persistence Volume uses local to the worker node filesystem. So create a host directory on all worker nodes in your cluster. hostPath PV could not failover to other worker node. NFS is shared between worker nodes. See this article to set up NFS on ubuntu . When crating the PV, use the key value pairs: server - 172.29.2.8 path - /data/local/share Then define PVC and configure your application to access the PVC defined. How to debug 503? See the troubleshooting here how to see ingress controller is up and running in k8s? Get the list of ingress: kubectl get ing -n greencompute Get a specific ingress: kubectl describe ing -n greencompute gc-customer-ms-greencompute-customerms Name: gc-customer-ms-greencompute-customerms Namespace: greencompute Address: 172.16.40.131 Default backend: default-http-backend:80 (<none>) Rules: Host Path Backends ---- ---- -------- customer.green.case / gc-customer-ms-greencompute-customerms:9080 (<none>) Annotations: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 42m nginx-ingress-controller Ingress greencompute/gc-customer-ms-greencompute-customerms Check the Ingress Controller Logs: kubectl get pods -n kube-system then kubectl logs nginx-ingress-controller-gvcj5 -n kube-system . Search for message with \"Event ... Kind: Ingress...\" Verify the nginx configuration with a command like: kubectl exec -it -n kube-system nginx-ingress-controller-gvcj5 cat /etc/nginx/nginx.conf Another source of information What is Calico ? It uses a IP-only routing connectivity to connect containers. It does not need tunneling and virtual multicast or broadcast. It is an implementation of the Container Network Interface protocol: a plug and play networking capability with the goal to support adding and removing container easily with dynamic IP address assignment. Each container has an IP address as Node has one too. When a pod is scheduled the orchestrator calls CNI plugin, Calico, which defines the IP address to use, and persists the states in etcd . There is a container, called felix , which runs in each host and modifies the linux kernel to reference the new IP address in IP routing table. There is also a BGP component that propagated the IP address to all the node of the cluster so they can send connection to this new IP address. This is to flatten the network. Container Network Interface ? Apply the network as software approach to container but focusing of connecting workload (or remove it) instead of doing lower level of network configuration. Network policy for security is not in CNI. CNI supports two commands: ADD and DEL that orchestrator can use to specify a container is added or removed. On the ADD command, CNI create a network interface (visible via ifconfig ) within the network namespace and add new route ( route -n ) in the host so traffic can be routed to the container and in the container define default route to be able to get out. What is the number of master needed to support HA? It is linked to the cluster size, and the expected fault tolerance which is computed as (#_of_master - 1) / 2. It should be at least 1. You must have an odd number of masters in your cluster. Majority is the number of master nodes needed for the cluster to be able to operate. For a cluster of 6 worker nodes, you need a majority of 4 and then a fault tolerance of 2. How to use k8s in docker edge? This tutorial to get Edge and k8s started. The following commands can help once docker + k8s are running too: # Set the cluster - Use tls-verify=true to avoid: Unable to connect to the server: x509: certificate signed by unknown authority $ kubectl config set-cluster docker-for-desktop --server=https://kubernetes:6443 --insecure-skip-tls-verify=true # Error from server (Forbidden): services is forbidden: User \"system:anonymous\" cannot list services in the namespace \"kube-system\"","title":"FAQ"},{"location":"k8s/k8s-faq/#kubernetes-faq","text":"","title":"Kubernetes FAQ"},{"location":"k8s/k8s-faq/#what-do-you-understand-by-kubernetes","text":"k8s is open source container management / orchestration tool to deploy scaling and descaling of contrainers. Writing in GO and it is part of the cloud foundation. The key features: * automated scheduling to launch container on cluster nodes * self healing: identify container dying and reschedule them * automated rollouts and rollbacks * horizontal scaling and load balancing","title":"what do you understand by kubernetes?"},{"location":"k8s/k8s-faq/#difference-between-deploying-an-app-on-host-versus-containers","text":"With host you share OS, libraries, and apps co-exist. Packaging with container includes OS, config, the libraries and app binary. Containers are made of: * namespaces: process PID, network, UTS host and domain name isolation, Inter Process Communication isolation, user (container UID are isolated from OS ones), mount tables, * cgroups: Control group is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes. * layered filesystem: some base layers (e.g linux base layers) can be shared by multiple containers. Layers are usually read-only, and containers make copy if they have to modify the layer * content: app, libraries... So containers are more portable.","title":"Difference between deploying an app on host versus containers?"},{"location":"k8s/k8s-faq/#how-k8s-simplify-container-deployment","text":"By externalizing configuration and requirements, and defining relation between container via configurations: deploymnet, service, secrets, config map... Developers focus on desired state of the application and defined configuration for k8s to manage.","title":"How k8s simplify container deployment?"},{"location":"k8s/k8s-faq/#what-is-a-heapster","text":"Heapster is a cluster-wide aggregator of monitoring and event data. It supports Kubernetes natively and works on all Kubernetes setups. It is deprecated. Consider using metrics-server and a third party metrics pipeline to gather Prometheus-format metrics instead.","title":"What is a Heapster?"},{"location":"k8s/k8s-faq/#which-process-runs-on-kubernetes-master-node","text":"kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.40.132 Kube-apiserver etcd when not clustered prometheus node exporter logging ELK Heapster Calico node Auth API keys Vulnerability advisor","title":"Which process runs on Kubernetes master node?"},{"location":"k8s/k8s-faq/#how-to-do-multiple-deployment-a-day","text":"Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Updates are versioned and any Deployment update can be reverted to previous. The approach is to deploy the image to a docker registry and then use: kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 then we can see the new pods scheduled: kubectl get pods . To rollback to a previous use kubectl rollout undo deployment/deployment-name Using helm we can do helm rollback <release-name>","title":"How to do multiple deployment a day?"},{"location":"k8s/k8s-faq/#what-is-the-best-practices-for-pod-to-pod-communications","text":"Don\u2019t specify a hostPort for a Pod unless it is absolutely necessary. When you bind a Pod to a hostPort, it limits the number of places the Pod can be scheduled, because each combination must be unique. Services are assigned a DNS A record for a name of the form my-svc.my-namespace.svc.cluster.local . This resolves to the cluster IP of the Service. So prefer to use DNS name to support this communication.","title":"What is the best practices for pod to pod communications?"},{"location":"k8s/k8s-faq/#how-to-isolate-pod-to-pod-communication","text":"By default, pods are non-isolated; they accept traffic from any source. Use network policy resource to group pods allowed to communicate with each other and other network endpoints. You need a network solution that support network policies. Like Calico. A simple example of network policy with nginx .","title":"How to isolate pod to pod communication?"},{"location":"k8s/k8s-faq/#how-to-secure-internal-traffic","text":"The first answer is to use to istio to simplify the management of certificates and pod to pod communication. A more manual is to add a reverse-proxy like nginx in each pod. The SSL proxy will terminate HTTPS connection, using the same nginx config. App in container listen to localhost.","title":"How to secure internal traffic ?"},{"location":"k8s/k8s-faq/#security-practices-to-consider","text":"apply security to environments like host OS of the different nodes restrict access to etcd implement network segmentation: define network policy use continuous security Vulnerability scanning limit access to nodes define resource quota to control overusage use private repository and verified images","title":"Security practices to Consider"},{"location":"k8s/k8s-faq/#how-rbac-works","text":"Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users. A role contains rules that represent a set of permissions. Two types: Role at the namespace level, or ClusterRole. There is no deny rules. A role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted To assess if RBAC is enabled: kubectl get clusterroles | grep -i rbac RBAC documentation","title":"How RBAC works?"},{"location":"k8s/k8s-faq/#how-to-do-ab-testing","text":"A/B testing deployments consists of routing a subset of users to a new functionality under specific conditions, like HTTP headers, cookie, weight... You need Istio to support A/B testing.","title":"How to do A/B testing ?"},{"location":"k8s/k8s-faq/#how-to-support-multiple-version-of-the-same-app","text":"See A/B testing for one approach and use one of the following deployment strategies: Blue/green release a new version alongside the old version then switch traffic at the load balancer level (example two different hostnames in the Ingress). Canary is used when releasing a new version to a subset of users, then proceed to a full rollout. It can be done using two Deployments with common pod labels. But the use of service mesh like istio will define greater control over traffic. A ramped deployment updates pods in a rolling update fashion, a secondary ReplicaSet is created with the new version of the application, then the number of replicas of the old version is decreased and the new version is increased until the correct number of replicas is reached. kubernetes-deployment-strategies examples","title":"How to support multiple version of the same app?"},{"location":"k8s/k8s-faq/#what-are-the-best-practices-around-logging","text":"Logs should have a separate storage and lifecycle independent of nodes, pods, or containers. This concept is called cluster-level-logging. To get container logs use one of: kubectl logs <podname> kubectl logs <podname> --previous By default, if a container restarts, the kubelet keeps one terminated container with its logs. Everything a containerized application writes to stdout and stderr is handled and redirected somewhere by a container engine to a logging driver. On machines with systemd, the kubelet and container runtime write to journald. f systemd is not present, they write to .log files in the /var/log directory. For cluster-level logging three choices: * Use a node-level logging agent that runs on every node: another container which access logs at node level and exposes logs or pushes logs to a backend. Better define a DaemonSet for that. This is the most common and encouraged approach for a Kubernetes cluster. For GKE Stackdriver Logging , or elasticsearch and kibana * Include a dedicated sidecar container for logging in an application pod. The sidecar streams application logs to its own stdout. It runs a logging agent, which is configured to pick up logs from an application container. Each sidecar container could tail a particular log file from a shared volume and then redirect the logs to its own stdout stream * Push logs directly to a backend from within an application. If you have an application that writes to a single file, it\u2019s generally better to set /dev/stdout as destination rather than implementing the streaming sidecar container approach.","title":"What are the best practices around Logging?"},{"location":"k8s/k8s-faq/#how-is-persistent-volume-allocated","text":"Before you can deploy most Kubernetes-based application, you must first create system-level storage from the underlying infrastructure. hostPath Persistence Volume uses local to the worker node filesystem. So create a host directory on all worker nodes in your cluster. hostPath PV could not failover to other worker node. NFS is shared between worker nodes. See this article to set up NFS on ubuntu . When crating the PV, use the key value pairs: server - 172.29.2.8 path - /data/local/share Then define PVC and configure your application to access the PVC defined.","title":"How is persistent volume allocated?"},{"location":"k8s/k8s-faq/#how-to-debug-503","text":"See the troubleshooting here","title":"How to debug 503?"},{"location":"k8s/k8s-faq/#how-to-see-ingress-controller-is-up-and-running-in-k8s","text":"Get the list of ingress: kubectl get ing -n greencompute Get a specific ingress: kubectl describe ing -n greencompute gc-customer-ms-greencompute-customerms Name: gc-customer-ms-greencompute-customerms Namespace: greencompute Address: 172.16.40.131 Default backend: default-http-backend:80 (<none>) Rules: Host Path Backends ---- ---- -------- customer.green.case / gc-customer-ms-greencompute-customerms:9080 (<none>) Annotations: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 42m nginx-ingress-controller Ingress greencompute/gc-customer-ms-greencompute-customerms Check the Ingress Controller Logs: kubectl get pods -n kube-system then kubectl logs nginx-ingress-controller-gvcj5 -n kube-system . Search for message with \"Event ... Kind: Ingress...\" Verify the nginx configuration with a command like: kubectl exec -it -n kube-system nginx-ingress-controller-gvcj5 cat /etc/nginx/nginx.conf Another source of information","title":"how to see ingress controller is up and running in k8s?"},{"location":"k8s/k8s-faq/#what-is-calico","text":"It uses a IP-only routing connectivity to connect containers. It does not need tunneling and virtual multicast or broadcast. It is an implementation of the Container Network Interface protocol: a plug and play networking capability with the goal to support adding and removing container easily with dynamic IP address assignment. Each container has an IP address as Node has one too. When a pod is scheduled the orchestrator calls CNI plugin, Calico, which defines the IP address to use, and persists the states in etcd . There is a container, called felix , which runs in each host and modifies the linux kernel to reference the new IP address in IP routing table. There is also a BGP component that propagated the IP address to all the node of the cluster so they can send connection to this new IP address. This is to flatten the network.","title":"What is Calico?"},{"location":"k8s/k8s-faq/#container-network-interface","text":"Apply the network as software approach to container but focusing of connecting workload (or remove it) instead of doing lower level of network configuration. Network policy for security is not in CNI. CNI supports two commands: ADD and DEL that orchestrator can use to specify a container is added or removed. On the ADD command, CNI create a network interface (visible via ifconfig ) within the network namespace and add new route ( route -n ) in the host so traffic can be routed to the container and in the container define default route to be able to get out.","title":"Container Network Interface?"},{"location":"k8s/k8s-faq/#what-is-the-number-of-master-needed-to-support-ha","text":"It is linked to the cluster size, and the expected fault tolerance which is computed as (#_of_master - 1) / 2. It should be at least 1. You must have an odd number of masters in your cluster. Majority is the number of master nodes needed for the cluster to be able to operate. For a cluster of 6 worker nodes, you need a majority of 4 and then a fault tolerance of 2.","title":"What is the number of master needed to support HA?"},{"location":"k8s/k8s-faq/#how-to-use-k8s-in-docker-edge","text":"This tutorial to get Edge and k8s started. The following commands can help once docker + k8s are running too: # Set the cluster - Use tls-verify=true to avoid: Unable to connect to the server: x509: certificate signed by unknown authority $ kubectl config set-cluster docker-for-desktop --server=https://kubernetes:6443 --insecure-skip-tls-verify=true # Error from server (Forbidden): services is forbidden: User \"system:anonymous\" cannot list services in the namespace \"kube-system\"","title":"How to use k8s in docker edge?"}]}